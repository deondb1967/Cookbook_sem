[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Structural equation modeling with lavaan in R: Examples and excercises",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit: https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Vectors_dataframes.html",
    "href": "Vectors_dataframes.html",
    "title": "2  Vectors and data frames",
    "section": "",
    "text": "2.1 Vector\nIn R a vector is a collection of elements that are all of the same type. A typical data set consists of a number of rows that each represents a case, and a number of columns. Each of the columns is a vector.\nWe can create a vector in R by concatanating the elements. We do this by typing the elements inside round brackets. Each element should be separated by a comma.String or character elements should be enclosed in quotation marks. Finally, we place a c before the round brackets.\nFor instance, say we have the following elements, 10, 8, 12, 15. We can combine them into a vector as follows: c(10, 8, 12, 15). Similarly, we can combine the elements Dutch, Mandarin, Swahili and Russian into a vector as follows: c(“Dutch”, “Mandarin”, “Swahili”, “Russian”).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors and data frames</span>"
    ]
  },
  {
    "objectID": "Vectors_dataframes.html#a-data-frame-consists-of-vectors",
    "href": "Vectors_dataframes.html#a-data-frame-consists-of-vectors",
    "title": "2  Vectors and data frames",
    "section": "2.2 A data frame consists of vectors",
    "text": "2.2 A data frame consists of vectors\nConsider the following mini data set. There are four rows, each of which contains data of a particular person. There are five columns, each of which represents a variable. Note that each of the columns is a vector. The first column (name) vector is a character vector, the second (sex) is also a character vector, the third (division) is a numeric vector, the fourth (graduate) is a logical vector, and the fifth (experience) is a numeric vector.\n\n\n\n\n\n\n\n\nname\nsex\ndivision\ngraduate\nexperience\n\n\n\n\nPeter\nmale\n1\nTRUE\n1\n\n\nPaul\nmale\n2\nFALSE\n3\n\n\nMary\nfemale\n2\nTRUE\n6\n\n\nJanette\nfemale\n3\nTRUE\n8\n\n\n\n\n\n\n\nWe would typically create such a data set in software such as Excel, and then import the data into R. It is informative, however, to create such a data set in R as an excercise. In the paragraphs that follow we do that by first creating five vectors (each with four elements) and then concatenating (combining) the vectors into a data frame.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors and data frames</span>"
    ]
  },
  {
    "objectID": "Vectors_dataframes.html#creating-a-data-frame-in-r",
    "href": "Vectors_dataframes.html#creating-a-data-frame-in-r",
    "title": "2  Vectors and data frames",
    "section": "2.3 Creating a data frame in R",
    "text": "2.3 Creating a data frame in R",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors and data frames</span>"
    ]
  },
  {
    "objectID": "Vectors_dataframes.html#logical-vectors",
    "href": "Vectors_dataframes.html#logical-vectors",
    "title": "2  Vectors and data frames",
    "section": "3.1 Logical vectors",
    "text": "3.1 Logical vectors\nSecond, we create a logical vector for the graduate variable. Note that we do not enclose TRUE and FALSE with quotation marks.\n\nc(TRUE, FALSE, TRUE, TRUE)\n\n[1]  TRUE FALSE  TRUE  TRUE",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors and data frames</span>"
    ]
  },
  {
    "objectID": "Vectors_dataframes.html#numeric-vectors",
    "href": "Vectors_dataframes.html#numeric-vectors",
    "title": "2  Vectors and data frames",
    "section": "3.2 Numeric vectors",
    "text": "3.2 Numeric vectors\nThird, we create numeric vectors for the division and experience variables. Note that the numbers in our two numeric vectors serve different purposes. The numbers in the experience vector represents differences in quantity (i.e. differences in years experience). By contrast, however, the numbers in the division column indicates differences in type or quality. For instance, in the division vector the numbers 1, 2 and 3 are used to indicate the divisions (finance, HR or marketing) in which Peter, Paul, Mary and Janette work.\n\n## Experience\nc(1,3,6,8)\n\n[1] 1 3 6 8\n\n## Division\nc(1, 2, 2, 3)\n\n[1] 1 2 2 3",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors and data frames</span>"
    ]
  },
  {
    "objectID": "Vectors_dataframes.html#storing-vectors-as-objects",
    "href": "Vectors_dataframes.html#storing-vectors-as-objects",
    "title": "2  Vectors and data frames",
    "section": "3.3 Storing vectors as objects",
    "text": "3.3 Storing vectors as objects\nWe can store a vector for further use by assigning it to an “object” that we give any name of our choosing. We can then use that name to access the vector. Below we assign the character vector with the names of the four participants to an object that we call “name”, the character vector containing information about the sex of the participants to an object we call “sex”, the numeric vector containing information about years experience to an object we call “experience”, the numeric vector containing information about the division in which a person works as an object we call “division”, and the logical vector containing information about graduate status as an object we call “graduate”.\nWe can access the contents of any object by typing its name and then running it.\n\nname &lt;- c(\"Peter\", \"Paul\", \"Mary\", \"Janette\") \n\nsex  &lt;- c(\"male\", \"male\", \"female\", \"female\") \n\ngraduate &lt;- c(TRUE, FALSE, TRUE, TRUE) \n\nexperience &lt;- c(1, 3, 6, 8)\n\ndivision &lt;- c(1, 2, 2, 3)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors and data frames</span>"
    ]
  },
  {
    "objectID": "Vectors_dataframes.html#combining-vectors-into-a-data-frame",
    "href": "Vectors_dataframes.html#combining-vectors-into-a-data-frame",
    "title": "2  Vectors and data frames",
    "section": "3.4 Combining vectors into a data frame",
    "text": "3.4 Combining vectors into a data frame\nWe can combine vectors into a data frame, on condition that the vectors are of the same length. The order in which we type the names of the vectors determines their positions in the data frame. Each of the vectors becomes a column in the data frame.\nBelow we combine our five vectors into a data frame that we call df (note that we could choose any name we want).\n\ndf &lt;- data.frame(name, sex, division, graduate, experience)\n\ndf\n\n     name    sex division graduate experience\n1   Peter   male        1     TRUE          1\n2    Paul   male        2    FALSE          3\n3    Mary female        2     TRUE          6\n4 Janette female        3     TRUE          8\n\nstr(df)\n\n'data.frame':   4 obs. of  5 variables:\n $ name      : chr  \"Peter\" \"Paul\" \"Mary\" \"Janette\"\n $ sex       : chr  \"male\" \"male\" \"female\" \"female\"\n $ division  : num  1 2 2 3\n $ graduate  : logi  TRUE FALSE TRUE TRUE\n $ experience: num  1 3 6 8",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors and data frames</span>"
    ]
  },
  {
    "objectID": "Vectors_dataframes.html#changing-a-character-or-numeric-vector-to-a-factor",
    "href": "Vectors_dataframes.html#changing-a-character-or-numeric-vector-to-a-factor",
    "title": "2  Vectors and data frames",
    "section": "3.5 Changing a character or numeric vector to a factor",
    "text": "3.5 Changing a character or numeric vector to a factor\nThe sex variable is stored as a character vector, whereas the division variable is stored as a numeric vector. Both of these variables indicate membership to different levels of a categorical variable. For instance, the words “male” and “female” are different levels of the categorical variable “sex”. Similarly, the numbers 1, 2 and 3 are used to indicate different levels, i.e. “finance”, “HR”, and “marketing” of the categorical variable “division”. We need to instruct R to treat the sex and division variables as categorical variables. In R, a categorical variables is referred to as a factor.\nIn the example below we use the factor() function to instruct R to treat the sex and division variables as factors. Note how we use the $ symbol to gain access to a vector in a data frame.\n\nlibrary(tidyverse)\ndf$sex &lt;- factor(df$sex)\n\n\ndf$division &lt;- factor(df$division, \n                      levels = c(1, 2, 3),\n                      labels = c(\"Finance\", \n                                 \"HR\",\n                                 \"Marketing\"))\n\nstr(df)\n\n'data.frame':   4 obs. of  5 variables:\n $ name      : chr  \"Peter\" \"Paul\" \"Mary\" \"Janette\"\n $ sex       : Factor w/ 2 levels \"female\",\"male\": 2 2 1 1\n $ division  : Factor w/ 3 levels \"Finance\",\"HR\",..: 1 2 2 3\n $ graduate  : logi  TRUE FALSE TRUE TRUE\n $ experience: num  1 3 6 8\n\n\nWe use the str() function to inspect the structure of the data frame. The division variable now is a factor and no longer a numeric vector.\n\ndf %&gt;% gt()\n\n\n\n\n\n\n\nname\nsex\ndivision\ngraduate\nexperience\n\n\n\n\nPeter\nmale\nFinance\nTRUE\n1\n\n\nPaul\nmale\nHR\nFALSE\n3\n\n\nMary\nfemale\nHR\nTRUE\n6\n\n\nJanette\nfemale\nMarketing\nTRUE\n8",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors and data frames</span>"
    ]
  },
  {
    "objectID": "Vectors_dataframes.html#exercise",
    "href": "Vectors_dataframes.html#exercise",
    "title": "2  Vectors and data frames",
    "section": "3.6 Exercise",
    "text": "3.6 Exercise\nConsider the data set below. Recreate this data set in R by first creating five separate vectors and then store it as a data frame with the name mydata. Be sure to instruct R to treat area, sex and rank as factors. Give appropriate labels to the levels of the sex and rank variables. Finally, use the str() function to inspect the structure of the data frame.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors and data frames</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html",
    "href": "ReadingStoringData.html",
    "title": "3  Reading and storing data in R",
    "section": "",
    "text": "3.1 Setting the working directory\nThe first step in any data analysis session with R is to set the working directory. The working directory is a folder that contains your data and any objects and files that you create and store while working with R.\nYou should create a folder that will serve as your working directory on your computer or on an external drive. It is strongly recommended to keep the name of this folder short and straightforward.\nYou can set the working directory via the Rstudio menu: Session/Set Working Directory/Choose Directory. R uses the setwd() function to set the working directory. The entire command, which includes the path to the working directory will be printed in the console window. You should copy this command and then paste it into the first line of the script window. The next time that you start R and want to set the working directory, you can highlight the first row of the script and run it.\nIn the example below I first used the menus to set the working directory to “C:/myR/Masters2021”. Next, I copied the line of code in the console window and pasted it into the first line of the script window. By running the line of code in the script window I can set the working directory in future sessions without having to return to the menu.\nI also included the getwd() function, which is used to see the active working directory. Users sometimes change the working directory during an R session and getwd() can be used at any time to remind you of the current working directory.\n#setwd(\"C:/Users/deondb/OneDrive - Stellenbosch University/myR/Masters2021\")\n#getwd()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html#reading-the-data-and-storing-it-as-an-object",
    "href": "ReadingStoringData.html#reading-the-data-and-storing-it-as-an-object",
    "title": "3  Reading and storing data in R",
    "section": "3.2 Reading the data and storing it as an object",
    "text": "3.2 Reading the data and storing it as an object\nData will typically be captured in a spreadsheet programme like Excel. One of the safest ways to get data into R is to first–in the working directory folder–store the Excel file as a comma separated values file (.csv), second to read the csv file in R, and third to store the data as an object in R. In R the stored data is referred to as a data frame*.\nWe accomplish the second and third steps jointly by using the read.csv() function. In the example we read the data and then store it to an object (a data frame) with the name “mydata”. Note that we can give the data frame any name we want (but the name should not start with a number and it should not contain any special characters such as @, #, $, %, !, etc). The name also should not contain spaces. Two words can be used in a name if they are separated by an underscore or by a full stop (e.g. “my.data” or “my_data”) It is good practice to keep the name short and straightforward, but as informative as possible.\n\nmydata &lt;- read.csv(\"SA_Swiss.csv\")\n\nOn some machines Excel uses a semi-colon rather than a comma to separate values. In such cases you should use the read.csv2() function rather than the read.csv() function.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html#check-that-the-data-has-been-properly-read",
    "href": "ReadingStoringData.html#check-that-the-data-has-been-properly-read",
    "title": "3  Reading and storing data in R",
    "section": "3.3 Check that the data has been properly read",
    "text": "3.3 Check that the data has been properly read\nYou can view the contents of the data frame with the View() function, which will open a spreadsheet in the script window. Scrolling through the spreadsheet will reveal if the data were properly read.\n\nView(mydata)\n\nYou can also request that R print the first few lines of the data frame by using the head() function.\n\nhead(mydata)\n\n  GWS1 GWS2 GWS3 GWS4 GWS5 GWS6 GWS7 GWS8 GWS9 UWES1 UWES2 UWES3 UWES4 UWES5\n1    2    1    2    1    1    1    1    1    1     5     5     5     5     5\n2    3    3    2    3    3    4    4    3    3     4     4     4     4     4\n3    2    2    2    2    2    3    4    2    3     5     5     6     6     5\n4    2    2    2    3    3    3    1    3    3     4     4     4     4     3\n5    1    1    1    2    2    1    1    1    1     6     6     6     6     5\n6    2    2    2    2    2    3    3    2    1     3     3     4     4     4\n  UWES6 UWES7 UWES8 UWES9 MBI1 MBI2 MBI3 MBI4 MBI5 MBI6 MBI7 MBI8 MBI9 MBI10\n1     5     5     4     4    1    1    1    1    1    1    1    1    1     1\n2     5     5     5     5    2    2    2    1    1    3    3    1    2     2\n3     6     6     5     4    1    1    2    1    1    1    1    1    1     1\n4     4     4     4     4    1    1    1    1    1    2    1    1    1     1\n5     5     6     6     3    1    1    1    1    1    1    1    1    1     1\n6     4     4     4     4    1    1    3    2    1    2    2    2    3     2\n  MBI11 MBI12 MBI13 MBI14 MBI15 MBI16 GHQ1 GHQ2 GHQ3 GHQ4 GHQ5 GHQ6 GHQ7 GHQ8\n1     4     3     2     1     1     1    1    1    2    2    1    4    1    1\n2     3     3     3     3     3     3    1    2    1    1    2    1    2    2\n3     3     3     3     2     3     2    1    2    1    1    3    2    2    1\n4     4     4     4     4     4     4    1    2    2    2    2    1    2    2\n5     7     7     7     7     7     7    2    1    1    1    1    4    1    2\n6     4     3     3     3     4     4    2    1    1    1    2    1    2    2\n  GHQ9 GHQ10 GHQ11 GHQ12 Country Age Gender\n1    1     1     1     1       2  35      1\n2    2     2     1     1       2  35      0\n3    2     1     1     2       2  22      0\n4    1     1     1     2       2  38      1\n5    1     1     1     1       2  20      0\n6    1     1     1     1       2  25      1\n\n\nFinally, it is good practice to use the names() function to print to the console window the names of the variables in the data frame. Each row of the printed names will start with a number in square brackets and that number represents the serial position in the data frame of the first variable in that row.\n\nnames(mydata)\n\n [1] \"GWS1\"    \"GWS2\"    \"GWS3\"    \"GWS4\"    \"GWS5\"    \"GWS6\"    \"GWS7\"   \n [8] \"GWS8\"    \"GWS9\"    \"UWES1\"   \"UWES2\"   \"UWES3\"   \"UWES4\"   \"UWES5\"  \n[15] \"UWES6\"   \"UWES7\"   \"UWES8\"   \"UWES9\"   \"MBI1\"    \"MBI2\"    \"MBI3\"   \n[22] \"MBI4\"    \"MBI5\"    \"MBI6\"    \"MBI7\"    \"MBI8\"    \"MBI9\"    \"MBI10\"  \n[29] \"MBI11\"   \"MBI12\"   \"MBI13\"   \"MBI14\"   \"MBI15\"   \"MBI16\"   \"GHQ1\"   \n[36] \"GHQ2\"    \"GHQ3\"    \"GHQ4\"    \"GHQ5\"    \"GHQ6\"    \"GHQ7\"    \"GHQ8\"   \n[43] \"GHQ9\"    \"GHQ10\"   \"GHQ11\"   \"GHQ12\"   \"Country\" \"Age\"     \"Gender\" \n\ndata.frame(names(mydata))\n\n   names.mydata.\n1           GWS1\n2           GWS2\n3           GWS3\n4           GWS4\n5           GWS5\n6           GWS6\n7           GWS7\n8           GWS8\n9           GWS9\n10         UWES1\n11         UWES2\n12         UWES3\n13         UWES4\n14         UWES5\n15         UWES6\n16         UWES7\n17         UWES8\n18         UWES9\n19          MBI1\n20          MBI2\n21          MBI3\n22          MBI4\n23          MBI5\n24          MBI6\n25          MBI7\n26          MBI8\n27          MBI9\n28         MBI10\n29         MBI11\n30         MBI12\n31         MBI13\n32         MBI14\n33         MBI15\n34         MBI16\n35          GHQ1\n36          GHQ2\n37          GHQ3\n38          GHQ4\n39          GHQ5\n40          GHQ6\n41          GHQ7\n42          GHQ8\n43          GHQ9\n44         GHQ10\n45         GHQ11\n46         GHQ12\n47       Country\n48           Age\n49        Gender",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html#missing-data",
    "href": "ReadingStoringData.html#missing-data",
    "title": "3  Reading and storing data in R",
    "section": "3.4 Missing data",
    "text": "3.4 Missing data\nIt is relatively common to find missing data in a data set. In Excel, missing data can be represented by an empty cell, by the letters NA, or by a special code such as -999. In the first two cases R will recognise the empty cells or cells containing NAs as missing data. In the third case it is necessary to indicate to read.csv() that the special code represents missing data. Note that in an R data frame missing data will always be represented by NA (an abbreviation for “not available”).\n\nSA_Swiss &lt;- read.csv(\"SA_Swiss.csv\", na.strings=\"-999\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html#reading-an-excel-file",
    "href": "ReadingStoringData.html#reading-an-excel-file",
    "title": "3  Reading and storing data in R",
    "section": "3.5 Reading an Excel file",
    "text": "3.5 Reading an Excel file\nIt is possible to directly read an Excel file without first storing it in csv format. This requires that the readxl package must be installed and activated. Note how a special code such as -999 can be used to indicate missing data. Note that empty cells or NA can also be used to indicate missing data in the Excel file, in which case the read_excel() function will automatically recognise the missing data.\n\n#library(readxl)\n#mydata &lt;- read_excel(\"SA_Swiss.xlsx\", na = \"-999\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html#reading-an-spss-file",
    "href": "ReadingStoringData.html#reading-an-spss-file",
    "title": "3  Reading and storing data in R",
    "section": "3.6 Reading an SPSS file",
    "text": "3.6 Reading an SPSS file\nIt is possible to read an SPSS data file without first storing it in csv format. This requires that the haven package must be installed and activated. The read_spss() function will automatically recognise missing data in an SPSS file, on condition that special codes have been assigned missing data status in the SPSS file.\n\n#library(haven)\n#mydata &lt;- read_spss(\"SA_Swiss.sav\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html#reading-data-from-the-clipboard",
    "href": "ReadingStoringData.html#reading-data-from-the-clipboard",
    "title": "3  Reading and storing data in R",
    "section": "3.7 Reading data from the clipboard",
    "text": "3.7 Reading data from the clipboard\nAt times it might be convenient to highlight and copy data in Excel and then use the read.clipboard() function from the psychTools package to read and store the copied data as a data frame. Whereas this is easy and quick to do it is not recommended for general practice, because the saved code will not explicitly state where the data were obtained from (other than the clipboard). In the example code below I copied some data in Excel and then used the read.clipboard() function to store it as a data frame called “tempdata”. To execute the code users will have to first copy data in Excel, then remove the hash tags at the beginning of the lines, and then run the code.\n\n#library(psychTools)\n#tempdata &lt;- read.clipboard()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "DataManagement.html",
    "href": "DataManagement.html",
    "title": "4  Data management in R",
    "section": "",
    "text": "4.1 Reading the data and storing it as a data frame\nOur first step is to read the data and to store it as a data frame. This needs to be done at the start of each session. If this had been done earlier in the session we do not need to do it again.\nmydata &lt;- read.csv(\"SA_Swiss.csv\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data management in *R*</span>"
    ]
  },
  {
    "objectID": "DataManagement.html#inspecting-the-structure-of-a-data-frame",
    "href": "DataManagement.html#inspecting-the-structure-of-a-data-frame",
    "title": "4  Data management in R",
    "section": "4.2 Inspecting the structure of a data frame",
    "text": "4.2 Inspecting the structure of a data frame\nWhen data are imported from a csv or Excel file, R will (usually) automatically classify variables that contain only numbers as numeric, whereas variables that contain text will (usually) be classified as categorical. In R, a categorical variable is referred to as a factor. There are two types of categorical variables or factors, namely nominal and ordinal.\nWe can inspect the classification of variables in a data frame with the str() function. In the example below we see that the items of the GWSS, the UWES, the MBI, and the GHQ are classified as integers, which is a type of numeric variable. We also note that Gender and Country are classified as integer (numeric) variables. The latter classification is problematic, in the sense that Gender and Country are actually categorical (nominal) variables where the numbers 0 and 1 for Gender indicate “Man” and “Woman”, respectively, and the numbers 1 and 2 for Country indicate “South Africa” and “Switzerland”, respectively.\nChecking if categorical variables were imported as factors is an important and essential step in the data analysis process. Similarly, converting variables that were incorrectly imported as numerical to factor is an essential step.\n\nstr(mydata)\n\n'data.frame':   1377 obs. of  49 variables:\n $ GWS1   : int  2 3 2 2 1 2 1 3 2 2 ...\n $ GWS2   : int  1 3 2 2 1 2 1 3 1 2 ...\n $ GWS3   : int  2 2 2 2 1 2 1 2 1 2 ...\n $ GWS4   : int  1 3 2 3 2 2 1 2 1 2 ...\n $ GWS5   : int  1 3 2 3 2 2 1 2 1 2 ...\n $ GWS6   : int  1 4 3 3 1 3 1 1 1 2 ...\n $ GWS7   : int  1 4 4 1 1 3 2 2 1 2 ...\n $ GWS8   : int  1 3 2 3 1 2 1 2 1 1 ...\n $ GWS9   : int  1 3 3 3 1 1 2 1 1 1 ...\n $ UWES1  : int  5 4 5 4 6 3 3 5 5 5 ...\n $ UWES2  : int  5 4 5 4 6 3 4 5 5 5 ...\n $ UWES3  : int  5 4 6 4 6 4 6 5 4 6 ...\n $ UWES4  : int  5 4 6 4 6 4 6 5 3 6 ...\n $ UWES5  : int  5 4 5 3 5 4 5 6 5 5 ...\n $ UWES6  : int  5 5 6 4 5 4 5 4 5 6 ...\n $ UWES7  : int  5 5 6 4 6 4 6 7 5 6 ...\n $ UWES8  : int  4 5 5 4 6 4 5 5 4 7 ...\n $ UWES9  : int  4 5 4 4 3 4 5 5 4 3 ...\n $ MBI1   : int  1 2 1 1 1 1 1 1 3 1 ...\n $ MBI2   : int  1 2 1 1 1 1 1 1 2 2 ...\n $ MBI3   : int  1 2 2 1 1 3 1 5 2 6 ...\n $ MBI4   : int  1 1 1 1 1 2 1 2 2 1 ...\n $ MBI5   : int  1 1 1 1 1 1 1 1 2 1 ...\n $ MBI6   : int  1 3 1 2 1 2 2 2 1 1 ...\n $ MBI7   : int  1 3 1 1 1 2 2 3 1 1 ...\n $ MBI8   : int  1 1 1 1 1 2 1 2 1 2 ...\n $ MBI9   : int  1 2 1 1 1 3 1 2 1 2 ...\n $ MBI10  : int  1 2 1 1 1 2 1 2 1 2 ...\n $ MBI11  : int  4 3 3 4 7 4 2 2 2 2 ...\n $ MBI12  : int  3 3 3 4 7 3 2 1 2 1 ...\n $ MBI13  : int  2 3 3 4 7 3 2 1 1 1 ...\n $ MBI14  : int  1 3 2 4 7 3 3 1 1 1 ...\n $ MBI15  : int  1 3 3 4 7 4 3 2 2 2 ...\n $ MBI16  : int  1 3 2 4 7 4 2 2 1 1 ...\n $ GHQ1   : int  1 1 1 1 2 2 2 2 2 2 ...\n $ GHQ2   : int  1 2 2 2 1 1 1 1 1 1 ...\n $ GHQ3   : int  2 1 1 2 1 1 1 2 2 2 ...\n $ GHQ4   : int  2 1 1 2 1 1 2 2 2 2 ...\n $ GHQ5   : int  1 2 3 2 1 2 1 1 1 2 ...\n $ GHQ6   : int  4 1 2 1 4 1 1 1 1 1 ...\n $ GHQ7   : int  1 2 2 2 1 2 2 2 2 3 ...\n $ GHQ8   : int  1 2 1 2 2 2 2 2 2 2 ...\n $ GHQ9   : int  1 2 2 1 1 1 1 1 2 1 ...\n $ GHQ10  : int  1 2 1 1 1 1 1 1 2 1 ...\n $ GHQ11  : int  1 1 1 1 1 1 1 1 1 1 ...\n $ GHQ12  : int  1 1 2 2 1 1 2 2 2 1 ...\n $ Country: int  2 2 2 2 2 2 2 2 2 2 ...\n $ Age    : num  35 35 22 38 20 25 58 26 28 34 ...\n $ Gender : int  1 0 0 1 0 1 1 0 1 0 ...\n\n\nTo ensure proper statistical analyses, we need to convert Gender and Country from numeric variables to factors. The code in the example below instructs R that the Gender variable in the mydata data frame should be treated as a factor, and that the newly converted Gender variable should be stored with the same name as the existing one (i.e. the new variable overwrites the old variable). The second line of code is used to check whether the newly overwritten Gender variable (mydata$Gender) actually is a factor.\n\nmydata$Gender &lt;- factor(mydata$Gender)\nis.factor(mydata$Gender)\n\n[1] TRUE\n\n\nIt is good practice, but not necessary, to assign labels to the numbers of a categorical variable or factor. In the present example we could assign the label “Man” to the value of 0, and the label “Woman” to the value of 1. To assign the labels we (a) instruct R what the levels of the categorical variable are, and (b) what the labels of each level are.\n\nmydata$Gender &lt;- factor(mydata$Gender, levels = c(\"0\", \"1\"), labels = c(\"Man\", \"Woman\"))\nis.factor(mydata$Gender)\n\n[1] TRUE\n\n\nWe now do the same with the Country variable (mydata$Country).\n\nmydata$Country &lt;- factor(mydata$Country, levels = c(\"1\", \"2\"), labels = c(\"ZAR\", \"CHE\"))\nis.factor(mydata$Country)\n\n[1] TRUE\n\n\nWe can now ask for a summary of basic descriptive statistics of all the variables in the data frame by using the summary() function. This function will return for each numeric variable the minimum and maximum values, the first and third quartiles, the median, and the mean. For each categorical variable or factor the frequencies of the different levels of the variable are returned.\n\nsummary(mydata)\n\n      GWS1            GWS2            GWS3            GWS4      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  \n Median :3.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :2.601   Mean   :2.145   Mean   :2.226   Mean   :2.243  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000  \n Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :5.000  \n      GWS5            GWS6            GWS7            GWS8      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:2.000   1st Qu.:1.000  \n Median :2.000   Median :2.000   Median :3.000   Median :2.000  \n Mean   :2.041   Mean   :2.078   Mean   :2.667   Mean   :1.969  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000  \n Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :5.000  \n      GWS9           UWES1           UWES2           UWES3          UWES4     \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.00   Min.   :1.00  \n 1st Qu.:1.000   1st Qu.:4.000   1st Qu.:4.000   1st Qu.:4.00   1st Qu.:4.00  \n Median :2.000   Median :5.000   Median :5.000   Median :5.00   Median :5.00  \n Mean   :1.993   Mean   :4.698   Mean   :4.892   Mean   :5.24   Mean   :4.99  \n 3rd Qu.:3.000   3rd Qu.:6.000   3rd Qu.:6.000   3rd Qu.:6.00   3rd Qu.:6.00  \n Max.   :5.000   Max.   :7.000   Max.   :7.000   Max.   :7.00   Max.   :7.00  \n     UWES5           UWES6           UWES7           UWES8      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:4.000   1st Qu.:5.000   1st Qu.:5.000   1st Qu.:4.000  \n Median :5.000   Median :6.000   Median :6.000   Median :5.000  \n Mean   :4.808   Mean   :5.373   Mean   :5.586   Mean   :4.914  \n 3rd Qu.:6.000   3rd Qu.:6.000   3rd Qu.:7.000   3rd Qu.:6.000  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000  \n     UWES9            MBI1            MBI2            MBI3      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:4.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:2.000  \n Median :5.000   Median :2.000   Median :2.000   Median :4.000  \n Mean   :4.754   Mean   :2.549   Mean   :2.741   Mean   :3.774  \n 3rd Qu.:6.000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:5.000  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000  \n      MBI4            MBI5            MBI6            MBI7      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:3.000  \n Median :3.000   Median :2.000   Median :3.000   Median :4.000  \n Mean   :2.873   Mean   :2.603   Mean   :3.001   Mean   :3.741  \n 3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:5.000  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000  \n      MBI8            MBI9           MBI10           MBI11      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000  \n Median :3.000   Median :3.000   Median :3.000   Median :3.000  \n Mean   :3.165   Mean   :2.895   Mean   :2.914   Mean   :2.675  \n 3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:3.000  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000  \n     MBI12           MBI13           MBI14           MBI15      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:1.000   1st Qu.:2.000   1st Qu.:2.000  \n Median :3.000   Median :2.000   Median :3.000   Median :3.000  \n Mean   :2.699   Mean   :2.261   Mean   :3.103   Mean   :2.788  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000  \n     MBI16            GHQ1            GHQ2            GHQ3      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:1.000   1st Qu.:2.000  \n Median :2.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :2.466   Mean   :2.104   Mean   :1.987   Mean   :1.977  \n 3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :7.000   Max.   :4.000   Max.   :4.000   Max.   :4.000  \n      GHQ4            GHQ5            GHQ6            GHQ7      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:1.000   1st Qu.:2.000  \n Median :2.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :1.954   Mean   :2.192   Mean   :1.938   Mean   :2.133  \n 3rd Qu.:2.000   3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :4.000  \n      GHQ8            GHQ9           GHQ10           GHQ11      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  \n Median :2.000   Median :2.000   Median :1.000   Median :1.000  \n Mean   :2.004   Mean   :1.933   Mean   :1.676   Mean   :1.441  \n 3rd Qu.:2.000   3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :4.000  \n     GHQ12       Country        Age          Gender   \n Min.   :1.000   ZAR:584   Min.   :16.00   Man  :607  \n 1st Qu.:2.000   CHE:793   1st Qu.:26.00   Woman:770  \n Median :2.000             Median :33.00              \n Mean   :1.992             Mean   :36.44              \n 3rd Qu.:2.000             3rd Qu.:47.00              \n Max.   :4.000             Max.   :65.00              \n\n\n\\[ C(n, k) = \\frac{n!}{k!(n-k)!} \\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data management in *R*</span>"
    ]
  },
  {
    "objectID": "Subsetting.html",
    "href": "Subsetting.html",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "",
    "text": "6 Using dplyr to filter rows from a data frame",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#filtering-rows-by-consecutive-row-numbers",
    "href": "Subsetting.html#filtering-rows-by-consecutive-row-numbers",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "6.1 Filtering rows by consecutive row numbers",
    "text": "6.1 Filtering rows by consecutive row numbers\n\nmybfi %&gt;% slice(1:10)\n\n   A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1   1  6  6  6  6  2  5  4  2  2      2         5  38\n2   3  4  4  3  4  4  5  3  6  2      1         1  43\n3   5  6  3  6  3  2  3  3  2  2      2         2  28\n4   3  5  6  6  5  1  1  2  3  2      2         3  19\n5   5  5  5  6  6  1  1  2  5  1      2         3  46\n6   4  5  4  4  5  4  2  1  1  5      2         3  20\n7   1  6  5  5  5  1  1  2  4  2      2         4  29\n8   2  6  5  6  5  5  5  5  5  5      2         1  19\n9   3  4  4  6  5  2  3  3  2  2      2         3  28\n10  6  1  2  2  2  2  2  2  3  1      1         3  29",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#filtering-cases-by-non-consecutive-row-numbers",
    "href": "Subsetting.html#filtering-cases-by-non-consecutive-row-numbers",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "6.2 Filtering cases by non-consecutive row numbers",
    "text": "6.2 Filtering cases by non-consecutive row numbers\n\nmybfi %&gt;% slice(1, 3, 5, 7)\n\n  A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1  1  6  6  6  6  2  5  4  2  2      2         5  38\n2  5  6  3  6  3  2  3  3  2  2      2         2  28\n3  5  5  5  6  6  1  1  2  5  1      2         3  46\n4  1  6  5  5  5  1  1  2  4  2      2         4  29",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#filtering-cases-that-meet-a-certain-condition",
    "href": "Subsetting.html#filtering-cases-that-meet-a-certain-condition",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "6.3 Filtering cases that meet a certain condition",
    "text": "6.3 Filtering cases that meet a certain condition\n\nmybfi %&gt;% filter(gender == 1)\n\nmybfi %&gt;% filter(education &gt; 3)\n\nmybfi %&gt;% filter(age == 30)\n\nmybfi %&gt;% filter(age &lt;= 30)\n\nmybfi %&gt;% filter(age &gt;= 31)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#filtering-cases-that-dont-meet-a-certain-condition",
    "href": "Subsetting.html#filtering-cases-that-dont-meet-a-certain-condition",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "6.4 Filtering cases that don’t meet a certain condition",
    "text": "6.4 Filtering cases that don’t meet a certain condition\n\nmybfi %&gt;% filter(!gender == 1)\n\nmybfi %&gt;% filter(!education &gt; 3)\n\nmybfi %&gt;% filter(!age &gt; 30)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#filtering-cases-on-the-basis-of-two-or-more-conditions",
    "href": "Subsetting.html#filtering-cases-on-the-basis-of-two-or-more-conditions",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "6.5 Filtering cases on the basis of two or more conditions",
    "text": "6.5 Filtering cases on the basis of two or more conditions\n\nmybfi %&gt;% filter(gender == 1 & age &gt; 60)\n\n  A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1  5  4  2  6  4  5  5  4  4  4      1         2  65\n2  4  3  1  1  5  4  2  1  2  1      1         5  74\n3  2  4  4  4  4  1  1  1  1  1      1         5  67\n4  1  5  6  5  6  2  2  2  2  2      1         5  68\n\nmybfi %&gt;% \n  filter(gender == 1 | age &gt; 60) %&gt;% \n  slice_sample(n = 20)\n\n   A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1   4  5  5  6  5  2  3  3  1  1      1         1  18\n2   4  5  4  2  4  3  5  4  3  1      1         3  23\n3   4  5  2  2  1  2  4  2  2  3      1         1  17\n4   2  6  5  6  5  1  1  1  6  1      1         5  21\n5   1  4  6  6  2  2  3  2  6  6      1         3  19\n6   6  4  5  5  5  1  1  3  1  1      1         3  23\n7   4  5  4  5  5  2  4  2  1  4      1         3  35\n8   3  3  5  4  5  1  2  1  4  1      1         5  29\n9   1  6  6  6  6  1  1  4  4  1      1         3  21\n10  3  3  2  1  2  5  5  5  5  2      1         3  38\n11  3  2  2  4  4  2  3  2  5  2      1         4  51\n12  2  5  4  5  5  4  3  2  2  2      1         3  19\n13  2  3  4  1  2  1  1  2  2  2      1         2  18\n14  4  5  3  2  3  3  5  4  5  5      1         3  20\n15  4  2  1  2  2  2  4  2  4  2      1         5  23\n16  2  5  6  5  5  3  4  1  2  1      1         2  31\n17  2  6  5  3  5  6  6  6  6  6      1         3  19\n18  1  5  5  6  5  3  3  1  2  1      1         3  27\n19  1  5  5  4  4  3  4  4  3  2      1         3  21\n20  1  6  6  6  6  1  1  1  1  1      2         3  63",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#storing-the-filtered-cases-in-a-new-data-frame",
    "href": "Subsetting.html#storing-the-filtered-cases-in-a-new-data-frame",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "6.6 Storing the filtered cases in a new data frame",
    "text": "6.6 Storing the filtered cases in a new data frame\n\ndf_new &lt;- mybfi %&gt;% \n  filter(age &gt; 28 & education &gt;= 3) %&gt;% \n  slice(1:15) %&gt;% \n  arrange(age)\n\ndf_new\n\n   A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1   1  6  5  5  5  1  1  2  4  2      2         4  29\n2   6  1  2  2  2  2  2  2  3  1      1         3  29\n3   3  4  5  6  3  2  2  2  1  1      1         3  29\n4   4  5  4  5  4  3  4  4  4  4      2         3  30\n5   3  4  3  4  3  3  4  3  4  5      2         5  31\n6   2  2  1  6  3  1  3  1  1  1      2         5  31\n7   6  2  5  6  2  6  6  6  5  6      2         3  32\n8   4  4  4  4  4  4  4  4  4  4      1         4  33\n9   4  4  5  4  5  4  5  5  4  5      2         3  37\n10  1  6  6  6  6  2  5  4  2  2      2         5  38\n11  2  4  4  3  5  1  1  1  1  2      2         5  38\n12  3  6  5  6  5  3  3  2  3  4      2         3  40\n13  5  5  5  6  6  1  1  2  5  1      2         3  46\n14  2  6  5  5  5  1  1  1  1  2      2         4  51\n15  2  6  5  6  5  2  4  2  4  2      2         5  52",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#combining-select-and-slice",
    "href": "Subsetting.html#combining-select-and-slice",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "7.1 Combining select and slice",
    "text": "7.1 Combining select and slice\n\nmybfi %&gt;%\n  select(A1:5) %&gt;% \n  slice(1,3,5)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#combing-select-and-filter",
    "href": "Subsetting.html#combing-select-and-filter",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "7.2 Combing select and filter",
    "text": "7.2 Combing select and filter\n\nmybfi %&gt;%\n  filter(gender == 2) %&gt;% \n  slice(1:10) %&gt;% \n  select(A1:A5)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#randomly-select-rows-from-a-data-frame-without-replacement",
    "href": "Subsetting.html#randomly-select-rows-from-a-data-frame-without-replacement",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "7.3 Randomly select rows from a data frame without replacement",
    "text": "7.3 Randomly select rows from a data frame without replacement\n\nmybfi %&gt;% \n  sample_n(10)\n\n   A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1   4  4  5  4  5  5  5  6  4  5      2         4  21\n2   1  6  6  6  6  4  4  5  5  5      2         2  45\n3   3  4  4  4  4  3  4  4  4  4      1         4  24\n4   2  5  5  3  4  3  4  2  4  1      2         5  31\n5   3  6  5  6  6  2  2  1  2  3      2         3  38\n6   2  4  4  4  5  3  3  3  4  2      2         3  35\n7   2  6  6  6  6  5  5  5  4  2      1         5  29\n8   5  4  1  2  2  2  3  2  2  2      2         4  24\n9   1  5  5  5  5  1  4  2  5  3      2         1  44\n10  2  6  5  6  4  3  5  2  2  2      2         1  19",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#randomly-select-rows-from-a-data-frame-with-replacement",
    "href": "Subsetting.html#randomly-select-rows-from-a-data-frame-with-replacement",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "7.4 Randomly select rows from a data frame with replacement",
    "text": "7.4 Randomly select rows from a data frame with replacement\n\nmybfi %&gt;% \n  sample_n(10, replace = TRUE)\n\n   A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1   1  6  5  4  6  4  4  4  4  4      1         5  54\n2   2  5  6  5  6  4  4  2  2  2      1         5  25\n3   2  4  4  4  3  3  3  4  5  5      2         5  29\n4   3  4  5  6  5  3  3  3  2  3      2         3  21\n5   3  4  4  4  4  6  6  5  5  6      2         2  23\n6   1  5  6  6  6  2  4  2  3  3      2         3  41\n7   1  6  6  6  5  2  3  2  1  2      2         3  49\n8   2  5  5  4  6  2  4  2  2  1      2         4  24\n9   2  5  4  5  5  4  3  2  2  2      1         3  19\n10  2  3  4  5  6  1  2  2  4  2      2         5  36",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#randomly-select-a-fraction-of-rows-without-replacement",
    "href": "Subsetting.html#randomly-select-a-fraction-of-rows-without-replacement",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "7.5 Randomly select a fraction of rows without replacement",
    "text": "7.5 Randomly select a fraction of rows without replacement\n\nmybfi %&gt;% \n  sample_frac(.01)\n\n   A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1   4  5  3  4  4  3  2  2  4  2      1         4  32\n2   4  4  4  4  4  3  5  3  3  4      1         3  36\n3   3  5  5  5  3  5  4  4  4  4      2         3  26\n4   3  5  5  6  1  6  6  6  5  1      1         2  25\n5   2  4  4  4  2  2  3  5  6  2      2         3  20\n6   3  6  5  6  6  2  2  1  2  3      2         3  38\n7   3  4  4  5  5  5  5  6  4  4      2         3  25\n8   2  4  4  4  5  4  5  6  4  4      2         3  50\n9   3  3  5  4  5  1  2  1  4  1      1         5  29\n10  2  6  5  3  5  6  6  6  6  6      1         3  19",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#randomly-select-a-fraction-of-rows-with-replacement",
    "href": "Subsetting.html#randomly-select-a-fraction-of-rows-with-replacement",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "7.6 Randomly select a fraction of rows with replacement",
    "text": "7.6 Randomly select a fraction of rows with replacement\n\nmybfi %&gt;% \n  sample_frac(0.01, replace = TRUE)\n\n   A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1   4  4  5  4  5  4  5  5  4  5      2         3  37\n2   1  6  6  5  6  1  5  1  1  3      1         3  32\n3   3  5  5  5  4  2  1  2  2  3      2         3  23\n4   2  6  6  5  5  1  2  2  2  1      2         2  27\n5   5  2  2  4  4  5  5  6  4  1      1         4  31\n6   4  4  4  3  4  2  2  3  3  3      1         1  19\n7   2  5  5  6  5  1  2  2  5  5      2         5  32\n8   2  5  4  5  5  3  4  2  3  3      2         3  18\n9   1  5  4  4  5  2  2  4  4  2      2         3  39\n10  5  4  3  4  5  3  2  3  5  3      1         2  22",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#randomly-split-a-data-frame-into-two-parts",
    "href": "Subsetting.html#randomly-split-a-data-frame-into-two-parts",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "7.7 Randomly split a data frame into two parts",
    "text": "7.7 Randomly split a data frame into two parts\n\n## Specify exactly how many persons you want in the two groups, respectively.\n## Here we want n = 600 in the first group and n = 400 in the second.\nv          &lt;- as.vector(c(rep(TRUE, 800), rep(FALSE, 200))) \n\nselection  &lt;- sample(v) \n\n\nmybfi1     &lt;- mybfi %&gt;%\n  filter(selection) \n\nmybfi2     &lt;- mybfi %&gt;% \n  filter(!selection) \n  \n\nnrow(mybfi1)\n\n[1] 800\n\nnrow(mybfi2)\n\n[1] 200",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#excercises",
    "href": "Subsetting.html#excercises",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "7.8 Excercises",
    "text": "7.8 Excercises\n\nFirst use dollar notation and then square brackets to find the mean and standard deviation of variable A3.\n\n\nmean(mybfi$A1)\n\n[1] 2.417\n\nsd(mybfi$A1)\n\n[1] 1.364908\n\nmean(mybfi[, 1])\n\n[1] 2.417\n\nsd(mybfi[, 1])\n\n[1] 1.364908\n\nmean(mybfi[, \"A1\"])\n\n[1] 2.417\n\nsd(mybfi[, \"A1\"])\n\n[1] 1.364908\n\n\n\nFirst use dollar notation and then square brackets to find the correlation of variables N1 and N3.\n\n\ncor(mybfi$N1, mybfi$N3)\n\n[1] 0.5668972\n\ncor(mybfi[, c(6, 8)])\n\n          N1        N3\nN1 1.0000000 0.5668972\nN3 0.5668972 1.0000000\n\ncor(mybfi[, c(\"N1\",\"N3\")])\n\n          N1        N3\nN1 1.0000000 0.5668972\nN3 0.5668972 1.0000000\n\n\n\nUse square bracket notation to find the correlations of the five agreeableness items for the first 500 persons.\n\n\ncor(mybfi[1:500, 1:5])\n\n            A1         A2         A3          A4         A5\nA1  1.00000000 -0.2483400 -0.2690251 -0.09381878 -0.1578184\nA2 -0.24833996  1.0000000  0.4972103  0.24329958  0.3640005\nA3 -0.26902508  0.4972103  1.0000000  0.28655854  0.5043441\nA4 -0.09381878  0.2432996  0.2865585  1.00000000  0.2420691\nA5 -0.15781837  0.3640005  0.5043441  0.24206907  1.0000000\n\ncor(mybfi[1:500, c(\"A1\", \"A2\", \"A3\", \"A4\", \"A5\")])\n\n            A1         A2         A3          A4         A5\nA1  1.00000000 -0.2483400 -0.2690251 -0.09381878 -0.1578184\nA2 -0.24833996  1.0000000  0.4972103  0.24329958  0.3640005\nA3 -0.26902508  0.4972103  1.0000000  0.28655854  0.5043441\nA4 -0.09381878  0.2432996  0.2865585  1.00000000  0.2420691\nA5 -0.15781837  0.3640005  0.5043441  0.24206907  1.0000000\n\n\n\nUse the dplyr package with the pipe operator to randomly select 500 cases from the mybfi data frame and then use the psych package to find the correlations of the five agreeableness variables.\n\n\nmybfi %&gt;% \n  sample_n(500) %&gt;% \n  select(A1:A5) %&gt;% \n  cor()\n\n           A1         A2         A3         A4         A5\nA1  1.0000000 -0.2927503 -0.2192822 -0.1526881 -0.1864511\nA2 -0.2927503  1.0000000  0.4665397  0.2883908  0.3677388\nA3 -0.2192822  0.4665397  1.0000000  0.2927107  0.4893307\nA4 -0.1526881  0.2883908  0.2927107  1.0000000  0.2876262\nA5 -0.1864511  0.3677388  0.4893307  0.2876262  1.0000000\n\n\n\nRandomly split (without replacement) the mybfi data frame into two data frames where the first data frame contains 800 cases and the second 200 cases. Store the first data frame as mybfi1 and the second as mybfi2. Next, use the pipe operator to select variables A1, A3, N2 and N4, and then find the correlations of these variables. Do this for each of the two data frames. Note that the corresponding correlations are not exactly the same across the two data sets, which illustrates the role of sampling error.\n\n\n## Specify exactly how many persons you want in the two groups, respectively.\n## Here we want n = 600 in the first group and n = 400 in the second.\nv          &lt;- as.vector(c(rep(TRUE, 800), rep(FALSE, 200))) \n\nselection  &lt;- sample(v) \n\n\nmybfi1     &lt;- mybfi %&gt;%\n  filter(selection) \n\nmybfi2     &lt;- mybfi %&gt;% \n  filter(!selection) \n  \nmybfi1 %&gt;% \n  select(A1, A3, N2, N4) %&gt;% \n  cor()\n\n            A1          A3          N2          N4\nA1  1.00000000 -0.22551986  0.13170547  0.05135981\nA3 -0.22551986  1.00000000 -0.06345315 -0.11259937\nN2  0.13170547 -0.06345315  1.00000000  0.40532681\nN4  0.05135981 -0.11259937  0.40532681  1.00000000\n\nmybfi2 %&gt;% \n  select(A1, A3, N2, N4) %&gt;% \n  cor()\n\n            A1          A3          N2         N4\nA1  1.00000000 -0.30073942  0.09612764  0.1503877\nA3 -0.30073942  1.00000000 -0.05156157 -0.1245821\nN2  0.09612764 -0.05156157  1.00000000  0.3937043\nN4  0.15038773 -0.12458213  0.39370428  1.0000000\n\n\n\nUse the dplyr package to select the rows of all the women (coded as 2 in the gender column). Then find the correlations of the variables N1 to N5. Do the same for the men.\n\n\nmybfi %&gt;% \n  filter(gender == 2) %&gt;% \n  select(N1:N5) %&gt;% \n  cor()\n\n          N1        N2        N3        N4        N5\nN1 1.0000000 0.7186166 0.6074627 0.4418162 0.4586262\nN2 0.7186166 1.0000000 0.5678481 0.4180748 0.4185746\nN3 0.6074627 0.5678481 1.0000000 0.5695031 0.4794441\nN4 0.4418162 0.4180748 0.5695031 1.0000000 0.4509958\nN5 0.4586262 0.4185746 0.4794441 0.4509958 1.0000000\n\nmybfi %&gt;% \n  filter(gender == 1) %&gt;% \n  select(N1:N5) %&gt;% \n  cor()\n\n          N1        N2        N3        N4        N5\nN1 1.0000000 0.6280840 0.4856439 0.3610567 0.3450229\nN2 0.6280840 1.0000000 0.4703934 0.3837679 0.3303561\nN3 0.4856439 0.4703934 1.0000000 0.5797286 0.3641473\nN4 0.3610567 0.3837679 0.5797286 1.0000000 0.3891029\nN5 0.3450229 0.3303561 0.3641473 0.3891029 1.0000000\n\n\n\nUse the dplyr package to select the rows of the men who (a) have education levels of 3 or above and (b) are older than 40. Then find the correlations of variables N1 to N5.\n\n\nmybfi %&gt;% \n  filter(gender == 1 & education &gt;= 3 & age &gt; 40) %&gt;% \n  select(N1:N5) %&gt;% \n  cor()\n\n          N1        N2        N3        N4        N5\nN1 1.0000000 0.7779814 0.7339804 0.5793051 0.6130807\nN2 0.7779814 1.0000000 0.6387199 0.4741402 0.5535637\nN3 0.7339804 0.6387199 1.0000000 0.6308001 0.6337416\nN4 0.5793051 0.4741402 0.6308001 1.0000000 0.5062702\nN5 0.6130807 0.5535637 0.6337416 0.5062702 1.0000000\n\n\n\n7.8.1 Selecting consecutive rows by their row numbers with square brackets\nWe can select rows of a data frame by their row numbers using square brackets. Any particular cell in a data frame can be indexed by its row number and its column number, for instance the score of the person in row one on the variable in column one has the index mybfi[1, 1]. We first type the name of the data frame and then the square brackets with the row and column numbers. Note that we place a comma in the square brackets to separate the row and column numbers. The value before the comma pertains to the row number, whereas the value after the comma pertains to the column number. So, the score of the person in row five on the variable in column six of the mybfi data frame is indexed by mybfi[5, 6].\nIf we want to select a row and include all the columns we leave the space after the comma empty. So if we want to use the scores of the person in the sixth row of the data frame on all the variables we would do it as follows: mybfi[6, ].\nIn the example below we select the persons in rows 1 to 20, using all the variables.\n\nmybfi[1:10, ]\n\n   A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1   1  6  6  6  6  2  5  4  2  2      2         5  38\n2   3  4  4  3  4  4  5  3  6  2      1         1  43\n3   5  6  3  6  3  2  3  3  2  2      2         2  28\n4   3  5  6  6  5  1  1  2  3  2      2         3  19\n5   5  5  5  6  6  1  1  2  5  1      2         3  46\n6   4  5  4  4  5  4  2  1  1  5      2         3  20\n7   1  6  5  5  5  1  1  2  4  2      2         4  29\n8   2  6  5  6  5  5  5  5  5  5      2         1  19\n9   3  4  4  6  5  2  3  3  2  2      2         3  28\n10  6  1  2  2  2  2  2  2  3  1      1         3  29\n\n\n\n\n7.8.2 Selecting non-consecutive rows by their row numbers with square brackets\nWe can select non-consecutive rows of a data frame by concatenating the row numbers inside round brackets, preceded by the letter “c”. For instance, to select rows 2, 4, and 7 of the mybfi data frame we type mybfi[c(2, 4, 7), ]. Note that the space after the comma indicates that we want to include all the variables.\nIn the example below we select rows 1, 3, 5, 7 and 9 from the bfi data frame.\n\nmybfi[c(1, 3, 5, 7, 9), ]\n\n  A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1  1  6  6  6  6  2  5  4  2  2      2         5  38\n3  5  6  3  6  3  2  3  3  2  2      2         2  28\n5  5  5  5  6  6  1  1  2  5  1      2         3  46\n7  1  6  5  5  5  1  1  2  4  2      2         4  29\n9  3  4  4  6  5  2  3  3  2  2      2         3  28",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#selecting-columns-variables-from-a-data-frame",
    "href": "Subsetting.html#selecting-columns-variables-from-a-data-frame",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "7.9 Selecting columns (variables) from a data frame",
    "text": "7.9 Selecting columns (variables) from a data frame\n\n7.9.1 Selecting a variable with $ notation\nWe can select a variable from a data frame using dollar notation by first typing the name of the data frame, second typing a $ symbol, and then typing the name of the variable. For instance, we select variable N1 of the mybfi data frame as follows: mybfi$N1\nIn this example we are selecting the variable A1 from the mybfi data frame. Next, I find the mean and standard deviation of variable A1.\n\nmybfi$A1\n\n   [1] 1 3 5 3 5 4 1 2 3 6 2 3 3 2 1 1 2 1 2 6 1 3 2 3 2 2 4 4 5 2 2 4 4 4 2 3 2\n  [38] 2 4 2 2 4 2 1 2 3 3 2 2 4 1 3 4 1 3 3 1 3 2 4 4 4 3 1 2 2 1 1 2 2 2 1 1 1\n  [75] 1 2 4 1 3 4 1 2 3 3 1 5 1 3 1 1 3 1 5 1 1 4 2 1 1 2 1 3 1 2 4 6 2 1 2 3 1\n [112] 6 3 1 3 2 4 2 1 1 1 2 1 5 2 1 1 1 1 4 3 1 5 2 1 3 4 2 4 1 1 1 2 5 1 4 3 1\n [149] 3 1 2 2 2 1 1 4 1 5 2 4 1 1 5 5 2 3 2 1 2 1 1 2 6 1 6 3 4 2 1 5 4 2 4 2 1\n [186] 2 1 2 1 3 4 3 4 6 1 2 4 5 3 3 1 3 2 5 4 3 1 3 4 4 4 3 4 2 2 1 1 2 1 4 2 1\n [223] 1 3 1 2 1 2 1 2 2 1 3 3 5 4 4 3 5 2 2 5 5 1 2 4 1 4 1 1 1 3 1 1 2 2 2 4 2\n [260] 1 4 4 2 5 1 1 2 5 1 4 6 6 1 1 1 4 2 2 1 2 3 3 2 2 2 1 1 1 5 5 1 2 1 1 1 1\n [297] 3 2 4 2 2 2 1 4 1 2 2 4 2 1 3 1 4 1 5 1 4 4 1 2 2 5 4 3 1 2 2 3 3 4 2 3 2\n [334] 1 1 5 3 1 1 1 2 1 2 1 6 2 1 2 1 1 5 2 1 2 1 3 3 2 1 1 1 3 1 1 2 1 5 2 3 2\n [371] 3 2 2 1 2 2 3 3 2 5 2 5 5 5 1 3 1 2 1 3 4 3 3 2 2 5 3 1 2 2 2 2 3 3 1 6 2\n [408] 2 2 2 5 4 3 5 1 2 1 2 2 1 4 2 2 4 4 2 4 1 3 2 1 1 2 1 2 2 5 1 2 1 1 1 1 1\n [445] 3 6 1 5 2 2 1 1 3 1 1 5 3 3 1 2 2 4 4 1 3 1 1 3 3 2 4 3 2 1 4 1 1 1 2 1 6\n [482] 1 1 2 1 2 2 2 1 3 4 2 2 3 4 2 1 2 4 2 2 1 2 2 4 4 3 1 3 2 4 1 3 2 2 2 1 2\n [519] 3 4 2 4 4 2 2 5 4 2 3 2 1 4 5 4 1 2 1 3 1 5 3 1 3 3 5 1 2 3 2 4 1 5 1 1 3\n [556] 4 2 2 4 2 4 6 3 2 4 1 3 5 1 2 2 1 1 4 4 1 2 1 3 2 3 2 1 2 4 3 2 5 3 1 2 1\n [593] 1 3 4 1 4 3 5 1 2 1 2 1 4 4 1 1 1 3 2 5 3 1 2 1 2 2 2 2 2 3 3 3 4 1 2 2 3\n [630] 6 1 2 2 2 3 4 3 2 1 1 1 4 1 3 2 3 1 5 2 1 3 3 2 1 2 1 3 1 3 1 4 5 1 1 6 1\n [667] 3 2 4 2 2 1 3 2 1 1 2 2 5 2 2 4 3 2 2 2 1 1 1 3 2 1 2 1 3 5 2 1 2 5 5 1 1\n [704] 3 2 4 1 2 1 1 4 2 1 1 1 5 1 2 1 5 5 2 1 1 1 1 1 2 1 4 1 3 2 2 1 1 2 5 1 1\n [741] 4 1 3 4 3 3 4 2 2 5 1 2 1 2 3 1 4 3 1 6 1 2 3 5 2 5 5 2 3 1 4 4 6 4 5 4 2\n [778] 2 4 2 1 2 3 1 3 4 1 4 2 1 2 3 1 2 2 1 1 6 2 1 1 2 1 4 2 2 1 2 1 1 2 2 1 2\n [815] 2 2 5 4 5 4 4 1 1 1 2 4 1 1 1 5 3 1 1 4 6 1 3 2 1 2 1 2 1 1 2 5 2 1 3 2 1\n [852] 2 1 2 4 5 3 1 2 3 2 4 2 2 1 2 3 3 4 2 3 4 1 2 2 2 2 5 5 2 1 1 1 3 2 1 4 2\n [889] 2 2 2 1 3 1 3 1 2 5 1 1 1 2 1 5 5 5 4 2 1 1 1 4 2 2 2 3 3 1 3 3 1 2 3 2 1\n [926] 2 1 3 2 2 2 2 2 2 2 2 1 2 1 2 2 4 2 1 3 3 1 2 2 2 1 5 2 2 1 2 2 1 6 1 2 1\n [963] 4 4 4 4 4 4 4 3 1 3 1 1 4 6 2 1 1 5 1 2 3 1 2 1 3 2 1 4 2 4 4 2 5 5 1 4 1\n[1000] 2\n\nmean(mybfi$A1)\n\n[1] 2.417\n\nsd(mybfi$A1)\n\n[1] 1.364908\n\n\n\n\n7.9.2 Selecting a variable by its column number with square brackets\nWe can select a variable by its column number in the data frame using square brackets. Any particular cell in a data frame can be indexed by its row number and its column number, for instance the score of the person in row one on the variable in column one has the index mybfi[1, 1]. We first type the name of the data frame and then the square brackets with the row and column numbers. Note that we place a comma in the square brackets to separate the row and column numbers. The value before the comma pertains to the row number, whereas the value after the comma pertains to the column number. So, the score of the person in row five on the variable in column six of the mybfi data frame is indexed by mybfi[5, 6].\nIf we want to select a variable and include all the rows we leave the space before the comma empty. So if we want to use the scores of all the persons in the data frame on the variable in column six we would do it as follows: mybfi[, 6].\nIn the example below we select variable A1, which is in the first column, using all the rows.\n\nmybfi[, 1] \n\n   [1] 1 3 5 3 5 4 1 2 3 6 2 3 3 2 1 1 2 1 2 6 1 3 2 3 2 2 4 4 5 2 2 4 4 4 2 3 2\n  [38] 2 4 2 2 4 2 1 2 3 3 2 2 4 1 3 4 1 3 3 1 3 2 4 4 4 3 1 2 2 1 1 2 2 2 1 1 1\n  [75] 1 2 4 1 3 4 1 2 3 3 1 5 1 3 1 1 3 1 5 1 1 4 2 1 1 2 1 3 1 2 4 6 2 1 2 3 1\n [112] 6 3 1 3 2 4 2 1 1 1 2 1 5 2 1 1 1 1 4 3 1 5 2 1 3 4 2 4 1 1 1 2 5 1 4 3 1\n [149] 3 1 2 2 2 1 1 4 1 5 2 4 1 1 5 5 2 3 2 1 2 1 1 2 6 1 6 3 4 2 1 5 4 2 4 2 1\n [186] 2 1 2 1 3 4 3 4 6 1 2 4 5 3 3 1 3 2 5 4 3 1 3 4 4 4 3 4 2 2 1 1 2 1 4 2 1\n [223] 1 3 1 2 1 2 1 2 2 1 3 3 5 4 4 3 5 2 2 5 5 1 2 4 1 4 1 1 1 3 1 1 2 2 2 4 2\n [260] 1 4 4 2 5 1 1 2 5 1 4 6 6 1 1 1 4 2 2 1 2 3 3 2 2 2 1 1 1 5 5 1 2 1 1 1 1\n [297] 3 2 4 2 2 2 1 4 1 2 2 4 2 1 3 1 4 1 5 1 4 4 1 2 2 5 4 3 1 2 2 3 3 4 2 3 2\n [334] 1 1 5 3 1 1 1 2 1 2 1 6 2 1 2 1 1 5 2 1 2 1 3 3 2 1 1 1 3 1 1 2 1 5 2 3 2\n [371] 3 2 2 1 2 2 3 3 2 5 2 5 5 5 1 3 1 2 1 3 4 3 3 2 2 5 3 1 2 2 2 2 3 3 1 6 2\n [408] 2 2 2 5 4 3 5 1 2 1 2 2 1 4 2 2 4 4 2 4 1 3 2 1 1 2 1 2 2 5 1 2 1 1 1 1 1\n [445] 3 6 1 5 2 2 1 1 3 1 1 5 3 3 1 2 2 4 4 1 3 1 1 3 3 2 4 3 2 1 4 1 1 1 2 1 6\n [482] 1 1 2 1 2 2 2 1 3 4 2 2 3 4 2 1 2 4 2 2 1 2 2 4 4 3 1 3 2 4 1 3 2 2 2 1 2\n [519] 3 4 2 4 4 2 2 5 4 2 3 2 1 4 5 4 1 2 1 3 1 5 3 1 3 3 5 1 2 3 2 4 1 5 1 1 3\n [556] 4 2 2 4 2 4 6 3 2 4 1 3 5 1 2 2 1 1 4 4 1 2 1 3 2 3 2 1 2 4 3 2 5 3 1 2 1\n [593] 1 3 4 1 4 3 5 1 2 1 2 1 4 4 1 1 1 3 2 5 3 1 2 1 2 2 2 2 2 3 3 3 4 1 2 2 3\n [630] 6 1 2 2 2 3 4 3 2 1 1 1 4 1 3 2 3 1 5 2 1 3 3 2 1 2 1 3 1 3 1 4 5 1 1 6 1\n [667] 3 2 4 2 2 1 3 2 1 1 2 2 5 2 2 4 3 2 2 2 1 1 1 3 2 1 2 1 3 5 2 1 2 5 5 1 1\n [704] 3 2 4 1 2 1 1 4 2 1 1 1 5 1 2 1 5 5 2 1 1 1 1 1 2 1 4 1 3 2 2 1 1 2 5 1 1\n [741] 4 1 3 4 3 3 4 2 2 5 1 2 1 2 3 1 4 3 1 6 1 2 3 5 2 5 5 2 3 1 4 4 6 4 5 4 2\n [778] 2 4 2 1 2 3 1 3 4 1 4 2 1 2 3 1 2 2 1 1 6 2 1 1 2 1 4 2 2 1 2 1 1 2 2 1 2\n [815] 2 2 5 4 5 4 4 1 1 1 2 4 1 1 1 5 3 1 1 4 6 1 3 2 1 2 1 2 1 1 2 5 2 1 3 2 1\n [852] 2 1 2 4 5 3 1 2 3 2 4 2 2 1 2 3 3 4 2 3 4 1 2 2 2 2 5 5 2 1 1 1 3 2 1 4 2\n [889] 2 2 2 1 3 1 3 1 2 5 1 1 1 2 1 5 5 5 4 2 1 1 1 4 2 2 2 3 3 1 3 3 1 2 3 2 1\n [926] 2 1 3 2 2 2 2 2 2 2 2 1 2 1 2 2 4 2 1 3 3 1 2 2 2 1 5 2 2 1 2 2 1 6 1 2 1\n [963] 4 4 4 4 4 4 4 3 1 3 1 1 4 6 2 1 1 5 1 2 3 1 2 1 3 2 1 4 2 4 4 2 5 5 1 4 1\n[1000] 2\n\nmean(mybfi[, 1])\n\n[1] 2.417\n\nsd(mybfi[, 1])\n\n[1] 1.364908",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#selecting-a-variable-by-its-name-with-square-brackets",
    "href": "Subsetting.html#selecting-a-variable-by-its-name-with-square-brackets",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "7.10 Selecting a variable by its name with square brackets",
    "text": "7.10 Selecting a variable by its name with square brackets\nWe can also use square brackets to select a variable by its name. For instance, to select variable N1 of the mybfi data frame, we type mybfi[, “N1”. Note that we have to type the name of the variable inside quotation marks.\nIn the example below we select variable A1 from the mybfi data frame.\n\nmybfi[, \"A1\"]\n\n   [1] 1 3 5 3 5 4 1 2 3 6 2 3 3 2 1 1 2 1 2 6 1 3 2 3 2 2 4 4 5 2 2 4 4 4 2 3 2\n  [38] 2 4 2 2 4 2 1 2 3 3 2 2 4 1 3 4 1 3 3 1 3 2 4 4 4 3 1 2 2 1 1 2 2 2 1 1 1\n  [75] 1 2 4 1 3 4 1 2 3 3 1 5 1 3 1 1 3 1 5 1 1 4 2 1 1 2 1 3 1 2 4 6 2 1 2 3 1\n [112] 6 3 1 3 2 4 2 1 1 1 2 1 5 2 1 1 1 1 4 3 1 5 2 1 3 4 2 4 1 1 1 2 5 1 4 3 1\n [149] 3 1 2 2 2 1 1 4 1 5 2 4 1 1 5 5 2 3 2 1 2 1 1 2 6 1 6 3 4 2 1 5 4 2 4 2 1\n [186] 2 1 2 1 3 4 3 4 6 1 2 4 5 3 3 1 3 2 5 4 3 1 3 4 4 4 3 4 2 2 1 1 2 1 4 2 1\n [223] 1 3 1 2 1 2 1 2 2 1 3 3 5 4 4 3 5 2 2 5 5 1 2 4 1 4 1 1 1 3 1 1 2 2 2 4 2\n [260] 1 4 4 2 5 1 1 2 5 1 4 6 6 1 1 1 4 2 2 1 2 3 3 2 2 2 1 1 1 5 5 1 2 1 1 1 1\n [297] 3 2 4 2 2 2 1 4 1 2 2 4 2 1 3 1 4 1 5 1 4 4 1 2 2 5 4 3 1 2 2 3 3 4 2 3 2\n [334] 1 1 5 3 1 1 1 2 1 2 1 6 2 1 2 1 1 5 2 1 2 1 3 3 2 1 1 1 3 1 1 2 1 5 2 3 2\n [371] 3 2 2 1 2 2 3 3 2 5 2 5 5 5 1 3 1 2 1 3 4 3 3 2 2 5 3 1 2 2 2 2 3 3 1 6 2\n [408] 2 2 2 5 4 3 5 1 2 1 2 2 1 4 2 2 4 4 2 4 1 3 2 1 1 2 1 2 2 5 1 2 1 1 1 1 1\n [445] 3 6 1 5 2 2 1 1 3 1 1 5 3 3 1 2 2 4 4 1 3 1 1 3 3 2 4 3 2 1 4 1 1 1 2 1 6\n [482] 1 1 2 1 2 2 2 1 3 4 2 2 3 4 2 1 2 4 2 2 1 2 2 4 4 3 1 3 2 4 1 3 2 2 2 1 2\n [519] 3 4 2 4 4 2 2 5 4 2 3 2 1 4 5 4 1 2 1 3 1 5 3 1 3 3 5 1 2 3 2 4 1 5 1 1 3\n [556] 4 2 2 4 2 4 6 3 2 4 1 3 5 1 2 2 1 1 4 4 1 2 1 3 2 3 2 1 2 4 3 2 5 3 1 2 1\n [593] 1 3 4 1 4 3 5 1 2 1 2 1 4 4 1 1 1 3 2 5 3 1 2 1 2 2 2 2 2 3 3 3 4 1 2 2 3\n [630] 6 1 2 2 2 3 4 3 2 1 1 1 4 1 3 2 3 1 5 2 1 3 3 2 1 2 1 3 1 3 1 4 5 1 1 6 1\n [667] 3 2 4 2 2 1 3 2 1 1 2 2 5 2 2 4 3 2 2 2 1 1 1 3 2 1 2 1 3 5 2 1 2 5 5 1 1\n [704] 3 2 4 1 2 1 1 4 2 1 1 1 5 1 2 1 5 5 2 1 1 1 1 1 2 1 4 1 3 2 2 1 1 2 5 1 1\n [741] 4 1 3 4 3 3 4 2 2 5 1 2 1 2 3 1 4 3 1 6 1 2 3 5 2 5 5 2 3 1 4 4 6 4 5 4 2\n [778] 2 4 2 1 2 3 1 3 4 1 4 2 1 2 3 1 2 2 1 1 6 2 1 1 2 1 4 2 2 1 2 1 1 2 2 1 2\n [815] 2 2 5 4 5 4 4 1 1 1 2 4 1 1 1 5 3 1 1 4 6 1 3 2 1 2 1 2 1 1 2 5 2 1 3 2 1\n [852] 2 1 2 4 5 3 1 2 3 2 4 2 2 1 2 3 3 4 2 3 4 1 2 2 2 2 5 5 2 1 1 1 3 2 1 4 2\n [889] 2 2 2 1 3 1 3 1 2 5 1 1 1 2 1 5 5 5 4 2 1 1 1 4 2 2 2 3 3 1 3 3 1 2 3 2 1\n [926] 2 1 3 2 2 2 2 2 2 2 2 1 2 1 2 2 4 2 1 3 3 1 2 2 2 1 5 2 2 1 2 2 1 6 1 2 1\n [963] 4 4 4 4 4 4 4 3 1 3 1 1 4 6 2 1 1 5 1 2 3 1 2 1 3 2 1 4 2 4 4 2 5 5 1 4 1\n[1000] 2\n\nmean(mybfi[, \"A1\"])\n\n[1] 2.417\n\nsd(mybfi[, \"A1\"])\n\n[1] 1.364908",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#selecting-two-or-more-contiguous-variables-by-their-column-numbers-with-square-brackets",
    "href": "Subsetting.html#selecting-two-or-more-contiguous-variables-by-their-column-numbers-with-square-brackets",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "7.11 Selecting two or more contiguous variables by their column numbers with square brackets",
    "text": "7.11 Selecting two or more contiguous variables by their column numbers with square brackets\nIf the columns we want to select are contiguous–i.e. next to each other or together in sequence–we can type the first and last column numbers separated by a colon. For instance, to select the variables in columns three to six of the mybfi data frame we would type mybfi[, 3:6].\nIn the example below we select the variables in columns one to five. Next we ask for descriptive statistics of these variables.\n\nmybfi[, 1:5]\n\n     A1 A2 A3 A4 A5\n1     1  6  6  6  6\n2     3  4  4  3  4\n3     5  6  3  6  3\n4     3  5  6  6  5\n5     5  5  5  6  6\n6     4  5  4  4  5\n7     1  6  5  5  5\n8     2  6  5  6  5\n9     3  4  4  6  5\n10    6  1  2  2  2\n11    2  5  5  5  5\n12    3  6  5  6  5\n13    3  5  4  6  5\n14    2  6  6  5  6\n15    1  2  1  4  2\n16    1  6  5  4  5\n17    2  5  5  5  5\n18    1  4  3  5  3\n19    2  6  5  3  4\n20    6  2  5  6  2\n21    1  5  3  4  4\n22    3  6  6  6  5\n23    2  4  3  3  3\n24    3  4  3  4  3\n25    2  4  4  4  2\n26    2  4  5  5  5\n27    4  6  5  6  4\n28    4  6  6  4  4\n29    5  4  3  5  4\n30    2  2  1  6  3\n31    2  3  4  4  4\n32    4  5  4  5  4\n33    4  4  5  4  5\n34    4  4  6  6  5\n35    2  6  6  6  6\n36    3  4  4  4  5\n37    2  6  5  6  5\n38    2  2  1  2  3\n39    4  4  4  4  4\n40    2  5  4  5  5\n41    2  4  4  3  5\n42    4  6  4  6  4\n43    2  6  5  5  5\n44    1  5  6  5  3\n45    2  6  5  3  4\n46    3  4  4  4  4\n47    3  4  5  6  3\n48    2  5  5  5  4\n49    2  3  4  6  4\n50    4  6  4  6  3\n51    1  6  5  6  5\n52    3  4  5  6  5\n53    4  5  4  5  2\n54    1  6  6  1  6\n55    3  5  3  5  3\n56    3  5  5  5  5\n57    1  5  4  3  5\n58    3  3  1  3  2\n59    2  5  5  5  3\n60    4  4  6  5  4\n61    4  5  6  6  5\n62    4  5  5  5  5\n63    3  4  2  4  4\n64    1  5  6  5  2\n65    2  6  6  4  6\n66    2  5  6  6  5\n67    1  6  5  5  5\n68    1  5  4  6  5\n69    2  5  5  5  5\n70    2  4  3  5  5\n71    2  6  6  2  4\n72    1  5  5  5  5\n73    1  6  6  6  6\n74    1  6  3  5  6\n75    1  6  6  6  6\n76    2  6  6  4  2\n77    4  5  5  4  5\n78    1  6  6  6  6\n79    3  4  5  4  6\n80    4  6  6  2  5\n81    1  6  6  6  6\n82    2  5  6  6  6\n83    3  5  5  6  1\n84    3  6  5  4  4\n85    1  6  6  4  6\n86    5  5  4  3  4\n87    1  5  6  5  5\n88    3  5  6  6  5\n89    1  6  1  6  6\n90    1  6  6  5  6\n91    3  5  5  6  6\n92    1  6  6  2  5\n93    5  2  1  1  1\n94    1  2  2  6  2\n95    1  5  5  6  4\n96    4  6  2  2  4\n97    2  6  5  6  6\n98    1  5  6  4  6\n99    1  6  5  5  4\n100   2  5  4  6  5\n101   1  6  6  5  6\n102   3  5  5  5  2\n103   1  6  5  1  3\n104   2  4  6  4  5\n105   4  5  5  6  4\n106   6  6  6  6  6\n107   2  6  5  6  6\n108   1  6  4  6  4\n109   2  4  4  2  6\n110   3  5  4  5  5\n111   1  4  5  6  4\n112   6  4  4  6  6\n113   3  5  5  5  5\n114   1  6  6  2  6\n115   3  4  4  6  3\n116   2  2  5  3  4\n117   4  4  5  6  5\n118   2  5  5  2  5\n119   1  5  4  2  5\n120   1  5  5  6  6\n121   1  5  5  5  6\n122   2  5  3  5  6\n123   1  6  6  6  4\n124   5  4  2  4  2\n125   2  4  4  3  5\n126   1  4  6  6  6\n127   1  6  6  6  5\n128   1  6  5  6  4\n129   1  6  5  6  4\n130   4  6  6  1  6\n131   3  6  3  3  4\n132   1  6  6  1  1\n133   5  6  6  6  5\n134   2  4  5  5  5\n135   1  6  6  6  6\n136   3  5  5  3  5\n137   4  4  5  5  5\n138   2  5  6  4  4\n139   4  5  3  5  4\n140   1  6  6  4  5\n141   1  6  5  6  5\n142   1  5  5  5  5\n143   2  5  6  4  6\n144   5  4  3  4  3\n145   1  5  5  6  5\n146   4  4  4  5  5\n147   3  3  5  5  5\n148   1  6  6  6  6\n149   3  2  3  5  3\n150   1  4  6  5  6\n151   2  3  5  4  5\n152   2  4  2  5  4\n153   2  6  6  6  5\n154   1  5  5  5  5\n155   1  5  6  6  6\n156   4  5  6  1  5\n157   1  6  5  5  6\n158   5  4  3  4  5\n159   2  6  5  6  6\n160   4  4  1  6  3\n161   1  6  6  6  5\n162   1  6  6  6  6\n163   5  1  4  3  4\n164   5  4  1  4  2\n165   2  4  2  6  4\n166   3  4  5  4  5\n167   2  5  5  1  5\n168   1  5  6  5  5\n169   2  4  4  5  3\n170   1  4  4  4  4\n171   1  5  4  5  5\n172   2  6  5  5  5\n173   6  6  6  6  6\n174   1  6  6  4  4\n175   6  6  1  3  5\n176   3  3  5  4  5\n177   4  5  5  6  6\n178   2  3  1  1  1\n179   1  4  5  1  5\n180   5  6  6  6  6\n181   4  6  6  5  6\n182   2  5  2  1  2\n183   4  4  4  4  3\n184   2  4  4  6  6\n185   1  4  6  6  3\n186   2  6  6  5  5\n187   1  5  2  6  4\n188   2  4  5  2  5\n189   1  6  6  5  5\n190   3  4  5  5  4\n191   4  2  1  4  1\n192   3  5  1  4  4\n193   4  5  4  2  4\n194   6  5  5  2  5\n195   1  6  2  6  5\n196   2  5  6  6  6\n197   4  5  4  3  3\n198   5  6  6  5  5\n199   3  4  4  6  5\n200   3  4  5  5  4\n201   1  6  6  3  6\n202   3  5  4  5  5\n203   2  6  6  5  5\n204   5  6  6  5  6\n205   4  4  4  6  5\n206   3  5  4  4  5\n207   1  5  6  6  6\n208   3  6  6  5  6\n209   4  6  5  5  6\n210   4  5  3  5  4\n211   4  5  3  4  4\n212   3  4  3  5  5\n213   4  2  4  6  5\n214   2  5  4  4  5\n215   2  4  4  6  4\n216   1  6  5  4  5\n217   1  6  6  6  6\n218   2  5  5  5  5\n219   1  6  5  4  6\n220   4  4  2  1  3\n221   2  5  5  4  6\n222   1  5  5  6  5\n223   1  6  5  2  5\n224   3  4  4  3  4\n225   1  6  5  1  5\n226   2  3  4  4  5\n227   1  5  5  6  5\n228   2  5  4  3  3\n229   1  6  6  5  6\n230   2  3  2  1  4\n231   2  3  4  5  6\n232   1  6  6  6  6\n233   3  4  4  5  5\n234   3  5  5  3  5\n235   5  4  2  6  4\n236   4  6  4  5  5\n237   4  5  5  5  6\n238   3  6  5  6  6\n239   5  5  4  5  5\n240   2  5  5  5  5\n241   2  2  2  3  2\n242   5  1  3  5  2\n243   5  6  5  6  4\n244   1  6  6  6  5\n245   2  6  5  4  6\n246   4  4  5  5  6\n247   1  5  5  4  4\n248   4  4  5  6  5\n249   1  4  4  2  2\n250   1  6  6  6  6\n251   1  4  3  6  3\n252   3  3  2  1  2\n253   1  6  6  6  6\n254   1  5  1  2  5\n255   2  6  5  6  3\n256   2  6  6  6  5\n257   2  3  5  4  4\n258   4  6  4  2  5\n259   2  6  5  6  4\n260   1  5  4  5  4\n261   4  5  4  2  4\n262   4  6  6  6  5\n263   2  5  5  5  2\n264   5  5  4  6  4\n265   1  6  6  5  6\n266   1  6  6  6  5\n267   2  5  5  4  6\n268   5  6  5  5  6\n269   1  5  5  5  4\n270   4  4  4  6  3\n271   6  4  1  5  1\n272   6  6  6  6  6\n273   1  5  4  6  4\n274   1  6  6  5  3\n275   1  5  1  5  5\n276   4  6  5  6  6\n277   2  6  6  6  5\n278   2  5  5  4  4\n279   1  5  3  1  4\n280   2  5  5  6  2\n281   3  4  4  1  4\n282   3  2  6  1  6\n283   2  6  6  4  6\n284   2  6  5  5  5\n285   2  5  5  5  5\n286   1  5  6  4  5\n287   1  5  5  6  5\n288   1  5  5  5  6\n289   5  3  2  5  2\n290   5  3  1  2  3\n291   1  6  6  6  6\n292   2  5  5  2  5\n293   1  5  6  6  6\n294   1  6  5  6  5\n295   1  5  5  6  4\n296   1  5  2  5  3\n297   3  4  5  6  5\n298   2  5  5  5  5\n299   4  4  5  6  5\n300   2  3  2  5  4\n301   2  4  5  1  4\n302   2  6  6  6  6\n303   1  6  6  6  5\n304   4  4  2  2  3\n305   1  5  4  4  4\n306   2  5  3  5  2\n307   2  5  5  4  5\n308   4  3  2  6  5\n309   2  6  6  6  6\n310   1  6  6  6  5\n311   3  5  5  4  5\n312   1  6  5  2  6\n313   4  6  5  2  5\n314   1  6  5  6  5\n315   5  5  5  4  6\n316   1  6  4  5  5\n317   4  4  3  1  3\n318   4  6  2  5  5\n319   1  6  6  6  4\n320   2  5  4  5  5\n321   2  6  5  6  5\n322   5  5  5  2  6\n323   4  4  5  2  3\n324   3  4  4  6  4\n325   1  6  6  5  6\n326   2  5  3  5  1\n327   2  4  4  1  3\n328   3  5  2  6  2\n329   3  6  4  4  4\n330   4  4  3  1  4\n331   2  6  6  3  5\n332   3  5  6  6  6\n333   2  4  3  4  4\n334   1  6  5  6  6\n335   1  5  6  6  5\n336   5  4  1  2  2\n337   3  5  3  3  4\n338   1  6  5  5  6\n339   1  6  5  6  4\n340   1  6  6  6  6\n341   2  4  5  5  6\n342   1  6  5  4  5\n343   2  6  5  6  5\n344   1  5  6  4  6\n345   6  6  5  6  6\n346   2  5  6  6  5\n347   1  6  5  6  6\n348   2  5  5  4  3\n349   1  5  4  6  4\n350   1  6  6  6  6\n351   5  2  2  4  4\n352   2  5  4  6  4\n353   1  5  3  4  5\n354   2  4  3  4  5\n355   1  5  3  4  4\n356   3  5  4  3  4\n357   3  5  5  5  5\n358   2  1  6  6  6\n359   1  5  4  5  4\n360   1  4  6  5  6\n361   1  6  6  3  6\n362   3  5  2  5  5\n363   1  5  6  5  5\n364   1  6  6  5  6\n365   2  4  5  6  5\n366   1  6  6  5  5\n367   5  4  4  4  3\n368   2  5  5  4  4\n369   3  5  2  4  5\n370   2  6  5  6  5\n371   3  5  3  5  4\n372   2  5  4  5  5\n373   2  5  2  1  3\n374   1  4  6  2  6\n375   2  6  5  6  5\n376   2  4  5  4  4\n377   3  6  6  6  6\n378   3  2  2  4  4\n379   2  4  3  2  3\n380   5  5  2  5  6\n381   2  4  5  5  5\n382   5  6  5  6  6\n383   5  4  4  3  1\n384   5  2  2  2  4\n385   1  6  5  6  4\n386   3  5  5  5  5\n387   1  4  5  6  4\n388   2  6  5  5  6\n389   1  5  5  6  6\n390   3  5  6  6  6\n391   4  5  5  6  5\n392   3  4  4  4  4\n393   3  4  4  4  4\n394   2  5  3  3  2\n395   2  4  4  4  5\n396   5  4  4  4  5\n397   3  5  5  6  2\n398   1  6  6  4  5\n399   2  5  6  4  5\n400   2  4  4  1  2\n401   2  5  4  5  5\n402   2  6  5  6  3\n403   3  4  4  4  4\n404   3  6  5  6  5\n405   1  6  6  6  4\n406   6  6  6  6  6\n407   2  3  5  2  5\n408   2  5  5  2  5\n409   2  5  1  6  6\n410   2  5  6  3  5\n411   5  6  6  5  6\n412   4  4  4  6  4\n413   3  5  5  5  4\n414   5  5  4  6  4\n415   1  4  4  5  4\n416   2  6  6  6  1\n417   1  3  3  5  3\n418   2  2  2  5  5\n419   2  6  6  4  5\n420   1  5  5  3  4\n421   4  4  5  2  5\n422   2  5  5  6  5\n423   2  3  6  4  5\n424   4  6  6  6  6\n425   4  4  4  4  2\n426   2  2  5  4  4\n427   4  4  4  5  5\n428   1  6  5  2  5\n429   3  4  5  6  6\n430   2  6  6  6  6\n431   1  5  6  6  5\n432   1  4  4  4  6\n433   2  2  5  6  6\n434   1  6  6  4  5\n435   2  5  6  5  6\n436   2  6  6  6  5\n437   5  5  5  5  3\n438   1  6  5  6  5\n439   2  5  5  5  5\n440   1  6  2  5  2\n441   1  4  4  5  4\n442   1  6  6  4  6\n443   1  6  6  6  5\n444   1  6  4  4  5\n445   3  4  3  5  5\n446   6  2  5  6  6\n447   1  5  6  6  1\n448   5  4  5  2  5\n449   2  6  5  6  5\n450   2  6  5  6  4\n451   1  1  1  1  1\n452   1  5  5  6  4\n453   3  5  4  6  5\n454   1  5  5  4  5\n455   1  5  5  6  5\n456   5  5  4  5  5\n457   3  3  2  4  3\n458   3  5  5  5  5\n459   1  6  6  6  5\n460   2  5  5  6  4\n461   2  5  6  5  5\n462   4  5  5  5  5\n463   4  5  3  6  3\n464   1  5  5  6  5\n465   3  5  5  6  3\n466   1  1  4  6  6\n467   1  5  6  6  4\n468   3  4  5  5  5\n469   3  4  3  1  4\n470   2  5  5  3  2\n471   4  3  3  4  3\n472   3  6  6  6  5\n473   2  6  3  6  2\n474   1  6  5  2  4\n475   4  5  3  2  3\n476   1  3  5  6  6\n477   1  6  5  6  5\n478   1  5  5  5  5\n479   2  6  6  5  5\n480   1  6  6  6  6\n481   6  6  6  6  6\n482   1  5  6  6  4\n483   1  6  6  6  6\n484   2  5  5  6  6\n485   1  6  6  6  5\n486   2  4  4  4  5\n487   2  6  4  6  5\n488   2  5  5  2  5\n489   1  6  4  6  6\n490   3  3  3  4  4\n491   4  3  1  1  5\n492   2  4  2  2  4\n493   2  4  5  5  4\n494   3  5  4  4  2\n495   4  4  4  6  1\n496   2  5  4  5  5\n497   1  6  6  6  6\n498   2  3  4  5  2\n499   4  3  1  5  1\n500   2  5  5  6  4\n501   2  5  3  6  5\n502   1  6  6  6  6\n503   2  4  1  1  5\n504   2  5  3  5  4\n505   4  6  6  6  2\n506   4  2  4  1  4\n507   3  3  4  5  2\n508   1  6  5  6  5\n509   3  4  4  2  6\n510   2  5  4  4  6\n511   4  5  5  6  5\n512   1  4  4  3  4\n513   3  5  6  5  5\n514   2  4  5  4  4\n515   2  6  4  6  4\n516   2  2  5  5  4\n517   1  6  6  5  5\n518   2  4  4  6  4\n519   3  6  5  6  6\n520   4  2  4  4  2\n521   2  5  6  6  5\n522   4  5  5  6  4\n523   4  4  5  1  5\n524   2  5  4  5  5\n525   2  6  6  5  4\n526   5  6  4  3  5\n527   4  4  4  4  4\n528   2  5  5  5  5\n529   3  4  4  3  4\n530   2  6  6  5  5\n531   1  6  5  6  6\n532   4  4  4  5  6\n533   5  2  3  2  2\n534   4  5  2  2  1\n535   1  6  5  4  5\n536   2  4  3  4  4\n537   1  6  5  5  6\n538   3  6  6  4  3\n539   1  2  6  4  6\n540   5  6  6  6  6\n541   3  4  4  5  4\n542   1  6  6  6  6\n543   3  5  5  5  5\n544   3  5  6  6  5\n545   5  5  3  2  5\n546   1  4  6  5  4\n547   2  6  6  6  6\n548   3  5  6  6  5\n549   2  6  5  6  6\n550   4  6  5  6  6\n551   1  6  6  6  6\n552   5  5  5  5  4\n553   1  6  6  6  5\n554   1  4  3  5  5\n555   3  2  2  2  2\n556   4  4  4  6  3\n557   2  3  3  4  3\n558   2  6  5  5  6\n559   4  5  4  5  5\n560   2  4  5  5  5\n561   4  6  6  6  6\n562   6  5  5  6  6\n563   3  5  5  6  6\n564   2  5  5  5  2\n565   4  4  4  4  4\n566   1  6  6  6  6\n567   3  5  5  5  5\n568   5  5  2  2  3\n569   1  6  6  6  6\n570   2  5  4  3  4\n571   2  4  6  2  5\n572   1  6  6  6  6\n573   1  6  6  6  5\n574   4  6  5  4  5\n575   4  5  5  4  5\n576   1  5  4  5  5\n577   2  5  6  6  5\n578   1  6  6  5  6\n579   3  6  6  4  3\n580   2  5  4  4  4\n581   3  5  2  4  5\n582   2  5  4  5  4\n583   1  6  6  6  6\n584   2  5  5  4  5\n585   4  5  6  4  5\n586   3  1  5  6  4\n587   2  5  5  5  5\n588   5  6  6  6  6\n589   3  4  4  3  3\n590   1  6  6  6  6\n591   2  4  4  3  4\n592   1  5  6  6  4\n593   1  6  5  6  6\n594   3  6  4  6  4\n595   4  2  1  2  2\n596   1  6  5  6  5\n597   4  4  6  6  6\n598   3  3  3  4  5\n599   5  6  4  1  5\n600   1  6  5  6  6\n601   2  3  1  3  1\n602   1  5  5  6  5\n603   2  2  5  5  5\n604   1  5  6  5  5\n605   4  6  6  6  5\n606   4  4  5  1  2\n607   1  5  5  6  4\n608   1  6  5  4  5\n609   1  6  6  6  5\n610   3  4  4  5  4\n611   2  5  5  5  4\n612   5  4  5  6  3\n613   3  3  5  6  5\n614   1  6  6  4  6\n615   2  6  5  3  5\n616   1  6  6  6  6\n617   2  5  4  5  3\n618   2  1  3  1  3\n619   2  6  4  1  5\n620   2  5  5  6  5\n621   2  5  6  5  6\n622   3  4  5  5  3\n623   3  4  5  1  5\n624   3  2  3  5  4\n625   4  5  2  1  4\n626   1  6  5  5  5\n627   2  5  5  3  4\n628   2  4  5  5  3\n629   3  6  6  3  5\n630   6  6  6  6  6\n631   1  6  6  4  6\n632   2  4  4  4  4\n633   2  6  6  6  6\n634   2  3  4  1  2\n635   3  5  5  5  3\n636   4  4  1  2  2\n637   3  4  3  6  3\n638   2  4  4  3  4\n639   1  5  5  6  5\n640   1  6  6  6  6\n641   1  6  6  6  5\n642   4  4  4  3  4\n643   1  5  5  6  5\n644   3  4  3  5  4\n645   2  6  5  3  5\n646   3  5  5  4  5\n647   1  4  6  6  2\n648   5  5  5  6  4\n649   2  6  6  6  6\n650   1  6  4  3  5\n651   3  2  5  6  6\n652   3  5  5  4  4\n653   2  5  5  4  2\n654   1  5  5  1  6\n655   2  5  4  6  4\n656   1  5  6  6  5\n657   3  4  6  6  4\n658   1  6  4  1  1\n659   3  5  5  6  6\n660   1  5  5  6  5\n661   4  4  5  4  5\n662   5  3  5  4  2\n663   1  5  5  6  6\n664   1  4  4  4  4\n665   6  5  5  3  3\n666   1  6  4  5  4\n667   3  4  4  6  3\n668   2  4  2  1  2\n669   4  5  5  5  4\n670   2  5  5  5  3\n671   2  6  5  6  4\n672   1  6  5  4  5\n673   3  5  5  4  4\n674   2  4  4  6  4\n675   1  5  5  6  5\n676   1  5  5  5  5\n677   2  6  6  6  6\n678   2  4  4  5  2\n679   5  5  5  6  5\n680   2  5  5  6  5\n681   2  6  4  5  4\n682   4  5  2  6  5\n683   3  6  6  6  5\n684   2  5  6  5  6\n685   2  5  5  5  5\n686   2  4  5  5  5\n687   1  6  6  5  4\n688   1  6  6  6  6\n689   1  4  5  3  4\n690   3  2  2  2  4\n691   2  5  6  3  5\n692   1  4  5  2  6\n693   2  4  2  1  3\n694   1  5  1  5  6\n695   3  4  5  6  5\n696   5  5  2  3  3\n697   2  4  5  5  4\n698   1  6  5  6  5\n699   2  6  5  5  6\n700   5  5  5  6  6\n701   5  2  3  4  2\n702   1  5  5  6  6\n703   1  1  1  1  1\n704   3  5  5  6  5\n705   2  6  3  6  5\n706   4  5  6  4  5\n707   1  6  6  5  4\n708   2  5  5  4  5\n709   1  2  3  4  2\n710   1  6  6  6  3\n711   4  4  5  3  4\n712   2  5  5  6  4\n713   1  6  5  6  6\n714   1  6  6  6  6\n715   1  5  6  6  6\n716   5  5  3  5  2\n717   1  6  6  2  5\n718   2  4  5  6  4\n719   1  6  6  6  6\n720   5  6  3  6  6\n721   5  6  5  6  5\n722   2  5  4  6  5\n723   1  6  6  6  6\n724   1  6  6  2  5\n725   1  6  5  6  5\n726   1  6  6  6  6\n727   1  6  5  5  5\n728   2  5  5  6  5\n729   1  6  6  6  6\n730   4  5  5  4  5\n731   1  4  4  3  6\n732   3  5  5  6  5\n733   2  5  5  4  4\n734   2  5  5  2  4\n735   1  6  6  6  5\n736   1  6  6  6  6\n737   2  4  4  5  4\n738   5  4  3  4  3\n739   1  6  5  6  5\n740   1  5  5  6  4\n741   4  5  5  6  5\n742   1  6  3  5  3\n743   3  5  4  4  4\n744   4  4  5  4  5\n745   3  4  4  5  5\n746   3  2  3  3  1\n747   4  4  2  5  2\n748   2  6  5  5  3\n749   2  5  4  5  5\n750   5  6  5  6  5\n751   1  6  6  6  5\n752   2  5  5  6  5\n753   1  4  6  2  5\n754   2  4  4  6  5\n755   3  4  5  6  5\n756   1  5  5  4  5\n757   4  5  5  5  6\n758   3  4  6  6  6\n759   1  6  5  6  6\n760   6  4  5  5  5\n761   1  5  4  2  5\n762   2  6  5  5  5\n763   3  4  4  5  5\n764   5  4  2  2  3\n765   2  5  4  6  5\n766   5  5  5  6  3\n767   5  3  5  3  3\n768   2  4  5  5  5\n769   3  4  4  6  4\n770   1  6  6  1  6\n771   4  4  3  2  3\n772   4  5  5  6  5\n773   6  1  6  6  5\n774   4  4  4  5  5\n775   5  4  4  4  3\n776   4  3  2  3  4\n777   2  5  5  6  5\n778   2  4  4  4  5\n779   4  3  6  6  6\n780   2  5  5  6  4\n781   1  6  6  6  5\n782   2  4  5  3  4\n783   3  4  5  6  2\n784   1  6  6  6  4\n785   3  6  5  6  6\n786   4  4  3  4  2\n787   1  6  5  3  5\n788   4  3  6  4  6\n789   2  4  5  2  5\n790   1  5  6  4  6\n791   2  5  5  3  5\n792   3  5  5  3  4\n793   1  5  5  6  6\n794   2  5  5  2  5\n795   2  4  6  5  5\n796   1  6  4  6  3\n797   1  5  5  6  6\n798   6  6  6  6  6\n799   2  6  3  3  3\n800   1  5  5  6  5\n801   1  6  6  5  6\n802   2  3  3  5  5\n803   1  4  5  5  5\n804   4  1  2  4  2\n805   2  4  4  5  4\n806   2  5  5  5  2\n807   1  6  6  6  6\n808   2  5  5  6  4\n809   1  2  2  4  3\n810   1  5  2  3  4\n811   2  5  6  5  6\n812   2  5  5  5  5\n813   1  6  6  6  5\n814   2  4  4  4  3\n815   2  3  4  5  3\n816   2  5  5  5  4\n817   5  4  4  3  5\n818   4  4  3  2  5\n819   5  5  5  5  5\n820   4  5  4  5  4\n821   4  6  5  6  6\n822   1  6  6  1  1\n823   1  6  6  6  6\n824   1  6  6  6  6\n825   2  4  6  6  6\n826   4  4  5  1  4\n827   1  6  6  6  5\n828   1  5  5  6  5\n829   1  5  4  6  5\n830   5  2  2  2  5\n831   3  6  5  6  5\n832   1  6  5  6  6\n833   1  6  4  6  4\n834   4  6  5  5  5\n835   6  6  6  5  5\n836   1  6  6  6  6\n837   3  5  6  4  5\n838   2  6  6  6  6\n839   1  3  2  3  2\n840   2  6  5  6  3\n841   1  6  6  3  5\n842   2  5  3  2  2\n843   1  6  6  4  1\n844   1  4  4  1  6\n845   2  5  4  6  4\n846   5  6  6  5  5\n847   2  5  5  5  3\n848   1  6  5  6  4\n849   3  3  4  2  4\n850   2  5  5  4  5\n851   1  6  4  6  4\n852   2  6  5  5  4\n853   1  5  5  6  5\n854   2  6  5  2  5\n855   4  4  5  3  4\n856   5  5  2  5  2\n857   3  6  5  6  5\n858   1  5  5  5  4\n859   2  3  4  5  5\n860   3  5  5  6  5\n861   2  5  4  6  4\n862   4  3  4  3  4\n863   2  5  5  5  5\n864   2  5  5  6  5\n865   1  5  6  5  6\n866   2  6  4  6  4\n867   3  2  2  1  5\n868   3  5  1  4  4\n869   4  3  2  5  3\n870   2  3  4  2  4\n871   3  4  5  4  4\n872   4  5  4  5  5\n873   1  6  6  6  6\n874   2  5  6  4  6\n875   2  4  3  5  4\n876   2  5  4  5  4\n877   2  5  5  6  5\n878   5  2  3  3  3\n879   5  2  2  2  4\n880   2  6  4  1  4\n881   1  6  6  5  5\n882   1  4  4  6  4\n883   1  5  6  4  5\n884   3  4  5  4  4\n885   2  5  5  6  6\n886   1  6  6  6  6\n887   4  4  4  3  3\n888   2  5  4  6  5\n889   2  3  1  4  2\n890   2  5  2  5  5\n891   2  6  6  6  4\n892   1  6  6  4  6\n893   3  6  6  3  2\n894   1  6  6  6  1\n895   3  5  2  6  5\n896   1  5  6  4  5\n897   2  4  4  4  5\n898   5  3  3  4  4\n899   1  5  4  4  5\n900   1  5  5  4  4\n901   1  4  2  2  2\n902   2  6  5  5  5\n903   1  5  6  6  6\n904   5  2  3  2  2\n905   5  5  6  2  5\n906   5  5  5  6  5\n907   4  4  4  5  4\n908   2  5  5  6  6\n909   1  5  6  6  5\n910   1  5  5  5  6\n911   1  1  1  1  5\n912   4  5  5  5  3\n913   2  4  5  6  5\n914   2  6  6  6  6\n915   2  4  4  3  5\n916   3  6  6  6  5\n917   3  4  5  2  5\n918   1  5  5  6  6\n919   3  6  5  4  5\n920   3  5  6  6  5\n921   1  5  6  6  6\n922   2  6  6  6  6\n923   3  4  4  4  4\n924   2  5  4  6  5\n925   1  6  1  6  5\n926   2  3  4  4  5\n927   1  6  6  5  5\n928   3  6  3  4  3\n929   2  4  4  4  5\n930   2  5  6  5  5\n931   2  5  5  6  5\n932   2  5  4  4  4\n933   2  5  5  2  5\n934   2  5  6  6  6\n935   2  5  2  4  2\n936   2  6  5  6  5\n937   1  5  6  4  6\n938   2  5  5  5  6\n939   1  6  5  6  4\n940   2  6  5  3  5\n941   2  4  4  5  4\n942   4  4  5  4  4\n943   2  4  5  5  5\n944   1  5  4  5  6\n945   3  4  3  1  3\n946   3  4  4  4  4\n947   1  5  6  5  6\n948   2  5  6  6  6\n949   2  6  5  4  5\n950   2  5  6  6  4\n951   1  5  5  6  5\n952   5  1  5  1  4\n953   2  5  6  6  6\n954   2  5  5  6  5\n955   1  6  6  6  6\n956   2  3  5  1  5\n957   2  6  4  2  5\n958   1  5  5  5  6\n959   6  6  6  6  6\n960   1  6  6  4  6\n961   2  3  6  5  5\n962   1  5  5  6  5\n963   4  5  5  5  6\n964   4  4  5  1  5\n965   4  5  5  6  6\n966   4  5  5  5  3\n967   4  5  5  5  4\n968   4  4  6  3  6\n969   4  6  5  5  4\n970   3  5  2  1  5\n971   1  5  1  6  6\n972   3  6  6  3  6\n973   1  5  5  4  4\n974   1  6  6  3  6\n975   4  6  4  6  4\n976   6  3  2  4  4\n977   2  1  6  6  6\n978   1  4  5  6  6\n979   1  6  4  6  6\n980   5  4  5  6  4\n981   1  6  6  6  6\n982   2  5  5  6  4\n983   3  2  4  6  5\n984   1  5  4  5  4\n985   2  6  6  6  6\n986   1  5  5  6  5\n987   3  4  2  2  3\n988   2  4  2  2  3\n989   1  5  5  4  5\n990   4  4  5  4  4\n991   2  1  4  2  4\n992   4  6  4  5  5\n993   4  4  3  3  4\n994   2  5  2  6  3\n995   5  4  6  2  4\n996   5  4  5  4  4\n997   1  5  4  6  4\n998   4  5  5  1  6\n999   1  5  5  5  5\n1000  2  5  5  5  4\n\npsych::describe(mybfi[, 1:5])\n\n   vars    n mean   sd median trimmed  mad min max range  skew kurtosis   se\nA1    1 1000 2.42 1.36      2    2.25 1.48   1   6     5  0.77    -0.37 0.04\nA2    2 1000 4.82 1.15      5    4.99 1.48   1   6     5 -1.09     1.07 0.04\nA3    3 1000 4.60 1.30      5    4.79 1.48   1   6     5 -1.02     0.46 0.04\nA4    4 1000 4.63 1.50      5    4.86 1.48   1   6     5 -0.98    -0.05 0.05\nA5    5 1000 4.56 1.23      5    4.71 1.48   1   6     5 -0.89     0.32 0.04\n\ncor(mybfi[, 1:5])\n\n           A1         A2         A3         A4         A5\nA1  1.0000000 -0.2685526 -0.2397655 -0.1370491 -0.1856911\nA2 -0.2685526  1.0000000  0.4732203  0.3099604  0.3642183\nA3 -0.2397655  0.4732203  1.0000000  0.3334171  0.4933920\nA4 -0.1370491  0.3099604  0.3334171  1.0000000  0.2872772\nA5 -0.1856911  0.3642183  0.4933920  0.2872772  1.0000000",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#selecting-two-or-more-non-contiguous-variables-by-their-column-numbers-with-square-brackets",
    "href": "Subsetting.html#selecting-two-or-more-non-contiguous-variables-by-their-column-numbers-with-square-brackets",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "7.12 Selecting two or more non-contiguous variables by their column numbers with square brackets",
    "text": "7.12 Selecting two or more non-contiguous variables by their column numbers with square brackets\n\nmybfi[, c(1,3,5)]\n\n     A1 A3 A5\n1     1  6  6\n2     3  4  4\n3     5  3  3\n4     3  6  5\n5     5  5  6\n6     4  4  5\n7     1  5  5\n8     2  5  5\n9     3  4  5\n10    6  2  2\n11    2  5  5\n12    3  5  5\n13    3  4  5\n14    2  6  6\n15    1  1  2\n16    1  5  5\n17    2  5  5\n18    1  3  3\n19    2  5  4\n20    6  5  2\n21    1  3  4\n22    3  6  5\n23    2  3  3\n24    3  3  3\n25    2  4  2\n26    2  5  5\n27    4  5  4\n28    4  6  4\n29    5  3  4\n30    2  1  3\n31    2  4  4\n32    4  4  4\n33    4  5  5\n34    4  6  5\n35    2  6  6\n36    3  4  5\n37    2  5  5\n38    2  1  3\n39    4  4  4\n40    2  4  5\n41    2  4  5\n42    4  4  4\n43    2  5  5\n44    1  6  3\n45    2  5  4\n46    3  4  4\n47    3  5  3\n48    2  5  4\n49    2  4  4\n50    4  4  3\n51    1  5  5\n52    3  5  5\n53    4  4  2\n54    1  6  6\n55    3  3  3\n56    3  5  5\n57    1  4  5\n58    3  1  2\n59    2  5  3\n60    4  6  4\n61    4  6  5\n62    4  5  5\n63    3  2  4\n64    1  6  2\n65    2  6  6\n66    2  6  5\n67    1  5  5\n68    1  4  5\n69    2  5  5\n70    2  3  5\n71    2  6  4\n72    1  5  5\n73    1  6  6\n74    1  3  6\n75    1  6  6\n76    2  6  2\n77    4  5  5\n78    1  6  6\n79    3  5  6\n80    4  6  5\n81    1  6  6\n82    2  6  6\n83    3  5  1\n84    3  5  4\n85    1  6  6\n86    5  4  4\n87    1  6  5\n88    3  6  5\n89    1  1  6\n90    1  6  6\n91    3  5  6\n92    1  6  5\n93    5  1  1\n94    1  2  2\n95    1  5  4\n96    4  2  4\n97    2  5  6\n98    1  6  6\n99    1  5  4\n100   2  4  5\n101   1  6  6\n102   3  5  2\n103   1  5  3\n104   2  6  5\n105   4  5  4\n106   6  6  6\n107   2  5  6\n108   1  4  4\n109   2  4  6\n110   3  4  5\n111   1  5  4\n112   6  4  6\n113   3  5  5\n114   1  6  6\n115   3  4  3\n116   2  5  4\n117   4  5  5\n118   2  5  5\n119   1  4  5\n120   1  5  6\n121   1  5  6\n122   2  3  6\n123   1  6  4\n124   5  2  2\n125   2  4  5\n126   1  6  6\n127   1  6  5\n128   1  5  4\n129   1  5  4\n130   4  6  6\n131   3  3  4\n132   1  6  1\n133   5  6  5\n134   2  5  5\n135   1  6  6\n136   3  5  5\n137   4  5  5\n138   2  6  4\n139   4  3  4\n140   1  6  5\n141   1  5  5\n142   1  5  5\n143   2  6  6\n144   5  3  3\n145   1  5  5\n146   4  4  5\n147   3  5  5\n148   1  6  6\n149   3  3  3\n150   1  6  6\n151   2  5  5\n152   2  2  4\n153   2  6  5\n154   1  5  5\n155   1  6  6\n156   4  6  5\n157   1  5  6\n158   5  3  5\n159   2  5  6\n160   4  1  3\n161   1  6  5\n162   1  6  6\n163   5  4  4\n164   5  1  2\n165   2  2  4\n166   3  5  5\n167   2  5  5\n168   1  6  5\n169   2  4  3\n170   1  4  4\n171   1  4  5\n172   2  5  5\n173   6  6  6\n174   1  6  4\n175   6  1  5\n176   3  5  5\n177   4  5  6\n178   2  1  1\n179   1  5  5\n180   5  6  6\n181   4  6  6\n182   2  2  2\n183   4  4  3\n184   2  4  6\n185   1  6  3\n186   2  6  5\n187   1  2  4\n188   2  5  5\n189   1  6  5\n190   3  5  4\n191   4  1  1\n192   3  1  4\n193   4  4  4\n194   6  5  5\n195   1  2  5\n196   2  6  6\n197   4  4  3\n198   5  6  5\n199   3  4  5\n200   3  5  4\n201   1  6  6\n202   3  4  5\n203   2  6  5\n204   5  6  6\n205   4  4  5\n206   3  4  5\n207   1  6  6\n208   3  6  6\n209   4  5  6\n210   4  3  4\n211   4  3  4\n212   3  3  5\n213   4  4  5\n214   2  4  5\n215   2  4  4\n216   1  5  5\n217   1  6  6\n218   2  5  5\n219   1  5  6\n220   4  2  3\n221   2  5  6\n222   1  5  5\n223   1  5  5\n224   3  4  4\n225   1  5  5\n226   2  4  5\n227   1  5  5\n228   2  4  3\n229   1  6  6\n230   2  2  4\n231   2  4  6\n232   1  6  6\n233   3  4  5\n234   3  5  5\n235   5  2  4\n236   4  4  5\n237   4  5  6\n238   3  5  6\n239   5  4  5\n240   2  5  5\n241   2  2  2\n242   5  3  2\n243   5  5  4\n244   1  6  5\n245   2  5  6\n246   4  5  6\n247   1  5  4\n248   4  5  5\n249   1  4  2\n250   1  6  6\n251   1  3  3\n252   3  2  2\n253   1  6  6\n254   1  1  5\n255   2  5  3\n256   2  6  5\n257   2  5  4\n258   4  4  5\n259   2  5  4\n260   1  4  4\n261   4  4  4\n262   4  6  5\n263   2  5  2\n264   5  4  4\n265   1  6  6\n266   1  6  5\n267   2  5  6\n268   5  5  6\n269   1  5  4\n270   4  4  3\n271   6  1  1\n272   6  6  6\n273   1  4  4\n274   1  6  3\n275   1  1  5\n276   4  5  6\n277   2  6  5\n278   2  5  4\n279   1  3  4\n280   2  5  2\n281   3  4  4\n282   3  6  6\n283   2  6  6\n284   2  5  5\n285   2  5  5\n286   1  6  5\n287   1  5  5\n288   1  5  6\n289   5  2  2\n290   5  1  3\n291   1  6  6\n292   2  5  5\n293   1  6  6\n294   1  5  5\n295   1  5  4\n296   1  2  3\n297   3  5  5\n298   2  5  5\n299   4  5  5\n300   2  2  4\n301   2  5  4\n302   2  6  6\n303   1  6  5\n304   4  2  3\n305   1  4  4\n306   2  3  2\n307   2  5  5\n308   4  2  5\n309   2  6  6\n310   1  6  5\n311   3  5  5\n312   1  5  6\n313   4  5  5\n314   1  5  5\n315   5  5  6\n316   1  4  5\n317   4  3  3\n318   4  2  5\n319   1  6  4\n320   2  4  5\n321   2  5  5\n322   5  5  6\n323   4  5  3\n324   3  4  4\n325   1  6  6\n326   2  3  1\n327   2  4  3\n328   3  2  2\n329   3  4  4\n330   4  3  4\n331   2  6  5\n332   3  6  6\n333   2  3  4\n334   1  5  6\n335   1  6  5\n336   5  1  2\n337   3  3  4\n338   1  5  6\n339   1  5  4\n340   1  6  6\n341   2  5  6\n342   1  5  5\n343   2  5  5\n344   1  6  6\n345   6  5  6\n346   2  6  5\n347   1  5  6\n348   2  5  3\n349   1  4  4\n350   1  6  6\n351   5  2  4\n352   2  4  4\n353   1  3  5\n354   2  3  5\n355   1  3  4\n356   3  4  4\n357   3  5  5\n358   2  6  6\n359   1  4  4\n360   1  6  6\n361   1  6  6\n362   3  2  5\n363   1  6  5\n364   1  6  6\n365   2  5  5\n366   1  6  5\n367   5  4  3\n368   2  5  4\n369   3  2  5\n370   2  5  5\n371   3  3  4\n372   2  4  5\n373   2  2  3\n374   1  6  6\n375   2  5  5\n376   2  5  4\n377   3  6  6\n378   3  2  4\n379   2  3  3\n380   5  2  6\n381   2  5  5\n382   5  5  6\n383   5  4  1\n384   5  2  4\n385   1  5  4\n386   3  5  5\n387   1  5  4\n388   2  5  6\n389   1  5  6\n390   3  6  6\n391   4  5  5\n392   3  4  4\n393   3  4  4\n394   2  3  2\n395   2  4  5\n396   5  4  5\n397   3  5  2\n398   1  6  5\n399   2  6  5\n400   2  4  2\n401   2  4  5\n402   2  5  3\n403   3  4  4\n404   3  5  5\n405   1  6  4\n406   6  6  6\n407   2  5  5\n408   2  5  5\n409   2  1  6\n410   2  6  5\n411   5  6  6\n412   4  4  4\n413   3  5  4\n414   5  4  4\n415   1  4  4\n416   2  6  1\n417   1  3  3\n418   2  2  5\n419   2  6  5\n420   1  5  4\n421   4  5  5\n422   2  5  5\n423   2  6  5\n424   4  6  6\n425   4  4  2\n426   2  5  4\n427   4  4  5\n428   1  5  5\n429   3  5  6\n430   2  6  6\n431   1  6  5\n432   1  4  6\n433   2  5  6\n434   1  6  5\n435   2  6  6\n436   2  6  5\n437   5  5  3\n438   1  5  5\n439   2  5  5\n440   1  2  2\n441   1  4  4\n442   1  6  6\n443   1  6  5\n444   1  4  5\n445   3  3  5\n446   6  5  6\n447   1  6  1\n448   5  5  5\n449   2  5  5\n450   2  5  4\n451   1  1  1\n452   1  5  4\n453   3  4  5\n454   1  5  5\n455   1  5  5\n456   5  4  5\n457   3  2  3\n458   3  5  5\n459   1  6  5\n460   2  5  4\n461   2  6  5\n462   4  5  5\n463   4  3  3\n464   1  5  5\n465   3  5  3\n466   1  4  6\n467   1  6  4\n468   3  5  5\n469   3  3  4\n470   2  5  2\n471   4  3  3\n472   3  6  5\n473   2  3  2\n474   1  5  4\n475   4  3  3\n476   1  5  6\n477   1  5  5\n478   1  5  5\n479   2  6  5\n480   1  6  6\n481   6  6  6\n482   1  6  4\n483   1  6  6\n484   2  5  6\n485   1  6  5\n486   2  4  5\n487   2  4  5\n488   2  5  5\n489   1  4  6\n490   3  3  4\n491   4  1  5\n492   2  2  4\n493   2  5  4\n494   3  4  2\n495   4  4  1\n496   2  4  5\n497   1  6  6\n498   2  4  2\n499   4  1  1\n500   2  5  4\n501   2  3  5\n502   1  6  6\n503   2  1  5\n504   2  3  4\n505   4  6  2\n506   4  4  4\n507   3  4  2\n508   1  5  5\n509   3  4  6\n510   2  4  6\n511   4  5  5\n512   1  4  4\n513   3  6  5\n514   2  5  4\n515   2  4  4\n516   2  5  4\n517   1  6  5\n518   2  4  4\n519   3  5  6\n520   4  4  2\n521   2  6  5\n522   4  5  4\n523   4  5  5\n524   2  4  5\n525   2  6  4\n526   5  4  5\n527   4  4  4\n528   2  5  5\n529   3  4  4\n530   2  6  5\n531   1  5  6\n532   4  4  6\n533   5  3  2\n534   4  2  1\n535   1  5  5\n536   2  3  4\n537   1  5  6\n538   3  6  3\n539   1  6  6\n540   5  6  6\n541   3  4  4\n542   1  6  6\n543   3  5  5\n544   3  6  5\n545   5  3  5\n546   1  6  4\n547   2  6  6\n548   3  6  5\n549   2  5  6\n550   4  5  6\n551   1  6  6\n552   5  5  4\n553   1  6  5\n554   1  3  5\n555   3  2  2\n556   4  4  3\n557   2  3  3\n558   2  5  6\n559   4  4  5\n560   2  5  5\n561   4  6  6\n562   6  5  6\n563   3  5  6\n564   2  5  2\n565   4  4  4\n566   1  6  6\n567   3  5  5\n568   5  2  3\n569   1  6  6\n570   2  4  4\n571   2  6  5\n572   1  6  6\n573   1  6  5\n574   4  5  5\n575   4  5  5\n576   1  4  5\n577   2  6  5\n578   1  6  6\n579   3  6  3\n580   2  4  4\n581   3  2  5\n582   2  4  4\n583   1  6  6\n584   2  5  5\n585   4  6  5\n586   3  5  4\n587   2  5  5\n588   5  6  6\n589   3  4  3\n590   1  6  6\n591   2  4  4\n592   1  6  4\n593   1  5  6\n594   3  4  4\n595   4  1  2\n596   1  5  5\n597   4  6  6\n598   3  3  5\n599   5  4  5\n600   1  5  6\n601   2  1  1\n602   1  5  5\n603   2  5  5\n604   1  6  5\n605   4  6  5\n606   4  5  2\n607   1  5  4\n608   1  5  5\n609   1  6  5\n610   3  4  4\n611   2  5  4\n612   5  5  3\n613   3  5  5\n614   1  6  6\n615   2  5  5\n616   1  6  6\n617   2  4  3\n618   2  3  3\n619   2  4  5\n620   2  5  5\n621   2  6  6\n622   3  5  3\n623   3  5  5\n624   3  3  4\n625   4  2  4\n626   1  5  5\n627   2  5  4\n628   2  5  3\n629   3  6  5\n630   6  6  6\n631   1  6  6\n632   2  4  4\n633   2  6  6\n634   2  4  2\n635   3  5  3\n636   4  1  2\n637   3  3  3\n638   2  4  4\n639   1  5  5\n640   1  6  6\n641   1  6  5\n642   4  4  4\n643   1  5  5\n644   3  3  4\n645   2  5  5\n646   3  5  5\n647   1  6  2\n648   5  5  4\n649   2  6  6\n650   1  4  5\n651   3  5  6\n652   3  5  4\n653   2  5  2\n654   1  5  6\n655   2  4  4\n656   1  6  5\n657   3  6  4\n658   1  4  1\n659   3  5  6\n660   1  5  5\n661   4  5  5\n662   5  5  2\n663   1  5  6\n664   1  4  4\n665   6  5  3\n666   1  4  4\n667   3  4  3\n668   2  2  2\n669   4  5  4\n670   2  5  3\n671   2  5  4\n672   1  5  5\n673   3  5  4\n674   2  4  4\n675   1  5  5\n676   1  5  5\n677   2  6  6\n678   2  4  2\n679   5  5  5\n680   2  5  5\n681   2  4  4\n682   4  2  5\n683   3  6  5\n684   2  6  6\n685   2  5  5\n686   2  5  5\n687   1  6  4\n688   1  6  6\n689   1  5  4\n690   3  2  4\n691   2  6  5\n692   1  5  6\n693   2  2  3\n694   1  1  6\n695   3  5  5\n696   5  2  3\n697   2  5  4\n698   1  5  5\n699   2  5  6\n700   5  5  6\n701   5  3  2\n702   1  5  6\n703   1  1  1\n704   3  5  5\n705   2  3  5\n706   4  6  5\n707   1  6  4\n708   2  5  5\n709   1  3  2\n710   1  6  3\n711   4  5  4\n712   2  5  4\n713   1  5  6\n714   1  6  6\n715   1  6  6\n716   5  3  2\n717   1  6  5\n718   2  5  4\n719   1  6  6\n720   5  3  6\n721   5  5  5\n722   2  4  5\n723   1  6  6\n724   1  6  5\n725   1  5  5\n726   1  6  6\n727   1  5  5\n728   2  5  5\n729   1  6  6\n730   4  5  5\n731   1  4  6\n732   3  5  5\n733   2  5  4\n734   2  5  4\n735   1  6  5\n736   1  6  6\n737   2  4  4\n738   5  3  3\n739   1  5  5\n740   1  5  4\n741   4  5  5\n742   1  3  3\n743   3  4  4\n744   4  5  5\n745   3  4  5\n746   3  3  1\n747   4  2  2\n748   2  5  3\n749   2  4  5\n750   5  5  5\n751   1  6  5\n752   2  5  5\n753   1  6  5\n754   2  4  5\n755   3  5  5\n756   1  5  5\n757   4  5  6\n758   3  6  6\n759   1  5  6\n760   6  5  5\n761   1  4  5\n762   2  5  5\n763   3  4  5\n764   5  2  3\n765   2  4  5\n766   5  5  3\n767   5  5  3\n768   2  5  5\n769   3  4  4\n770   1  6  6\n771   4  3  3\n772   4  5  5\n773   6  6  5\n774   4  4  5\n775   5  4  3\n776   4  2  4\n777   2  5  5\n778   2  4  5\n779   4  6  6\n780   2  5  4\n781   1  6  5\n782   2  5  4\n783   3  5  2\n784   1  6  4\n785   3  5  6\n786   4  3  2\n787   1  5  5\n788   4  6  6\n789   2  5  5\n790   1  6  6\n791   2  5  5\n792   3  5  4\n793   1  5  6\n794   2  5  5\n795   2  6  5\n796   1  4  3\n797   1  5  6\n798   6  6  6\n799   2  3  3\n800   1  5  5\n801   1  6  6\n802   2  3  5\n803   1  5  5\n804   4  2  2\n805   2  4  4\n806   2  5  2\n807   1  6  6\n808   2  5  4\n809   1  2  3\n810   1  2  4\n811   2  6  6\n812   2  5  5\n813   1  6  5\n814   2  4  3\n815   2  4  3\n816   2  5  4\n817   5  4  5\n818   4  3  5\n819   5  5  5\n820   4  4  4\n821   4  5  6\n822   1  6  1\n823   1  6  6\n824   1  6  6\n825   2  6  6\n826   4  5  4\n827   1  6  5\n828   1  5  5\n829   1  4  5\n830   5  2  5\n831   3  5  5\n832   1  5  6\n833   1  4  4\n834   4  5  5\n835   6  6  5\n836   1  6  6\n837   3  6  5\n838   2  6  6\n839   1  2  2\n840   2  5  3\n841   1  6  5\n842   2  3  2\n843   1  6  1\n844   1  4  6\n845   2  4  4\n846   5  6  5\n847   2  5  3\n848   1  5  4\n849   3  4  4\n850   2  5  5\n851   1  4  4\n852   2  5  4\n853   1  5  5\n854   2  5  5\n855   4  5  4\n856   5  2  2\n857   3  5  5\n858   1  5  4\n859   2  4  5\n860   3  5  5\n861   2  4  4\n862   4  4  4\n863   2  5  5\n864   2  5  5\n865   1  6  6\n866   2  4  4\n867   3  2  5\n868   3  1  4\n869   4  2  3\n870   2  4  4\n871   3  5  4\n872   4  4  5\n873   1  6  6\n874   2  6  6\n875   2  3  4\n876   2  4  4\n877   2  5  5\n878   5  3  3\n879   5  2  4\n880   2  4  4\n881   1  6  5\n882   1  4  4\n883   1  6  5\n884   3  5  4\n885   2  5  6\n886   1  6  6\n887   4  4  3\n888   2  4  5\n889   2  1  2\n890   2  2  5\n891   2  6  4\n892   1  6  6\n893   3  6  2\n894   1  6  1\n895   3  2  5\n896   1  6  5\n897   2  4  5\n898   5  3  4\n899   1  4  5\n900   1  5  4\n901   1  2  2\n902   2  5  5\n903   1  6  6\n904   5  3  2\n905   5  6  5\n906   5  5  5\n907   4  4  4\n908   2  5  6\n909   1  6  5\n910   1  5  6\n911   1  1  5\n912   4  5  3\n913   2  5  5\n914   2  6  6\n915   2  4  5\n916   3  6  5\n917   3  5  5\n918   1  5  6\n919   3  5  5\n920   3  6  5\n921   1  6  6\n922   2  6  6\n923   3  4  4\n924   2  4  5\n925   1  1  5\n926   2  4  5\n927   1  6  5\n928   3  3  3\n929   2  4  5\n930   2  6  5\n931   2  5  5\n932   2  4  4\n933   2  5  5\n934   2  6  6\n935   2  2  2\n936   2  5  5\n937   1  6  6\n938   2  5  6\n939   1  5  4\n940   2  5  5\n941   2  4  4\n942   4  5  4\n943   2  5  5\n944   1  4  6\n945   3  3  3\n946   3  4  4\n947   1  6  6\n948   2  6  6\n949   2  5  5\n950   2  6  4\n951   1  5  5\n952   5  5  4\n953   2  6  6\n954   2  5  5\n955   1  6  6\n956   2  5  5\n957   2  4  5\n958   1  5  6\n959   6  6  6\n960   1  6  6\n961   2  6  5\n962   1  5  5\n963   4  5  6\n964   4  5  5\n965   4  5  6\n966   4  5  3\n967   4  5  4\n968   4  6  6\n969   4  5  4\n970   3  2  5\n971   1  1  6\n972   3  6  6\n973   1  5  4\n974   1  6  6\n975   4  4  4\n976   6  2  4\n977   2  6  6\n978   1  5  6\n979   1  4  6\n980   5  5  4\n981   1  6  6\n982   2  5  4\n983   3  4  5\n984   1  4  4\n985   2  6  6\n986   1  5  5\n987   3  2  3\n988   2  2  3\n989   1  5  5\n990   4  5  4\n991   2  4  4\n992   4  4  5\n993   4  3  4\n994   2  2  3\n995   5  6  4\n996   5  5  4\n997   1  4  4\n998   4  5  6\n999   1  5  5\n1000  2  5  4",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#selecting-two-or-more-variables-by-their-names-with-square-brackets",
    "href": "Subsetting.html#selecting-two-or-more-variables-by-their-names-with-square-brackets",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "7.13 Selecting two or more variables by their names with square brackets",
    "text": "7.13 Selecting two or more variables by their names with square brackets\n\nmybfi[, c(\"A1\", \"A3\", \"A5\")]\n\n     A1 A3 A5\n1     1  6  6\n2     3  4  4\n3     5  3  3\n4     3  6  5\n5     5  5  6\n6     4  4  5\n7     1  5  5\n8     2  5  5\n9     3  4  5\n10    6  2  2\n11    2  5  5\n12    3  5  5\n13    3  4  5\n14    2  6  6\n15    1  1  2\n16    1  5  5\n17    2  5  5\n18    1  3  3\n19    2  5  4\n20    6  5  2\n21    1  3  4\n22    3  6  5\n23    2  3  3\n24    3  3  3\n25    2  4  2\n26    2  5  5\n27    4  5  4\n28    4  6  4\n29    5  3  4\n30    2  1  3\n31    2  4  4\n32    4  4  4\n33    4  5  5\n34    4  6  5\n35    2  6  6\n36    3  4  5\n37    2  5  5\n38    2  1  3\n39    4  4  4\n40    2  4  5\n41    2  4  5\n42    4  4  4\n43    2  5  5\n44    1  6  3\n45    2  5  4\n46    3  4  4\n47    3  5  3\n48    2  5  4\n49    2  4  4\n50    4  4  3\n51    1  5  5\n52    3  5  5\n53    4  4  2\n54    1  6  6\n55    3  3  3\n56    3  5  5\n57    1  4  5\n58    3  1  2\n59    2  5  3\n60    4  6  4\n61    4  6  5\n62    4  5  5\n63    3  2  4\n64    1  6  2\n65    2  6  6\n66    2  6  5\n67    1  5  5\n68    1  4  5\n69    2  5  5\n70    2  3  5\n71    2  6  4\n72    1  5  5\n73    1  6  6\n74    1  3  6\n75    1  6  6\n76    2  6  2\n77    4  5  5\n78    1  6  6\n79    3  5  6\n80    4  6  5\n81    1  6  6\n82    2  6  6\n83    3  5  1\n84    3  5  4\n85    1  6  6\n86    5  4  4\n87    1  6  5\n88    3  6  5\n89    1  1  6\n90    1  6  6\n91    3  5  6\n92    1  6  5\n93    5  1  1\n94    1  2  2\n95    1  5  4\n96    4  2  4\n97    2  5  6\n98    1  6  6\n99    1  5  4\n100   2  4  5\n101   1  6  6\n102   3  5  2\n103   1  5  3\n104   2  6  5\n105   4  5  4\n106   6  6  6\n107   2  5  6\n108   1  4  4\n109   2  4  6\n110   3  4  5\n111   1  5  4\n112   6  4  6\n113   3  5  5\n114   1  6  6\n115   3  4  3\n116   2  5  4\n117   4  5  5\n118   2  5  5\n119   1  4  5\n120   1  5  6\n121   1  5  6\n122   2  3  6\n123   1  6  4\n124   5  2  2\n125   2  4  5\n126   1  6  6\n127   1  6  5\n128   1  5  4\n129   1  5  4\n130   4  6  6\n131   3  3  4\n132   1  6  1\n133   5  6  5\n134   2  5  5\n135   1  6  6\n136   3  5  5\n137   4  5  5\n138   2  6  4\n139   4  3  4\n140   1  6  5\n141   1  5  5\n142   1  5  5\n143   2  6  6\n144   5  3  3\n145   1  5  5\n146   4  4  5\n147   3  5  5\n148   1  6  6\n149   3  3  3\n150   1  6  6\n151   2  5  5\n152   2  2  4\n153   2  6  5\n154   1  5  5\n155   1  6  6\n156   4  6  5\n157   1  5  6\n158   5  3  5\n159   2  5  6\n160   4  1  3\n161   1  6  5\n162   1  6  6\n163   5  4  4\n164   5  1  2\n165   2  2  4\n166   3  5  5\n167   2  5  5\n168   1  6  5\n169   2  4  3\n170   1  4  4\n171   1  4  5\n172   2  5  5\n173   6  6  6\n174   1  6  4\n175   6  1  5\n176   3  5  5\n177   4  5  6\n178   2  1  1\n179   1  5  5\n180   5  6  6\n181   4  6  6\n182   2  2  2\n183   4  4  3\n184   2  4  6\n185   1  6  3\n186   2  6  5\n187   1  2  4\n188   2  5  5\n189   1  6  5\n190   3  5  4\n191   4  1  1\n192   3  1  4\n193   4  4  4\n194   6  5  5\n195   1  2  5\n196   2  6  6\n197   4  4  3\n198   5  6  5\n199   3  4  5\n200   3  5  4\n201   1  6  6\n202   3  4  5\n203   2  6  5\n204   5  6  6\n205   4  4  5\n206   3  4  5\n207   1  6  6\n208   3  6  6\n209   4  5  6\n210   4  3  4\n211   4  3  4\n212   3  3  5\n213   4  4  5\n214   2  4  5\n215   2  4  4\n216   1  5  5\n217   1  6  6\n218   2  5  5\n219   1  5  6\n220   4  2  3\n221   2  5  6\n222   1  5  5\n223   1  5  5\n224   3  4  4\n225   1  5  5\n226   2  4  5\n227   1  5  5\n228   2  4  3\n229   1  6  6\n230   2  2  4\n231   2  4  6\n232   1  6  6\n233   3  4  5\n234   3  5  5\n235   5  2  4\n236   4  4  5\n237   4  5  6\n238   3  5  6\n239   5  4  5\n240   2  5  5\n241   2  2  2\n242   5  3  2\n243   5  5  4\n244   1  6  5\n245   2  5  6\n246   4  5  6\n247   1  5  4\n248   4  5  5\n249   1  4  2\n250   1  6  6\n251   1  3  3\n252   3  2  2\n253   1  6  6\n254   1  1  5\n255   2  5  3\n256   2  6  5\n257   2  5  4\n258   4  4  5\n259   2  5  4\n260   1  4  4\n261   4  4  4\n262   4  6  5\n263   2  5  2\n264   5  4  4\n265   1  6  6\n266   1  6  5\n267   2  5  6\n268   5  5  6\n269   1  5  4\n270   4  4  3\n271   6  1  1\n272   6  6  6\n273   1  4  4\n274   1  6  3\n275   1  1  5\n276   4  5  6\n277   2  6  5\n278   2  5  4\n279   1  3  4\n280   2  5  2\n281   3  4  4\n282   3  6  6\n283   2  6  6\n284   2  5  5\n285   2  5  5\n286   1  6  5\n287   1  5  5\n288   1  5  6\n289   5  2  2\n290   5  1  3\n291   1  6  6\n292   2  5  5\n293   1  6  6\n294   1  5  5\n295   1  5  4\n296   1  2  3\n297   3  5  5\n298   2  5  5\n299   4  5  5\n300   2  2  4\n301   2  5  4\n302   2  6  6\n303   1  6  5\n304   4  2  3\n305   1  4  4\n306   2  3  2\n307   2  5  5\n308   4  2  5\n309   2  6  6\n310   1  6  5\n311   3  5  5\n312   1  5  6\n313   4  5  5\n314   1  5  5\n315   5  5  6\n316   1  4  5\n317   4  3  3\n318   4  2  5\n319   1  6  4\n320   2  4  5\n321   2  5  5\n322   5  5  6\n323   4  5  3\n324   3  4  4\n325   1  6  6\n326   2  3  1\n327   2  4  3\n328   3  2  2\n329   3  4  4\n330   4  3  4\n331   2  6  5\n332   3  6  6\n333   2  3  4\n334   1  5  6\n335   1  6  5\n336   5  1  2\n337   3  3  4\n338   1  5  6\n339   1  5  4\n340   1  6  6\n341   2  5  6\n342   1  5  5\n343   2  5  5\n344   1  6  6\n345   6  5  6\n346   2  6  5\n347   1  5  6\n348   2  5  3\n349   1  4  4\n350   1  6  6\n351   5  2  4\n352   2  4  4\n353   1  3  5\n354   2  3  5\n355   1  3  4\n356   3  4  4\n357   3  5  5\n358   2  6  6\n359   1  4  4\n360   1  6  6\n361   1  6  6\n362   3  2  5\n363   1  6  5\n364   1  6  6\n365   2  5  5\n366   1  6  5\n367   5  4  3\n368   2  5  4\n369   3  2  5\n370   2  5  5\n371   3  3  4\n372   2  4  5\n373   2  2  3\n374   1  6  6\n375   2  5  5\n376   2  5  4\n377   3  6  6\n378   3  2  4\n379   2  3  3\n380   5  2  6\n381   2  5  5\n382   5  5  6\n383   5  4  1\n384   5  2  4\n385   1  5  4\n386   3  5  5\n387   1  5  4\n388   2  5  6\n389   1  5  6\n390   3  6  6\n391   4  5  5\n392   3  4  4\n393   3  4  4\n394   2  3  2\n395   2  4  5\n396   5  4  5\n397   3  5  2\n398   1  6  5\n399   2  6  5\n400   2  4  2\n401   2  4  5\n402   2  5  3\n403   3  4  4\n404   3  5  5\n405   1  6  4\n406   6  6  6\n407   2  5  5\n408   2  5  5\n409   2  1  6\n410   2  6  5\n411   5  6  6\n412   4  4  4\n413   3  5  4\n414   5  4  4\n415   1  4  4\n416   2  6  1\n417   1  3  3\n418   2  2  5\n419   2  6  5\n420   1  5  4\n421   4  5  5\n422   2  5  5\n423   2  6  5\n424   4  6  6\n425   4  4  2\n426   2  5  4\n427   4  4  5\n428   1  5  5\n429   3  5  6\n430   2  6  6\n431   1  6  5\n432   1  4  6\n433   2  5  6\n434   1  6  5\n435   2  6  6\n436   2  6  5\n437   5  5  3\n438   1  5  5\n439   2  5  5\n440   1  2  2\n441   1  4  4\n442   1  6  6\n443   1  6  5\n444   1  4  5\n445   3  3  5\n446   6  5  6\n447   1  6  1\n448   5  5  5\n449   2  5  5\n450   2  5  4\n451   1  1  1\n452   1  5  4\n453   3  4  5\n454   1  5  5\n455   1  5  5\n456   5  4  5\n457   3  2  3\n458   3  5  5\n459   1  6  5\n460   2  5  4\n461   2  6  5\n462   4  5  5\n463   4  3  3\n464   1  5  5\n465   3  5  3\n466   1  4  6\n467   1  6  4\n468   3  5  5\n469   3  3  4\n470   2  5  2\n471   4  3  3\n472   3  6  5\n473   2  3  2\n474   1  5  4\n475   4  3  3\n476   1  5  6\n477   1  5  5\n478   1  5  5\n479   2  6  5\n480   1  6  6\n481   6  6  6\n482   1  6  4\n483   1  6  6\n484   2  5  6\n485   1  6  5\n486   2  4  5\n487   2  4  5\n488   2  5  5\n489   1  4  6\n490   3  3  4\n491   4  1  5\n492   2  2  4\n493   2  5  4\n494   3  4  2\n495   4  4  1\n496   2  4  5\n497   1  6  6\n498   2  4  2\n499   4  1  1\n500   2  5  4\n501   2  3  5\n502   1  6  6\n503   2  1  5\n504   2  3  4\n505   4  6  2\n506   4  4  4\n507   3  4  2\n508   1  5  5\n509   3  4  6\n510   2  4  6\n511   4  5  5\n512   1  4  4\n513   3  6  5\n514   2  5  4\n515   2  4  4\n516   2  5  4\n517   1  6  5\n518   2  4  4\n519   3  5  6\n520   4  4  2\n521   2  6  5\n522   4  5  4\n523   4  5  5\n524   2  4  5\n525   2  6  4\n526   5  4  5\n527   4  4  4\n528   2  5  5\n529   3  4  4\n530   2  6  5\n531   1  5  6\n532   4  4  6\n533   5  3  2\n534   4  2  1\n535   1  5  5\n536   2  3  4\n537   1  5  6\n538   3  6  3\n539   1  6  6\n540   5  6  6\n541   3  4  4\n542   1  6  6\n543   3  5  5\n544   3  6  5\n545   5  3  5\n546   1  6  4\n547   2  6  6\n548   3  6  5\n549   2  5  6\n550   4  5  6\n551   1  6  6\n552   5  5  4\n553   1  6  5\n554   1  3  5\n555   3  2  2\n556   4  4  3\n557   2  3  3\n558   2  5  6\n559   4  4  5\n560   2  5  5\n561   4  6  6\n562   6  5  6\n563   3  5  6\n564   2  5  2\n565   4  4  4\n566   1  6  6\n567   3  5  5\n568   5  2  3\n569   1  6  6\n570   2  4  4\n571   2  6  5\n572   1  6  6\n573   1  6  5\n574   4  5  5\n575   4  5  5\n576   1  4  5\n577   2  6  5\n578   1  6  6\n579   3  6  3\n580   2  4  4\n581   3  2  5\n582   2  4  4\n583   1  6  6\n584   2  5  5\n585   4  6  5\n586   3  5  4\n587   2  5  5\n588   5  6  6\n589   3  4  3\n590   1  6  6\n591   2  4  4\n592   1  6  4\n593   1  5  6\n594   3  4  4\n595   4  1  2\n596   1  5  5\n597   4  6  6\n598   3  3  5\n599   5  4  5\n600   1  5  6\n601   2  1  1\n602   1  5  5\n603   2  5  5\n604   1  6  5\n605   4  6  5\n606   4  5  2\n607   1  5  4\n608   1  5  5\n609   1  6  5\n610   3  4  4\n611   2  5  4\n612   5  5  3\n613   3  5  5\n614   1  6  6\n615   2  5  5\n616   1  6  6\n617   2  4  3\n618   2  3  3\n619   2  4  5\n620   2  5  5\n621   2  6  6\n622   3  5  3\n623   3  5  5\n624   3  3  4\n625   4  2  4\n626   1  5  5\n627   2  5  4\n628   2  5  3\n629   3  6  5\n630   6  6  6\n631   1  6  6\n632   2  4  4\n633   2  6  6\n634   2  4  2\n635   3  5  3\n636   4  1  2\n637   3  3  3\n638   2  4  4\n639   1  5  5\n640   1  6  6\n641   1  6  5\n642   4  4  4\n643   1  5  5\n644   3  3  4\n645   2  5  5\n646   3  5  5\n647   1  6  2\n648   5  5  4\n649   2  6  6\n650   1  4  5\n651   3  5  6\n652   3  5  4\n653   2  5  2\n654   1  5  6\n655   2  4  4\n656   1  6  5\n657   3  6  4\n658   1  4  1\n659   3  5  6\n660   1  5  5\n661   4  5  5\n662   5  5  2\n663   1  5  6\n664   1  4  4\n665   6  5  3\n666   1  4  4\n667   3  4  3\n668   2  2  2\n669   4  5  4\n670   2  5  3\n671   2  5  4\n672   1  5  5\n673   3  5  4\n674   2  4  4\n675   1  5  5\n676   1  5  5\n677   2  6  6\n678   2  4  2\n679   5  5  5\n680   2  5  5\n681   2  4  4\n682   4  2  5\n683   3  6  5\n684   2  6  6\n685   2  5  5\n686   2  5  5\n687   1  6  4\n688   1  6  6\n689   1  5  4\n690   3  2  4\n691   2  6  5\n692   1  5  6\n693   2  2  3\n694   1  1  6\n695   3  5  5\n696   5  2  3\n697   2  5  4\n698   1  5  5\n699   2  5  6\n700   5  5  6\n701   5  3  2\n702   1  5  6\n703   1  1  1\n704   3  5  5\n705   2  3  5\n706   4  6  5\n707   1  6  4\n708   2  5  5\n709   1  3  2\n710   1  6  3\n711   4  5  4\n712   2  5  4\n713   1  5  6\n714   1  6  6\n715   1  6  6\n716   5  3  2\n717   1  6  5\n718   2  5  4\n719   1  6  6\n720   5  3  6\n721   5  5  5\n722   2  4  5\n723   1  6  6\n724   1  6  5\n725   1  5  5\n726   1  6  6\n727   1  5  5\n728   2  5  5\n729   1  6  6\n730   4  5  5\n731   1  4  6\n732   3  5  5\n733   2  5  4\n734   2  5  4\n735   1  6  5\n736   1  6  6\n737   2  4  4\n738   5  3  3\n739   1  5  5\n740   1  5  4\n741   4  5  5\n742   1  3  3\n743   3  4  4\n744   4  5  5\n745   3  4  5\n746   3  3  1\n747   4  2  2\n748   2  5  3\n749   2  4  5\n750   5  5  5\n751   1  6  5\n752   2  5  5\n753   1  6  5\n754   2  4  5\n755   3  5  5\n756   1  5  5\n757   4  5  6\n758   3  6  6\n759   1  5  6\n760   6  5  5\n761   1  4  5\n762   2  5  5\n763   3  4  5\n764   5  2  3\n765   2  4  5\n766   5  5  3\n767   5  5  3\n768   2  5  5\n769   3  4  4\n770   1  6  6\n771   4  3  3\n772   4  5  5\n773   6  6  5\n774   4  4  5\n775   5  4  3\n776   4  2  4\n777   2  5  5\n778   2  4  5\n779   4  6  6\n780   2  5  4\n781   1  6  5\n782   2  5  4\n783   3  5  2\n784   1  6  4\n785   3  5  6\n786   4  3  2\n787   1  5  5\n788   4  6  6\n789   2  5  5\n790   1  6  6\n791   2  5  5\n792   3  5  4\n793   1  5  6\n794   2  5  5\n795   2  6  5\n796   1  4  3\n797   1  5  6\n798   6  6  6\n799   2  3  3\n800   1  5  5\n801   1  6  6\n802   2  3  5\n803   1  5  5\n804   4  2  2\n805   2  4  4\n806   2  5  2\n807   1  6  6\n808   2  5  4\n809   1  2  3\n810   1  2  4\n811   2  6  6\n812   2  5  5\n813   1  6  5\n814   2  4  3\n815   2  4  3\n816   2  5  4\n817   5  4  5\n818   4  3  5\n819   5  5  5\n820   4  4  4\n821   4  5  6\n822   1  6  1\n823   1  6  6\n824   1  6  6\n825   2  6  6\n826   4  5  4\n827   1  6  5\n828   1  5  5\n829   1  4  5\n830   5  2  5\n831   3  5  5\n832   1  5  6\n833   1  4  4\n834   4  5  5\n835   6  6  5\n836   1  6  6\n837   3  6  5\n838   2  6  6\n839   1  2  2\n840   2  5  3\n841   1  6  5\n842   2  3  2\n843   1  6  1\n844   1  4  6\n845   2  4  4\n846   5  6  5\n847   2  5  3\n848   1  5  4\n849   3  4  4\n850   2  5  5\n851   1  4  4\n852   2  5  4\n853   1  5  5\n854   2  5  5\n855   4  5  4\n856   5  2  2\n857   3  5  5\n858   1  5  4\n859   2  4  5\n860   3  5  5\n861   2  4  4\n862   4  4  4\n863   2  5  5\n864   2  5  5\n865   1  6  6\n866   2  4  4\n867   3  2  5\n868   3  1  4\n869   4  2  3\n870   2  4  4\n871   3  5  4\n872   4  4  5\n873   1  6  6\n874   2  6  6\n875   2  3  4\n876   2  4  4\n877   2  5  5\n878   5  3  3\n879   5  2  4\n880   2  4  4\n881   1  6  5\n882   1  4  4\n883   1  6  5\n884   3  5  4\n885   2  5  6\n886   1  6  6\n887   4  4  3\n888   2  4  5\n889   2  1  2\n890   2  2  5\n891   2  6  4\n892   1  6  6\n893   3  6  2\n894   1  6  1\n895   3  2  5\n896   1  6  5\n897   2  4  5\n898   5  3  4\n899   1  4  5\n900   1  5  4\n901   1  2  2\n902   2  5  5\n903   1  6  6\n904   5  3  2\n905   5  6  5\n906   5  5  5\n907   4  4  4\n908   2  5  6\n909   1  6  5\n910   1  5  6\n911   1  1  5\n912   4  5  3\n913   2  5  5\n914   2  6  6\n915   2  4  5\n916   3  6  5\n917   3  5  5\n918   1  5  6\n919   3  5  5\n920   3  6  5\n921   1  6  6\n922   2  6  6\n923   3  4  4\n924   2  4  5\n925   1  1  5\n926   2  4  5\n927   1  6  5\n928   3  3  3\n929   2  4  5\n930   2  6  5\n931   2  5  5\n932   2  4  4\n933   2  5  5\n934   2  6  6\n935   2  2  2\n936   2  5  5\n937   1  6  6\n938   2  5  6\n939   1  5  4\n940   2  5  5\n941   2  4  4\n942   4  5  4\n943   2  5  5\n944   1  4  6\n945   3  3  3\n946   3  4  4\n947   1  6  6\n948   2  6  6\n949   2  5  5\n950   2  6  4\n951   1  5  5\n952   5  5  4\n953   2  6  6\n954   2  5  5\n955   1  6  6\n956   2  5  5\n957   2  4  5\n958   1  5  6\n959   6  6  6\n960   1  6  6\n961   2  6  5\n962   1  5  5\n963   4  5  6\n964   4  5  5\n965   4  5  6\n966   4  5  3\n967   4  5  4\n968   4  6  6\n969   4  5  4\n970   3  2  5\n971   1  1  6\n972   3  6  6\n973   1  5  4\n974   1  6  6\n975   4  4  4\n976   6  2  4\n977   2  6  6\n978   1  5  6\n979   1  4  6\n980   5  5  4\n981   1  6  6\n982   2  5  4\n983   3  4  5\n984   1  4  4\n985   2  6  6\n986   1  5  5\n987   3  2  3\n988   2  2  3\n989   1  5  5\n990   4  5  4\n991   2  4  4\n992   4  4  5\n993   4  3  4\n994   2  2  3\n995   5  6  4\n996   5  5  4\n997   1  4  4\n998   4  5  6\n999   1  5  5\n1000  2  5  4",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#fun-with-starwars-characters",
    "href": "Subsetting.html#fun-with-starwars-characters",
    "title": "5  Subsetting a data frame using the dplyr package",
    "section": "7.14 Fun with Starwars characters",
    "text": "7.14 Fun with Starwars characters\n\nstarwars\n\n# A tibble: 87 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Luke Sk…    172    77 blond      fair       blue            19   male  mascu…\n 2 C-3PO       167    75 &lt;NA&gt;       gold       yellow         112   none  mascu…\n 3 R2-D2        96    32 &lt;NA&gt;       white, bl… red             33   none  mascu…\n 4 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n 5 Leia Or…    150    49 brown      light      brown           19   fema… femin…\n 6 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n 7 Beru Wh…    165    75 brown      light      blue            47   fema… femin…\n 8 R5-D4        97    32 &lt;NA&gt;       white, red red             NA   none  mascu…\n 9 Biggs D…    183    84 black      light      brown           24   male  mascu…\n10 Obi-Wan…    182    77 auburn, w… fair       blue-gray       57   male  mascu…\n# ℹ 77 more rows\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nstarwars %&gt;% \n  filter(!species == \"Human\") %&gt;% \n  select(name, height, mass, eye_color, species, films)\n\n# A tibble: 48 × 6\n   name                  height  mass eye_color species        films    \n   &lt;chr&gt;                  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;          &lt;list&gt;   \n 1 C-3PO                    167    75 yellow    Droid          &lt;chr [6]&gt;\n 2 R2-D2                     96    32 red       Droid          &lt;chr [7]&gt;\n 3 R5-D4                     97    32 red       Droid          &lt;chr [1]&gt;\n 4 Chewbacca                228   112 blue      Wookiee        &lt;chr [5]&gt;\n 5 Greedo                   173    74 black     Rodian         &lt;chr [1]&gt;\n 6 Jabba Desilijic Tiure    175  1358 orange    Hutt           &lt;chr [3]&gt;\n 7 Yoda                      66    17 brown     Yoda's species &lt;chr [5]&gt;\n 8 IG-88                    200   140 red       Droid          &lt;chr [1]&gt;\n 9 Bossk                    190   113 red       Trandoshan     &lt;chr [1]&gt;\n10 Ackbar                   180    83 orange    Mon Calamari   &lt;chr [2]&gt;\n# ℹ 38 more rows",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Estimating_correlations.html",
    "href": "Estimating_correlations.html",
    "title": "6  Estimating correlations with lavaan",
    "section": "",
    "text": "6.1 Finding the Pearson correlations of a set of variables\nSay we want to find the correlations of the five Agreeableness items, which are located in columns 1 to 5 in the bfi data frame. We can use the lavCor() function of the lavaan package to easily find the correlations as follows: lavCor(bfi[1:5]).\nIn the example below we find the Pearson correlations of the five Neuroticism items, which are located in columns 16 to 20.\nlavCor(bfi[16:20])\n\n      N1    N2    N3    N4    N5\nN1 1.000                        \nN2 0.706 1.000                  \nN3 0.556 0.545 1.000            \nN4 0.399 0.390 0.518 1.000      \nN5 0.377 0.352 0.428 0.398 1.000",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estimating correlations with lavaan</span>"
    ]
  },
  {
    "objectID": "Estimating_correlations.html#the-statistical-signifcance-of-the-correlations",
    "href": "Estimating_correlations.html#the-statistical-signifcance-of-the-correlations",
    "title": "6  Estimating correlations with lavaan",
    "section": "6.2 The statistical signifcance of the correlations",
    "text": "6.2 The statistical signifcance of the correlations\nWe can also obtain information about (a) the number of cases whose data were included in estimating the correlations (by default cases with missing data are removed listwise), (b) the covariances and variances of the items, and (c) the statistical significance of the correlations. To do this we need to specify that we want the output in lavaan format and that we want to estimate the standard errors of the covariances. We could do this for the five Agreeableness items as follows: Agree_cors &lt;- lavCor(bfi[1:5], se = standard, output = \"lavaan\"). Note that we store the results in an object that here is named Agree_cors. Finally, we ask for a summary of Agree_cors and specify that we want to see the standardized parameters (i.e. the correlations). We could do that as follows: summary(Agree_cors, standardized = TRUE)\nIn the example below we find the correlations of the five Neuroticism items, request standard errors, specify that the output should be in lavaan format, and store the results as Neur_cors.\n\nNeur_cors &lt;- lavCor(bfi[16:20], output = \"lavaan\", se = \"standard\")\nsummary(Neur_cors, standardized = TRUE)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n\n                                                  Used       Total\n  Number of observations                          2694        2800\n\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  N1 ~~                                                                 \n    N2                1.694    0.057   29.927    0.000    1.694    0.706\n    N3                1.399    0.055   25.220    0.000    1.399    0.556\n    N4                0.986    0.051   19.217    0.000    0.986    0.399\n    N5                0.961    0.053   18.302    0.000    0.961    0.377\n  N2 ~~                                                                 \n    N3                1.332    0.054   24.854    0.000    1.332    0.545\n    N4                0.937    0.050   18.869    0.000    0.937    0.390\n    N5                0.872    0.051   17.247    0.000    0.872    0.352\n  N3 ~~                                                                 \n    N4                1.304    0.055   23.875    0.000    1.304    0.518\n    N5                1.110    0.054   20.422    0.000    1.110    0.428\n  N4 ~~                                                                 \n    N5                1.014    0.053   19.176    0.000    1.014    0.398\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    N1                2.474    0.067   36.701    0.000    2.474    1.000\n    N2                2.329    0.063   36.701    0.000    2.329    1.000\n    N3                2.560    0.070   36.701    0.000    2.560    1.000\n    N4                2.474    0.067   36.701    0.000    2.474    1.000\n    N5                2.630    0.072   36.701    0.000    2.630    1.000",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estimating correlations with lavaan</span>"
    ]
  },
  {
    "objectID": "Estimating_correlations.html#using-full-information-maximum-likelihood-to-deal-with-missing-data",
    "href": "Estimating_correlations.html#using-full-information-maximum-likelihood-to-deal-with-missing-data",
    "title": "6  Estimating correlations with lavaan",
    "section": "6.3 Using full information maximum likelihood to deal with missing data",
    "text": "6.3 Using full information maximum likelihood to deal with missing data\nNote that the lavaan output shows that only 2694 of the 2800 cases were used to find the correlations (this is due to some missing data). We can use the data of all 2800 cases by using full information maximum likelihood (fiml) to estimate the correlations. This technique uses all the information in the data set to estimate the correlations and no cases are removed.\nWe can use the same code as before, but add the specification that the missing data should be dealt with using full information maximum likelihood.\n\nlavCor(bfi[16:20], missing = \"fiml\")\n\n      N1    N2    N3    N4    N5\nN1 1.000                        \nN2 0.707 1.000                  \nN3 0.557 0.549 1.000            \nN4 0.398 0.391 0.519 1.000      \nN5 0.378 0.351 0.428 0.398 1.000\n\n\n\nNeur_cors_fiml &lt;- lavCor(bfi[16:20], missing = \"fiml\", se = \"standard\", output = \"lavaan\")\nsummary(Neur_cors_fiml, standardized = TRUE)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        20\n\n  Number of observations                          2800\n  Number of missing patterns                        11\n\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  N1 ~~                                                                 \n    N2                1.693    0.056   30.482    0.000    1.693    0.707\n    N3                1.400    0.055   25.676    0.000    1.400    0.557\n    N4                0.980    0.050   19.466    0.000    0.980    0.398\n    N5                0.961    0.052   18.607    0.000    0.961    0.378\n  N2 ~~                                                                 \n    N3                1.342    0.053   25.424    0.000    1.342    0.549\n    N4                0.937    0.049   19.188    0.000    0.937    0.391\n    N5                0.867    0.050   17.449    0.000    0.867    0.351\n  N3 ~~                                                                 \n    N4                1.305    0.054   24.278    0.000    1.305    0.519\n    N5                1.110    0.054   20.749    0.000    1.110    0.428\n  N4 ~~                                                                 \n    N5                1.012    0.052   19.445    0.000    1.012    0.398\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    N1                2.932    0.030   98.584    0.000    2.932    1.866\n    N2                3.508    0.029  121.510    0.000    3.508    2.300\n    N3                3.217    0.030  106.148    0.000    3.217    2.008\n    N4                3.185    0.030  106.899    0.000    3.185    2.029\n    N5                2.969    0.031   96.675    0.000    2.969    1.834\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    N1                2.467    0.066   37.311    0.000    2.467    1.000\n    N2                2.326    0.062   37.328    0.000    2.326    1.000\n    N3                2.566    0.069   37.378    0.000    2.566    1.000\n    N4                2.464    0.066   37.193    0.000    2.464    1.000\n    N5                2.620    0.070   37.225    0.000    2.620    1.000",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estimating correlations with lavaan</span>"
    ]
  },
  {
    "objectID": "Spearman.html",
    "href": "Spearman.html",
    "title": "7  Single factor model",
    "section": "",
    "text": "library(psychTools)\nlibrary(lavaan)\nlibrary(flexplavaan)\n\nmod1 &lt;- '\nNeuroticism =~ N1 + N2 + N3 + N4 + N5\n'\n\nfit.mod1 &lt;- cfa(mod1, data = bfi)\nsummary(fit.mod1, standardized = TRUE, fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 26 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n                                                  Used       Total\n  Number of observations                          2694        2800\n\nModel Test User Model:\n                                                      \n  Test statistic                               360.932\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              4724.621\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.925\n  Tucker-Lewis Index (TLI)                       0.849\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -23078.504\n  Loglikelihood unrestricted model (H1)     -22898.038\n                                                      \n  Akaike (AIC)                               46177.007\n  Bayesian (BIC)                             46235.995\n  Sample-size adjusted Bayesian (SABIC)      46204.222\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.163\n  90 Percent confidence interval - lower         0.149\n  90 Percent confidence interval - upper         0.177\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.056\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Neuroticism =~                                                        \n    N1                1.000                               1.286    0.818\n    N2                0.952    0.023   40.826    0.000    1.225    0.803\n    N3                0.892    0.024   36.883    0.000    1.147    0.717\n    N4                0.677    0.024   27.818    0.000    0.872    0.554\n    N5                0.632    0.025   24.973    0.000    0.813    0.502\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                0.819    0.036   22.656    0.000    0.819    0.331\n   .N2                0.828    0.034   24.025    0.000    0.828    0.356\n   .N3                1.245    0.042   29.526    0.000    1.245    0.486\n   .N4                1.714    0.051   33.779    0.000    1.714    0.693\n   .N5                1.968    0.057   34.494    0.000    1.968    0.748\n    Neuroticism       1.655    0.070   23.756    0.000    1.000    1.000\n\nvisualize(fit.mod1, subset = 1:5)\n\n\n\n\n\n\n\nmeasurement_plot(fit.mod1, sample = 100)\n\n\n\n\n\n\n\nresidual_plots(fit.mod1)\n\n\n\n\n\n\n\n\n\nGWS2020 &lt;- read.csv(\"C:/Users/deondb/Downloads/GWS2020.csv\")\n\nmodgws &lt;- '\nStress =~ Item1 + Item2 + Item3 + Item4 + Item5 + Item6 + Item7 + Item8 + Item9\n'\n\nfit.modgws &lt;- cfa(modgws, data = GWS2020)\nsummary(fit.modgws, standardized = TRUE, fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 22 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        18\n\n  Number of observations                          1377\n\nModel Test User Model:\n                                                      \n  Test statistic                               808.572\n  Degrees of freedom                                27\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              6506.495\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.879\n  Tucker-Lewis Index (TLI)                       0.839\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -14826.170\n  Loglikelihood unrestricted model (H1)     -14421.884\n                                                      \n  Akaike (AIC)                               29688.341\n  Bayesian (BIC)                             29782.439\n  Sample-size adjusted Bayesian (SABIC)      29725.260\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.145\n  90 Percent confidence interval - lower         0.136\n  90 Percent confidence interval - upper         0.154\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.057\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Stress =~                                                             \n    Item1             1.000                               0.810    0.762\n    Item2             1.114    0.036   31.162    0.000    0.902    0.814\n    Item3             0.963    0.035   27.841    0.000    0.780    0.737\n    Item4             0.895    0.036   24.948    0.000    0.725    0.668\n    Item5             0.763    0.031   24.824    0.000    0.618    0.665\n    Item6             0.831    0.029   28.328    0.000    0.673    0.748\n    Item7             0.813    0.035   23.238    0.000    0.659    0.627\n    Item8             0.837    0.031   27.235    0.000    0.678    0.723\n    Item9             0.771    0.032   24.225    0.000    0.624    0.651\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Item1             0.474    0.021   22.527    0.000    0.474    0.419\n   .Item2             0.414    0.020   20.962    0.000    0.414    0.337\n   .Item3             0.511    0.022   23.054    0.000    0.511    0.457\n   .Item4             0.651    0.027   24.084    0.000    0.651    0.553\n   .Item5             0.481    0.020   24.118    0.000    0.481    0.557\n   .Item6             0.356    0.016   22.826    0.000    0.356    0.440\n   .Item7             0.671    0.027   24.515    0.000    0.671    0.607\n   .Item8             0.420    0.018   23.311    0.000    0.420    0.478\n   .Item9             0.530    0.022   24.278    0.000    0.530    0.576\n    Stress            0.656    0.041   16.177    0.000    1.000    1.000\n\nvisualize(fit.modgws, subset = c(1:9))\n\n\n\n\n\n\n\nmeasurement_plot(fit.modgws)\n\n\n\n\n\n\n\nresidual_plots(fit.modgws)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Single factor model</span>"
    ]
  },
  {
    "objectID": "Measurement_models.html",
    "href": "Measurement_models.html",
    "title": "8  Congeneric, tau-equivalent and parallel measurement models",
    "section": "",
    "text": "library(psychTools)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(flexplavaan)\n\n\nmod.congeneric &lt;- '\nN =~ N1 + N2 + N3 + N4 + N5\n'\n\nfit.mod.congeneric &lt;- cfa(mod.congeneric, data = bfi)\n\nvisualize(fit.mod.congeneric, subset = 1:5)\n\n\n\n\n\n\n\nmeasurement_plot(fit.mod.congeneric, sample = 300)\n\nNote: You didn't specify a reference for the ghost line, which means I get to chose it. That shows a lot of trust. I appreciate that. I won't let you down.\n\n\n\n\n\n\n\n\nresidual_plots(fit.mod.congeneric)\n\n\n\n\n\n\n\n\n\nmod.tau.eq &lt;- '\nN =~ a*N1 + a*N2 + a*N3 + a*N4 + a*N5\n'\n\nfit.mod.tau.eq &lt;- cfa(mod.tau.eq, data = bfi, std.lv = TRUE)\n\nvisualize(fit.mod.tau.eq, subset = 1:5)\n\n\n\n\n\n\n\nmeasurement_plot(fit.mod.tau.eq, sample = 300)\n\nNote: You didn't specify a reference for the ghost line, which means I get to chose it. That shows a lot of trust. I appreciate that. I won't let you down.\n\n\n\n\n\n\n\n\nresidual_plots(fit.mod.tau.eq)\n\n\n\n\n\n\n\n\n\nmod.parallel &lt;- '\nN =~ a*N1 + a*N2 + a*N3 + a*N4 + a*N5\n\nN1 ~~ b*N1\nN2 ~~ b*N2\nN3 ~~ b*N3\nN4 ~~ b*N4\nN5 ~~ b*N5\n'\n\nfit.mod.parallel &lt;- cfa(mod.parallel, data = bfi, std.lv = TRUE)\n\n\nvisualize(fit.mod.parallel, subset = 1:5)\n\n\n\n\n\n\n\nmeasurement_plot(fit.mod.parallel, sample = 300)\n\nNote: You didn't specify a reference for the ghost line, which means I get to chose it. That shows a lot of trust. I appreciate that. I won't let you down.\n\n\n\n\n\n\n\n\nresidual_plots(fit.mod.parallel)\n\n\n\n\n\n\n\n\n\nmodels.fit &lt;- compareFit(fit.mod.congeneric, fit.mod.tau.eq, fit.mod.parallel)\n\nsummary(models.fit)\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n                   Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)\nfit.mod.congeneric  5 46177 46236 360.93                                      \nfit.mod.tau.eq      9 46392 46428 584.30     223.36 0.14268       4  &lt; 2.2e-16\nfit.mod.parallel   13 46617 46628 816.60     232.31 0.14556       4  &lt; 2.2e-16\n                      \nfit.mod.congeneric    \nfit.mod.tau.eq     ***\nfit.mod.parallel   ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n####################### Model Fit Indices ###########################\n                      chisq df pvalue rmsea   cfi   tli  srmr        aic\nfit.mod.congeneric 360.932†  5   .000 .163  .925† .849  .056† 46177.007†\nfit.mod.tau.eq     584.296   9   .000 .154  .878  .864  .108  46392.372 \nfit.mod.parallel   816.601  13   .000 .151† .830  .869† .086  46616.677 \n                          bic\nfit.mod.congeneric 46235.995†\nfit.mod.tau.eq     46427.764 \nfit.mod.parallel   46628.475 \n\n################## Differences in Fit Indices #######################\n                                    df  rmsea    cfi   tli   srmr     aic\nfit.mod.tau.eq - fit.mod.congeneric  4 -0.009 -0.047 0.015  0.051 215.364\nfit.mod.parallel - fit.mod.tau.eq    4 -0.003 -0.048 0.004 -0.021 224.305\n                                        bic\nfit.mod.tau.eq - fit.mod.congeneric 191.769\nfit.mod.parallel - fit.mod.tau.eq   200.710",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Congeneric, tau-equivalent and parallel measurement models</span>"
    ]
  },
  {
    "objectID": "Multiple_factors.html",
    "href": "Multiple_factors.html",
    "title": "9  Multiple factors",
    "section": "",
    "text": "library(lavaan)\nlibrary(flexplavaan)\nlibrary(psychTools)\n\n\nmod3 &lt;- '\nNeuroticism   =~ N1 + N2 + N3 + N4 + N5\nAgreeableness =~ A1 + A2 + A3 + A4 + A5\n\nN1 ~~ N2\n'\n\nfit.mod3 &lt;- cfa(model = mod3, \n                data  = bfi)\n\nsummary(fit.mod3, \n        standardized = TRUE, \n        fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 39 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        22\n\n                                                  Used       Total\n  Number of observations                          2618        2800\n\nModel Test User Model:\n                                                      \n  Test statistic                               394.745\n  Degrees of freedom                                33\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              7465.706\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.951\n  Tucker-Lewis Index (TLI)                       0.934\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -43229.863\n  Loglikelihood unrestricted model (H1)     -43032.491\n                                                      \n  Akaike (AIC)                               86503.726\n  Bayesian (BIC)                             86632.870\n  Sample-size adjusted Bayesian (SABIC)      86562.969\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.065\n  90 Percent confidence interval - lower         0.059\n  90 Percent confidence interval - upper         0.071\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.048\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Neuroticism =~                                                        \n    N1                1.000                               1.069    0.681\n    N2                0.940    0.024   38.841    0.000    1.005    0.658\n    N3                1.206    0.041   29.617    0.000    1.289    0.807\n    N4                0.944    0.035   26.800    0.000    1.009    0.643\n    N5                0.833    0.035   23.538    0.000    0.890    0.549\n  Agreeableness =~                                                      \n    A1                1.000                               0.542    0.387\n    A2               -1.413    0.086  -16.450    0.000   -0.765   -0.651\n    A3               -1.838    0.109  -16.933    0.000   -0.996   -0.761\n    A4               -1.343    0.091  -14.789    0.000   -0.728   -0.488\n    A5               -1.487    0.091  -16.357    0.000   -0.806   -0.638\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .N1 ~~                                                                 \n   .N2                0.621    0.040   15.696    0.000    0.621    0.471\n  Neuroticism ~~                                                        \n    Agreeableness     0.117    0.016    7.304    0.000    0.203    0.203\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                1.319    0.048   27.230    0.000    1.319    0.536\n   .N2                1.319    0.047   28.112    0.000    1.319    0.567\n   .N3                0.890    0.048   18.678    0.000    0.890    0.349\n   .N4                1.447    0.049   29.606    0.000    1.447    0.587\n   .N5                1.836    0.057   32.264    0.000    1.836    0.699\n   .A1                1.668    0.049   34.239    0.000    1.668    0.850\n   .A2                0.795    0.029   27.568    0.000    0.795    0.576\n   .A3                0.719    0.035   20.374    0.000    0.719    0.420\n   .A4                1.692    0.052   32.681    0.000    1.692    0.762\n   .A5                0.943    0.033   28.181    0.000    0.943    0.592\n    Neuroticism       1.142    0.066   17.384    0.000    1.000    1.000\n    Agreeableness     0.293    0.033    9.006    0.000    1.000    1.000\n\nvisualize(fit.mod3, \n          subset = 1:10)\n\n\n\n\n\n\n\nmeasurement_plot(fit.mod3, \n                 sample = 300)\n\n$Neuroticism\n\n\n\n\n\n\n\n\n\n\n$Agreeableness\n\n\n\n\n\n\n\n\nresidual_plots(fit.mod3)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Multiple factors</span>"
    ]
  },
  {
    "objectID": "Plotting_models.html",
    "href": "Plotting_models.html",
    "title": "10  Plotting",
    "section": "",
    "text": "library(psychTools)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(semPlot)\nlibrary(psych)\n\n\nmod1 &lt;- '\nNeuroticism =~ N1 + N2 + N3 + N4 + N5\n'\n\nfit.mod1 &lt;- cfa(mod1, data = bfi, std.lv = TRUE)\nsummary(fit.mod1)\n\nlavaan 0.6-19 ended normally after 19 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n                                                  Used       Total\n  Number of observations                          2694        2800\n\nModel Test User Model:\n                                                      \n  Test statistic                               360.932\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  Neuroticism =~                                      \n    N1                1.286    0.027   47.511    0.000\n    N2                1.225    0.026   46.328    0.000\n    N3                1.147    0.029   39.920    0.000\n    N4                0.872    0.030   28.942    0.000\n    N5                0.813    0.032   25.766    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .N1                0.819    0.036   22.656    0.000\n   .N2                0.828    0.034   24.025    0.000\n   .N3                1.245    0.042   29.526    0.000\n   .N4                1.714    0.051   33.779    0.000\n   .N5                1.968    0.057   34.494    0.000\n    Neuroticism       1.000                           \n\n\n\nlibrary(semPlot)\nsemPaths(fit.mod1,\n         ###color       = \"black\",\n         what           = c(\"std\"),\n         weighted       = FALSE,\n         sizeLat        = 14,\n         sizeLat2       = 10,          \n         shapeLat       = \"ellipse\",\n         sizeMan        = 6,\n         nCharNodes     = 0,\n         edge.label.cex = 0.8,\n         label.cex      = 1, \n         edge.color     = \"black\",\n         mar            = c(5,5,5,5))",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "Modification_indices.html",
    "href": "Modification_indices.html",
    "title": "11  Modification Indices",
    "section": "",
    "text": "library(lavaan)\n\nThis is lavaan 0.6-19\nlavaan is FREE software! Please report any bugs.\n\nlibrary(psychTools)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::recode() masks psychTools::recode()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nmod3 &lt;- '\nNeuroticism   =~ N1 + N2 + N3 + N4 + N5\nAgreeableness =~ A1 + A2 + A3 + A4 + A5\n'\n\nfit.mod3 &lt;- cfa(model = mod3, \n                data  = bfi)\n\nsummary(fit.mod3, \n        standardized = TRUE, \n        fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 36 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n                                                  Used       Total\n  Number of observations                          2618        2800\n\nModel Test User Model:\n                                                      \n  Test statistic                               694.620\n  Degrees of freedom                                34\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              7465.706\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.911\n  Tucker-Lewis Index (TLI)                       0.882\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -43379.800\n  Loglikelihood unrestricted model (H1)     -43032.491\n                                                      \n  Akaike (AIC)                               86801.601\n  Bayesian (BIC)                             86924.874\n  Sample-size adjusted Bayesian (SABIC)      86858.151\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.086\n  90 Percent confidence interval - lower         0.081\n  90 Percent confidence interval - upper         0.092\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.966\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.054\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Neuroticism =~                                                        \n    N1                1.000                               1.292    0.823\n    N2                0.949    0.023   40.791    0.000    1.226    0.803\n    N3                0.884    0.024   36.571    0.000    1.141    0.714\n    N4                0.680    0.024   27.853    0.000    0.878    0.559\n    N5                0.626    0.025   24.591    0.000    0.809    0.499\n  Agreeableness =~                                                      \n    A1                1.000                               0.545    0.389\n    A2               -1.404    0.085  -16.527    0.000   -0.765   -0.651\n    A3               -1.822    0.107  -17.018    0.000   -0.992   -0.759\n    A4               -1.338    0.090  -14.855    0.000   -0.729   -0.489\n    A5               -1.483    0.090  -16.448    0.000   -0.808   -0.640\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Neuroticism ~~                                                        \n    Agreeableness     0.155    0.019    8.049    0.000    0.220    0.220\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                0.793    0.036   22.094    0.000    0.793    0.322\n   .N2                0.826    0.035   23.877    0.000    0.826    0.355\n   .N3                1.249    0.043   29.352    0.000    1.249    0.490\n   .N4                1.694    0.051   33.284    0.000    1.694    0.687\n   .N5                1.974    0.058   34.081    0.000    1.974    0.751\n   .A1                1.665    0.049   34.211    0.000    1.665    0.849\n   .A2                0.796    0.029   27.597    0.000    0.796    0.576\n   .A3                0.725    0.035   20.614    0.000    0.725    0.424\n   .A4                1.690    0.052   32.667    0.000    1.690    0.761\n   .A5                0.939    0.033   28.108    0.000    0.939    0.590\n    Neuroticism       1.668    0.070   23.696    0.000    1.000    1.000\n    Agreeableness     0.297    0.033    9.060    0.000    1.000    1.000\n\n\n\nmodificationindices(fit.mod3)\n\n             lhs op rhs      mi    epc sepc.lv sepc.all sepc.nox\n24   Neuroticism =~  A1  19.035  0.098   0.127    0.091    0.091\n25   Neuroticism =~  A2  20.795  0.079   0.102    0.087    0.087\n26   Neuroticism =~  A3  29.798  0.104   0.135    0.103    0.103\n27   Neuroticism =~  A4   5.262 -0.053  -0.069   -0.046   -0.046\n28   Neuroticism =~  A5  50.336 -0.132  -0.171   -0.135   -0.135\n29 Agreeableness =~  N1   0.630  0.038   0.021    0.013    0.013\n30 Agreeableness =~  N2   0.295  0.025   0.014    0.009    0.009\n31 Agreeableness =~  N3  10.202 -0.168  -0.091   -0.057   -0.057\n32 Agreeableness =~  N4  25.954  0.293   0.160    0.102    0.102\n33 Agreeableness =~  N5  11.128 -0.205  -0.112   -0.069   -0.069\n34            N1 ~~  N2 345.707  0.775   0.775    0.958    0.958\n35            N1 ~~  N3  52.136 -0.272  -0.272   -0.274   -0.274\n36            N1 ~~  N4  76.839 -0.291  -0.291   -0.251   -0.251\n37            N1 ~~  N5  19.022 -0.149  -0.149   -0.119   -0.119\n38            N1 ~~  A1  24.399  0.137   0.137    0.119    0.119\n39            N1 ~~  A2   4.495 -0.044  -0.044   -0.055   -0.055\n40            N1 ~~  A3  13.106  0.079   0.079    0.104    0.104\n41            N1 ~~  A4   5.855  0.069   0.069    0.059    0.059\n42            N1 ~~  A5   5.301 -0.051  -0.051   -0.059   -0.059\n43            N2 ~~  N3  37.647 -0.220  -0.220   -0.216   -0.216\n44            N2 ~~  N4  68.849 -0.266  -0.266   -0.225   -0.225\n45            N2 ~~  N5  42.497 -0.217  -0.217   -0.170   -0.170\n46            N2 ~~  A1   3.438  0.051   0.051    0.043    0.043\n47            N2 ~~  A2  14.146  0.077   0.077    0.095    0.095\n48            N2 ~~  A3   0.861  0.020   0.020    0.026    0.026\n49            N2 ~~  A4  14.742 -0.108  -0.108   -0.091   -0.091\n50            N2 ~~  A5   3.613 -0.042  -0.042   -0.048   -0.048\n51            N3 ~~  N4 163.251  0.436   0.436    0.300    0.300\n52            N3 ~~  N5  47.254  0.247   0.247    0.157    0.157\n53            N3 ~~  A1   0.111  0.010   0.010    0.007    0.007\n54            N3 ~~  A2   0.254  0.012   0.012    0.012    0.012\n55            N3 ~~  A3   3.474  0.046   0.046    0.048    0.048\n56            N3 ~~  A4   0.614  0.025   0.025    0.017    0.017\n57            N3 ~~  A5   0.171  0.010   0.010    0.010    0.010\n58            N4 ~~  N5  86.112  0.360   0.360    0.197    0.197\n59            N4 ~~  A1  17.767 -0.146  -0.146   -0.087   -0.087\n60            N4 ~~  A2   0.251  0.013   0.013    0.011    0.011\n61            N4 ~~  A3   3.030 -0.047  -0.047   -0.043   -0.043\n62            N4 ~~  A4  20.479 -0.161  -0.161   -0.095   -0.095\n63            N4 ~~  A5  10.813 -0.092  -0.092   -0.073   -0.073\n64            N5 ~~  A1   8.114 -0.106  -0.106   -0.058   -0.058\n65            N5 ~~  A2   7.324  0.075   0.075    0.060    0.060\n66            N5 ~~  A3   5.725 -0.069  -0.069   -0.058   -0.058\n67            N5 ~~  A4   8.224  0.109   0.109    0.060    0.060\n68            N5 ~~  A5   0.301  0.016   0.016    0.012    0.012\n69            A1 ~~  A2  64.589 -0.219  -0.219   -0.190   -0.190\n70            A1 ~~  A3   6.688  0.080   0.080    0.073    0.073\n71            A1 ~~  A4   4.991  0.080   0.080    0.048    0.048\n72            A1 ~~  A5  22.962  0.140   0.140    0.112    0.112\n73            A2 ~~  A3   0.654 -0.027  -0.027   -0.036   -0.036\n74            A2 ~~  A4   3.109  0.051   0.051    0.044    0.044\n75            A2 ~~  A5  20.439 -0.124  -0.124   -0.143   -0.143\n76            A3 ~~  A4   0.417 -0.022  -0.022   -0.020   -0.020\n77            A3 ~~  A5  31.301  0.197   0.197    0.239    0.239\n78            A4 ~~  A5   0.192 -0.014  -0.014   -0.011   -0.011\n\nmymi &lt;- modificationindices(fit.mod3)\n\n\nmymi %&gt;% \n  filter(op == \"~~\") %&gt;% \n  arrange(desc(mi)) %&gt;% \n  slice(1:20)\n\n   lhs op rhs      mi    epc sepc.lv sepc.all sepc.nox\n1   N1 ~~  N2 345.707  0.775   0.775    0.958    0.958\n2   N3 ~~  N4 163.251  0.436   0.436    0.300    0.300\n3   N4 ~~  N5  86.112  0.360   0.360    0.197    0.197\n4   N1 ~~  N4  76.839 -0.291  -0.291   -0.251   -0.251\n5   N2 ~~  N4  68.849 -0.266  -0.266   -0.225   -0.225\n6   A1 ~~  A2  64.589 -0.219  -0.219   -0.190   -0.190\n7   N1 ~~  N3  52.136 -0.272  -0.272   -0.274   -0.274\n8   N3 ~~  N5  47.254  0.247   0.247    0.157    0.157\n9   N2 ~~  N5  42.497 -0.217  -0.217   -0.170   -0.170\n10  N2 ~~  N3  37.647 -0.220  -0.220   -0.216   -0.216\n11  A3 ~~  A5  31.301  0.197   0.197    0.239    0.239\n12  N1 ~~  A1  24.399  0.137   0.137    0.119    0.119\n13  A1 ~~  A5  22.962  0.140   0.140    0.112    0.112\n14  N4 ~~  A4  20.479 -0.161  -0.161   -0.095   -0.095\n15  A2 ~~  A5  20.439 -0.124  -0.124   -0.143   -0.143\n16  N1 ~~  N5  19.022 -0.149  -0.149   -0.119   -0.119\n17  N4 ~~  A1  17.767 -0.146  -0.146   -0.087   -0.087\n18  N2 ~~  A4  14.742 -0.108  -0.108   -0.091   -0.091\n19  N2 ~~  A2  14.146  0.077   0.077    0.095    0.095\n20  N1 ~~  A3  13.106  0.079   0.079    0.104    0.104\n\nmymi %&gt;% \n  filter(op == \"~~\" & mi &gt; 100)\n\n  lhs op rhs      mi   epc sepc.lv sepc.all sepc.nox\n1  N1 ~~  N2 345.707 0.775   0.775    0.958    0.958\n2  N3 ~~  N4 163.251 0.436   0.436    0.300    0.300\n\n\n\nmod4 &lt;- '\nNeuroticism   =~ N1 + N2 + N3 + N4 + N5\nAgreeableness =~ A1 + A2 + A3 + A4 + A5\nN1 ~~ N2\n'\n\nfit.mod4 &lt;- cfa(model = mod4, \n                data  = bfi)\n\nsummary(fit.mod4, \n        standardized = TRUE, \n        fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 39 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        22\n\n                                                  Used       Total\n  Number of observations                          2618        2800\n\nModel Test User Model:\n                                                      \n  Test statistic                               394.745\n  Degrees of freedom                                33\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              7465.706\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.951\n  Tucker-Lewis Index (TLI)                       0.934\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -43229.863\n  Loglikelihood unrestricted model (H1)     -43032.491\n                                                      \n  Akaike (AIC)                               86503.726\n  Bayesian (BIC)                             86632.870\n  Sample-size adjusted Bayesian (SABIC)      86562.969\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.065\n  90 Percent confidence interval - lower         0.059\n  90 Percent confidence interval - upper         0.071\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.048\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Neuroticism =~                                                        \n    N1                1.000                               1.069    0.681\n    N2                0.940    0.024   38.841    0.000    1.005    0.658\n    N3                1.206    0.041   29.617    0.000    1.289    0.807\n    N4                0.944    0.035   26.800    0.000    1.009    0.643\n    N5                0.833    0.035   23.538    0.000    0.890    0.549\n  Agreeableness =~                                                      \n    A1                1.000                               0.542    0.387\n    A2               -1.413    0.086  -16.450    0.000   -0.765   -0.651\n    A3               -1.838    0.109  -16.933    0.000   -0.996   -0.761\n    A4               -1.343    0.091  -14.789    0.000   -0.728   -0.488\n    A5               -1.487    0.091  -16.357    0.000   -0.806   -0.638\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .N1 ~~                                                                 \n   .N2                0.621    0.040   15.696    0.000    0.621    0.471\n  Neuroticism ~~                                                        \n    Agreeableness     0.117    0.016    7.304    0.000    0.203    0.203\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                1.319    0.048   27.230    0.000    1.319    0.536\n   .N2                1.319    0.047   28.112    0.000    1.319    0.567\n   .N3                0.890    0.048   18.678    0.000    0.890    0.349\n   .N4                1.447    0.049   29.606    0.000    1.447    0.587\n   .N5                1.836    0.057   32.264    0.000    1.836    0.699\n   .A1                1.668    0.049   34.239    0.000    1.668    0.850\n   .A2                0.795    0.029   27.568    0.000    0.795    0.576\n   .A3                0.719    0.035   20.374    0.000    0.719    0.420\n   .A4                1.692    0.052   32.681    0.000    1.692    0.762\n   .A5                0.943    0.033   28.181    0.000    0.943    0.592\n    Neuroticism       1.142    0.066   17.384    0.000    1.000    1.000\n    Agreeableness     0.293    0.033    9.006    0.000    1.000    1.000\n\n\n\ncompfit &lt;- semTools::compareFit(fit.mod3, fit.mod4)\nsummary(compfit)\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n         Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nfit.mod4 33 86504 86633 394.75                                          \nfit.mod3 34 86802 86925 694.62     299.87 0.33788       1  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n####################### Model Fit Indices ###########################\n            chisq df pvalue rmsea   cfi   tli  srmr        aic        bic\nfit.mod4 394.745† 33   .000 .065† .951† .934† .048† 86503.726† 86632.870†\nfit.mod3 694.620  34   .000 .086  .911  .882  .054  86801.601  86924.874 \n\n################## Differences in Fit Indices #######################\n                    df rmsea   cfi    tli  srmr     aic     bic\nfit.mod3 - fit.mod4  1 0.021 -0.04 -0.051 0.005 297.874 292.004",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modification Indices</span>"
    ]
  },
  {
    "objectID": "EFA.html",
    "href": "EFA.html",
    "title": "12  Exploratory factor analysis",
    "section": "",
    "text": "library(lavaan)\nlibrary(flexplavaan)\nlibrary(psychTools)\nlibrary(psych)\nlibrary(tidyverse)\n\nbfi %&gt;% \n  select(1:5, 16:20) %&gt;% \n  scree()\n\n\n\n\n\n\n\nbfi %&gt;% \n  select(1:5, 16:20) %&gt;% \n  fa.parallel()\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  3  and the number of components =  2 \n\nmyfa &lt;- bfi %&gt;% \n  select(1:5, 16:20) %&gt;% \n  fa(nfactors = 2)\n\nprint(myfa$loadings, cut = 0)\n\n\nLoadings:\n   MR1    MR2   \nA1  0.074 -0.359\nA2  0.054  0.692\nA3  0.031  0.756\nA4 -0.052  0.474\nA5 -0.119  0.595\nN1  0.776 -0.027\nN2  0.760 -0.022\nN3  0.770  0.051\nN4  0.581 -0.078\nN5  0.543  0.078\n\n                 MR1   MR2\nSS loadings    2.432 1.773\nProportion Var 0.243 0.177\nCumulative Var 0.243 0.420\n\ndata.frame(myfa$complexity) %&gt;% \n  arrange(myfa.complexity)\n\n   myfa.complexity\nN2        1.001752\nN1        1.002362\nA3        1.003383\nN3        1.008872\nA2        1.012251\nA4        1.023934\nN4        1.035619\nN5        1.041347\nA5        1.079964\nA1        1.085700",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploratory factor analysis</span>"
    ]
  },
  {
    "objectID": "Correlated_residuals.html",
    "href": "Correlated_residuals.html",
    "title": "13  Correlated residuals",
    "section": "",
    "text": "library(psychTools)\nlibrary(lavaan)\nlibrary(flexplavaan)\n\nmod4 &lt;- '\nNeuroticism   =~ N1 + N2 + N3 + N4 + N5\nAgreeableness =~ A1 + A2 + A3 + A4 + A5\n\nN1 ~~ N2\nN4 ~~ N5\n'\n\nfit.mod4 &lt;- cfa(mod4, \n                data = bfi, std.lv = TRUE)\n\nsummary(fit.mod4, \n        standardized = TRUE, \n        fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 25 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        23\n\n                                                  Used       Total\n  Number of observations                          2618        2800\n\nModel Test User Model:\n                                                      \n  Test statistic                               374.075\n  Degrees of freedom                                32\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              7465.706\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.954\n  Tucker-Lewis Index (TLI)                       0.935\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -43219.528\n  Loglikelihood unrestricted model (H1)     -43032.491\n                                                      \n  Akaike (AIC)                               86485.056\n  Bayesian (BIC)                             86620.070\n  Sample-size adjusted Bayesian (SABIC)      86546.992\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.064\n  90 Percent confidence interval - lower         0.058\n  90 Percent confidence interval - upper         0.070\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.048\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Neuroticism =~                                                        \n    N1                1.072    0.031   34.548    0.000    1.072    0.684\n    N2                1.010    0.030   33.138    0.000    1.010    0.662\n    N3                1.308    0.031   42.422    0.000    1.308    0.819\n    N4                0.980    0.031   31.196    0.000    0.980    0.624\n    N5                0.853    0.034   25.396    0.000    0.853    0.526\n  Agreeableness =~                                                      \n    A1                0.542    0.030   18.024    0.000    0.542    0.387\n    A2               -0.766    0.024  -32.231    0.000   -0.766   -0.652\n    A3               -0.996    0.026  -38.227    0.000   -0.996   -0.761\n    A4               -0.728    0.031  -23.243    0.000   -0.728   -0.488\n    A5               -0.805    0.026  -31.494    0.000   -0.805   -0.638\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .N1 ~~                                                                 \n   .N2                0.612    0.040   15.115    0.000    0.612    0.467\n .N4 ~~                                                                 \n   .N5                0.180    0.040    4.451    0.000    0.180    0.106\n  Neuroticism ~~                                                        \n    Agreeableness     0.200    0.024    8.297    0.000    0.200    0.200\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                1.312    0.049   26.597    0.000    1.312    0.533\n   .N2                1.309    0.048   27.511    0.000    1.309    0.562\n   .N3                0.841    0.051   16.442    0.000    0.841    0.329\n   .N4                1.504    0.051   29.470    0.000    1.504    0.610\n   .N5                1.900    0.059   31.996    0.000    1.900    0.723\n   .A1                1.668    0.049   34.235    0.000    1.668    0.850\n   .A2                0.795    0.029   27.550    0.000    0.795    0.575\n   .A3                0.719    0.035   20.381    0.000    0.719    0.420\n   .A4                1.692    0.052   32.680    0.000    1.692    0.762\n   .A5                0.943    0.033   28.186    0.000    0.943    0.593\n    Neuroticism       1.000                               1.000    1.000\n    Agreeableness     1.000                               1.000    1.000\n\n\n\nlibrary(semPlot)\nsemPaths(fit.mod4,\n         ###color       = \"black\",\n         what           = c(\"std\"),\n         weighted       = FALSE,\n         sizeLat        = 14,\n         sizeLat2       = 10,          \n         shapeLat       = \"ellipse\",\n         sizeMan        = 6,\n         nCharNodes     = 0,\n         edge.label.cex = 0.8,\n         label.cex      = 1, \n         edge.color     = \"black\",\n         mar            = c(5,5,5,5),\n         style          = \"lisrel\")",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlated residuals</span>"
    ]
  },
  {
    "objectID": "MissingDataExamples.html",
    "href": "MissingDataExamples.html",
    "title": "14  Dealing with missing data",
    "section": "",
    "text": "14.1 R packages used in the demonstration\nWe use the following packages: psychTools (to access the bfi data set), mice (to detect the patterns of missing data), naniar (to perform Little’s MCAR test) , Amelia (to perform the multiple imputation), lavaan (to perform the confirmatory factor analysis), and lavaan.mi (to perform the confirmatory factor analysis with multiple complete data sets).\nThese packages need to be installed once. To install the packages you need to remove the # at the beginning of each row (R does not evaluate any code that follows a #).\n#install.packages(\"psychTools\")\n#install.packages(\"naniar\")\n#install.packages(\"lavaan\")\n#install.packages(\"mice\")\n#install.packages(\"Amelia\")\nThe lavaan.mi package is not yet available from the CRAN repository, but it can be downloaded and installed from Terence Jorgenson’s github page. For this you will also need the remotes package.\n#install.packages(\"remotes\")\n#remotes::install_github(\"TDJorgensen/lavaan.mi\")",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Dealing with missing data</span>"
    ]
  },
  {
    "objectID": "MissingDataExamples.html#cfa-of-the-5-item-neuroticism-scale-of-the-big-five-inventory",
    "href": "MissingDataExamples.html#cfa-of-the-5-item-neuroticism-scale-of-the-big-five-inventory",
    "title": "14  Dealing with missing data",
    "section": "14.2 CFA of the 5-item Neuroticism scale of the Big Five Inventory",
    "text": "14.2 CFA of the 5-item Neuroticism scale of the Big Five Inventory\nI performed a confirmatory factor analysis of the Neuroticism scale of the Big Five Inventory. The data are in the bfi data frame, which can be found in the psychTools package. There were 2800 participants, but there were only 2694 complete cases. For convenience, I stored the five Neuroticism items (which are in columns 16 to 20 of the bfi data frame) in a new data frame called Ndata.\nIn this baseline analysis I ignored the missing data. By default, lavaan employs listwise deletion when missing data are encountered. At the top of the output it can be seen that the number of observed (n = 2800) and used (n = 2694) cases differ.\n\nlibrary(psychTools)\nlibrary(lavaan)\nlibrary(lavaan.mi)\n\nNdata &lt;- bfi[16:20]\n\nNmodel  &lt;- '\nNfactor =~ N1 + N2 + N3 + N4 + N5\n'\n\nfit.Nmodel &lt;- cfa(Nmodel, \n                  data      = Ndata, \n                  estimator = \"MLR\")\n\nsummary(fit.Nmodel,\n        standardized = TRUE,\n        fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 26 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n                                                  Used       Total\n  Number of observations                          2694        2800\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               360.932     313.521\n  Degrees of freedom                                 5           5\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.151\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              4724.621    3492.154\n  Degrees of freedom                                10          10\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.353\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.925       0.911\n  Tucker-Lewis Index (TLI)                       0.849       0.823\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.925\n  Robust Tucker-Lewis Index (TLI)                            0.849\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -23078.504  -23078.504\n  Scaling correction factor                                  1.007\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -22898.038  -22898.038\n  Scaling correction factor                                  1.055\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               46177.007   46177.007\n  Bayesian (BIC)                             46235.995   46235.995\n  Sample-size adjusted Bayesian (SABIC)      46204.222   46204.222\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.163       0.151\n  90 Percent confidence interval - lower         0.149       0.138\n  90 Percent confidence interval - upper         0.177       0.165\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.162\n  90 Percent confidence interval - lower                     0.147\n  90 Percent confidence interval - upper                     0.178\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.056       0.056\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Nfactor =~                                                            \n    N1                1.000                               1.286    0.818\n    N2                0.952    0.017   54.734    0.000    1.225    0.803\n    N3                0.892    0.028   31.540    0.000    1.147    0.717\n    N4                0.677    0.030   22.224    0.000    0.872    0.554\n    N5                0.632    0.030   21.255    0.000    0.813    0.502\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                0.819    0.048   16.954    0.000    0.819    0.331\n   .N2                0.828    0.046   18.124    0.000    0.828    0.356\n   .N3                1.245    0.052   23.807    0.000    1.245    0.486\n   .N4                1.714    0.055   31.118    0.000    1.714    0.693\n   .N5                1.968    0.057   34.381    0.000    1.968    0.748\n    Nfactor           1.655    0.065   25.460    0.000    1.000    1.000",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Dealing with missing data</span>"
    ]
  },
  {
    "objectID": "MissingDataExamples.html#examining-patterns-of-missing-data-and-testing-for-missing-completely-at-random",
    "href": "MissingDataExamples.html#examining-patterns-of-missing-data-and-testing-for-missing-completely-at-random",
    "title": "14  Dealing with missing data",
    "section": "14.3 Examining patterns of missing data and testing for “missing completely at random”",
    "text": "14.3 Examining patterns of missing data and testing for “missing completely at random”\nNext, I employed the md.pattern() function of the mice package to identify the patterns of missing data. I also used the mcar_test() function of the naniar package to perform Little’s missing completely at random (MCAR) test. There were 11 missing data patterns. There were 11 missing values for item N3, 21 for item N2, 22 for item N1, 29 for item N5, and 36 for item N4, which gives a total of 119 missing values. The pattern with the most missing values contained three missing values (four persons produced this pattern).\nLittle’s MCAR test showed that the null hypothesis that the missing data are completely at random could not be rejected: \\(\\chi^2(34) = 27.2, p = 0.791\\)\n\nlibrary(mice)\nmd.pattern(Ndata)\n\n\n\n\n\n\n\n\n     N3 N2 N1 N5 N4    \n2694  1  1  1  1  1   0\n32    1  1  1  1  0   1\n22    1  1  1  0  1   1\n15    1  1  0  1  1   1\n2     1  1  0  0  1   2\n4     1  1  0  0  0   3\n19    1  0  1  1  1   1\n1     1  0  1  0  1   2\n9     0  1  1  1  1   1\n1     0  1  0  1  1   2\n1     0  0  1  1  1   2\n     11 21 22 29 36 119\n\nlibrary(naniar)\nmcar_test(Ndata)\n\n# A tibble: 1 × 4\n  statistic    df p.value missing.patterns\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;int&gt;\n1      27.2    34   0.791               11",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Dealing with missing data</span>"
    ]
  },
  {
    "objectID": "MissingDataExamples.html#full-information-maximum-likelihood-estimation",
    "href": "MissingDataExamples.html#full-information-maximum-likelihood-estimation",
    "title": "14  Dealing with missing data",
    "section": "14.4 Full information maximum likelihood estimation",
    "text": "14.4 Full information maximum likelihood estimation\nSecond, I estimated the parameters of the confirmatory factor analysis model using full information maximum likelihood (“fiml”). This approach uses all the available information in the data set to estimate the parameters. It does not estimate what the missing values would be and it does not fill it in to complete the data set. Note that all 2800 cases were now used.\n\nfit.Nmodel.fiml &lt;- cfa(Nmodel,\n                       data      = Ndata, \n                       estimator = \"MLR\", \n                       missing   = \"fiml\",\n                       fixed.x   = FALSE)\n\nsummary(fit.Nmodel.fiml,\n        standardized = TRUE,\n        fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 30 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n\n  Number of observations                          2800\n  Number of missing patterns                        11\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               372.501     322.671\n  Degrees of freedom                                 5           5\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.154\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              4874.228    3604.773\n  Degrees of freedom                                10          10\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.352\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.924       0.912\n  Tucker-Lewis Index (TLI)                       0.849       0.823\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.924\n  Robust Tucker-Lewis Index (TLI)                            0.848\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -23768.675  -23768.675\n  Scaling correction factor                                  1.004\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -23582.425  -23582.425\n  Scaling correction factor                                  1.041\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               47567.350   47567.350\n  Bayesian (BIC)                             47656.411   47656.411\n  Sample-size adjusted Bayesian (SABIC)      47608.751   47608.751\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.162       0.151\n  90 Percent confidence interval - lower         0.148       0.138\n  90 Percent confidence interval - upper         0.176       0.164\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.163\n  90 Percent confidence interval - lower                     0.148\n  90 Percent confidence interval - upper                     0.179\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.049       0.049\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Nfactor =~                                                            \n    N1                1.000                               1.284    0.818\n    N2                0.956    0.017   55.605    0.000    1.227    0.804\n    N3                0.896    0.028   32.194    0.000    1.151    0.718\n    N4                0.677    0.030   22.550    0.000    0.869    0.554\n    N5                0.632    0.029   21.614    0.000    0.811    0.501\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                2.932    0.030   98.558    0.000    2.932    1.867\n   .N2                3.508    0.029  121.478    0.000    3.508    2.300\n   .N3                3.217    0.030  106.137    0.000    3.217    2.008\n   .N4                3.185    0.030  106.886    0.000    3.185    2.030\n   .N5                2.969    0.031   96.676    0.000    2.969    1.834\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                0.818    0.047   17.284    0.000    0.818    0.331\n   .N2                0.821    0.045   18.271    0.000    0.821    0.353\n   .N3                1.242    0.051   24.204    0.000    1.242    0.484\n   .N4                1.707    0.054   31.557    0.000    1.707    0.693\n   .N5                1.962    0.056   34.977    0.000    1.962    0.749\n    Nfactor           1.649    0.064   25.915    0.000    1.000    1.000",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Dealing with missing data</span>"
    ]
  },
  {
    "objectID": "MissingDataExamples.html#multiple-imputation-with-the-amelia-and-lavaan.mi-packages",
    "href": "MissingDataExamples.html#multiple-imputation-with-the-amelia-and-lavaan.mi-packages",
    "title": "14  Dealing with missing data",
    "section": "14.5 Multiple imputation with the Amelia and lavaan.mi packages",
    "text": "14.5 Multiple imputation with the Amelia and lavaan.mi packages\nNext, I used the Amelia and lavaan.mi packages to (a) perform multiple imputation to obtain 20 data sets that contain plausible estimates of the missing values, (b) fit the confirmatory factor analysis model to each of the data sets, and (c) report the pooled results.\nThe 20 complete data sets were stored as a list in an object I labeled Ndata.mi. I stored this list in a new object I labeled imps.\n\nlibrary(Amelia)\nlibrary(lavaan.mi)\n\nset.seed(12345)\nNdata.mi      &lt;- amelia(Ndata, \n                        m = 20)\n\nimps          &lt;- Ndata.mi$imputations\n\n\n14.5.1 Fit the model to the imputed data sets and pool the results\nI fitted the confirmatory factor analysis model to the imputed data sets in imps using the cfa.mi() function of the lavaan.mi package. The results are stored in fit.Nmodel.mi. We access the results by asking for a summary. Note that the parameter estimates are very similar to those obtained with listwise deletion and full information maximum likelihood estimation. Also, the standard errors of the parameters are very similar across the three analyses.\n\nfit.Nmodel.mi &lt;- cfa.mi(Nmodel, \n                        data      = imps,\n                        estimator = \"MLR\")\n\nsummary(fit.Nmodel.mi, \n        fit.measures = TRUE,\n        standardized = TRUE)\n\nlavaan.mi object fit to 20 imputed data sets using:\n - lavaan    (0.6-19)\n - lavaan.mi (0.1-0.0030)\nSee class?lavaan.mi help page for available methods. \n\nConvergence information:\nThe model converged on 20 imputed data sets.\nStandard errors were available for all imputations.\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                          2800\n\nModel Test User Model:\n\n                                                    Standard      Scaled\n  Test statistic                                     375.374     326.698\n  Degrees of freedom                                       5           5\n  P-value                                              0.000       0.000\n  Average scaling correction factor                                1.149\n  Pooling method                                          D4            \n    Pooled statistic                              \"standard\"            \n    \"yuan.bentler.mplus\" correction applied            AFTER     pooling\n\nModel Test Baseline Model:\n\n  Test statistic                              4849.800    3598.394\n  Degrees of freedom                                10          10\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.348\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.923       0.910\n  Tucker-Lewis Index (TLI)                       0.847       0.821\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.924\n  Robust Tucker-Lewis Index (TLI)                            0.847\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -23961.856  -23961.856\n  Scaling correction factor                                  1.005\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -23770.632  -23770.632\n  Scaling correction factor                                  1.054\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               47943.711   47943.711\n  Bayesian (BIC)                             48003.085   48003.085\n  Sample-size adjusted Bayesian (SABIC)      47971.312   47971.312\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.163       0.152\n  90 Percent confidence interval - lower         0.149       0.139\n  90 Percent confidence interval - upper         0.177       0.165\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.162\n  90 Percent confidence interval - lower                     0.148\n  90 Percent confidence interval - upper                     0.178\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.057       0.057\n\nParameter Estimates:\n\n  Standard errors                                    Sandwich\n  Information bread                                  Observed\n  Observed information based on                       Hessian\n                                                             \n  Pooled across imputations              Rubin's (1987) rules\n  Augment within-imputation variance     Scale by average RIV\n  Wald test for pooled parameters          t(df) distribution\n\n  Pooled t statistics with df &gt;= 1000 are displayed with\n  df = Inf(inity) to save space. Although the t distribution\n  with large df closely approximates a standard normal\n  distribution, exact df for reporting these t tests can be\n  obtained from parameterEstimates.mi() \n\n\nLatent Variables:\n                   Estimate  Std.Err  t-value       df  P(&gt;|t|)   Std.lv\n  Nfactor =~                                                            \n    N1                1.000                                        1.285\n    N2                0.955    0.017   55.650      Inf    0.000    1.227\n    N3                0.896    0.028   32.023      Inf    0.000    1.151\n    N4                0.677    0.030   22.524      Inf    0.000    0.870\n    N5                0.631    0.029   21.570      Inf    0.000    0.811\n  Std.all\n         \n    0.818\n    0.804\n    0.719\n    0.554\n    0.501\n\nVariances:\n                   Estimate  Std.Err  t-value       df  P(&gt;|t|)   Std.lv\n   .N1                0.818    0.047   17.270      Inf    0.000    0.818\n   .N2                0.821    0.045   18.257      Inf    0.000    0.821\n   .N3                1.239    0.051   24.060      Inf    0.000    1.239\n   .N4                1.707    0.054   31.524      Inf    0.000    1.707\n   .N5                1.961    0.056   34.947      Inf    0.000    1.961\n    Nfactor           1.650    0.064   25.835      Inf    0.000    1.000\n  Std.all\n    0.331\n    0.353\n    0.483\n    0.693\n    0.749\n    1.000",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Dealing with missing data</span>"
    ]
  },
  {
    "objectID": "MissingDataExamples.html#multiple-imputation-with-ordinal-variables",
    "href": "MissingDataExamples.html#multiple-imputation-with-ordinal-variables",
    "title": "14  Dealing with missing data",
    "section": "14.6 Multiple imputation with ordinal variables",
    "text": "14.6 Multiple imputation with ordinal variables\nThe items of the Neuroticism scale are strictly ordinal with six ordered categories. The observed values should be 1, 2, 3, 4, 5 or 6. If we don’t instruct amelia() to treat the items as ordinal, the function will treat them as continuous variables and the imputed values will contain decimals (which is not what we want). The imputed values may even extend beyond the range of the original six-point rating scale (which is really not what we want). Here I impute the missing data with the amelia() function, but now with the added argument that the items are ordinal. The imputed values will now be integers rather than decimals, which is what we want. The results are almost indistinguishable from the previous results.\n\nlibrary(Amelia)\nlibrary(lavaan.mi)\n\nset.seed(12345)\nNdata.mi2      &lt;- amelia(Ndata, \n                         m    = 20, \n                         ords = c(\"N1\", \"N2\", \"N3\", \"N4\", \"N5\"))\n\nimps2          &lt;- Ndata.mi2$imputations\n\n\nfit.Nmodel.mi2 &lt;- cfa.mi(Nmodel, \n                         data      = imps2,\n                         estimator = \"MLR\")\n\nsummary(fit.Nmodel.mi2, \n        fit.measures = TRUE,\n        standardized = TRUE)\n\nlavaan.mi object fit to 20 imputed data sets using:\n - lavaan    (0.6-19)\n - lavaan.mi (0.1-0.0030)\nSee class?lavaan.mi help page for available methods. \n\nConvergence information:\nThe model converged on 20 imputed data sets.\nStandard errors were available for all imputations.\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                          2800\n\nModel Test User Model:\n\n                                                    Standard      Scaled\n  Test statistic                                     363.290     315.881\n  Degrees of freedom                                       5           5\n  P-value                                              0.000       0.000\n  Average scaling correction factor                                1.150\n  Pooling method                                          D4            \n    Pooled statistic                              \"standard\"            \n    \"yuan.bentler.mplus\" correction applied            AFTER     pooling\n\nModel Test Baseline Model:\n\n  Test statistic                              4776.413    3547.745\n  Degrees of freedom                                10          10\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.346\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.925       0.912\n  Tucker-Lewis Index (TLI)                       0.850       0.824\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.925\n  Robust Tucker-Lewis Index (TLI)                            0.850\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -23982.925  -23982.925\n  Scaling correction factor                                  1.003\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -23795.906  -23795.906\n  Scaling correction factor                                  1.052\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               47985.849   47985.849\n  Bayesian (BIC)                             48045.223   48045.223\n  Sample-size adjusted Bayesian (SABIC)      48013.449   48013.449\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.160       0.149\n  90 Percent confidence interval - lower         0.146       0.136\n  90 Percent confidence interval - upper         0.174       0.162\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.160\n  90 Percent confidence interval - lower                     0.145\n  90 Percent confidence interval - upper                     0.175\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.057       0.057\n\nParameter Estimates:\n\n  Standard errors                                    Sandwich\n  Information bread                                  Observed\n  Observed information based on                       Hessian\n                                                             \n  Pooled across imputations              Rubin's (1987) rules\n  Augment within-imputation variance     Scale by average RIV\n  Wald test for pooled parameters          t(df) distribution\n\n  Pooled t statistics with df &gt;= 1000 are displayed with\n  df = Inf(inity) to save space. Although the t distribution\n  with large df closely approximates a standard normal\n  distribution, exact df for reporting these t tests can be\n  obtained from parameterEstimates.mi() \n\n\nLatent Variables:\n                   Estimate  Std.Err  t-value       df  P(&gt;|t|)   Std.lv\n  Nfactor =~                                                            \n    N1                1.000                                        1.284\n    N2                0.955    0.017   55.326      Inf    0.000    1.226\n    N3                0.896    0.028   32.064      Inf    0.000    1.151\n    N4                0.678    0.030   22.568      Inf    0.000    0.870\n    N5                0.631    0.029   21.614      Inf    0.000    0.810\n  Std.all\n         \n    0.817\n    0.803\n    0.718\n    0.554\n    0.501\n\nVariances:\n                   Estimate  Std.Err  t-value       df  P(&gt;|t|)   Std.lv\n   .N1                0.823    0.047   17.334      Inf    0.000    0.823\n   .N2                0.827    0.045   18.370      Inf    0.000    0.827\n   .N3                1.243    0.051   24.148      Inf    0.000    1.243\n   .N4                1.712    0.054   31.665      Inf    0.000    1.712\n   .N5                1.963    0.056   35.022      Inf    0.000    1.963\n    Nfactor           1.650    0.064   25.816      Inf    0.000    1.000\n  Std.all\n    0.333\n    0.355\n    0.484\n    0.693\n    0.749\n    1.000",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Dealing with missing data</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "15  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  }
]