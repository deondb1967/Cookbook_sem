[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Structural equation modeling with lavaan in R: Examples and excercises",
    "section": "",
    "text": "Preface\nThis book contains examples of how to fit structural equation models to observed data with the lavaan package in R. The book also contains exercises to help students build their skills.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Installing_packages.html",
    "href": "Installing_packages.html",
    "title": "2  Packages",
    "section": "",
    "text": "2.1 Exercise",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "Installing_packages.html#exercise",
    "href": "Installing_packages.html#exercise",
    "title": "2  Packages",
    "section": "",
    "text": "Download and install the janitor package from CRAN.\nDownload and install the EFAtools package from CRAN.\nDownload and install the FAtools package from Github. It is available at the following address: “mattkcole/FAtools” on Github.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "Vectors_dataframes.html",
    "href": "Vectors_dataframes.html",
    "title": "3  Vectors and data frames",
    "section": "",
    "text": "3.1 Vector\nIn R a vector is a collection of elements that are all of the same type. A typical data set consists of a number of rows that each represents a case, and a number of columns. Each of the columns is a vector.\nWe can create a vector in R by concatenating the elements. We do this by typing the elements inside round brackets. Each element should be separated by a comma.String or character elements should be enclosed in quotation marks. Finally, we place a c before the round brackets.\nFor instance, say we have the following elements, 10, 8, 12, 15. We can combine them into a vector as follows: c(10, 8, 12, 15). Similarly, we can combine the elements Dutch, Mandarin, Swahili and Russian into a vector as follows: c(“Dutch”, “Mandarin”, “Swahili”, “Russian”).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors and data frames</span>"
    ]
  },
  {
    "objectID": "Vectors_dataframes.html#a-data-frame-consists-of-vectors",
    "href": "Vectors_dataframes.html#a-data-frame-consists-of-vectors",
    "title": "3  Vectors and data frames",
    "section": "3.2 A data frame consists of vectors",
    "text": "3.2 A data frame consists of vectors\nConsider the following mini data set. There are four rows, each of which contains data of a particular person. There are five columns, each of which represents a variable. Note that each of the columns is a vector. The first column (name) vector is a character vector, the second (sex) is also a character vector, the third (division) is a numeric vector, the fourth (graduate) is a logical vector, and the fifth (experience) is a numeric vector. The numbers in the division vector have the following meaning: 1 = Finance, 2 = HR, and 3 = Marketing.\n\n\n\n\n\n\n\n\nname\nsex\ndivision\ngraduate\nexperience\n\n\n\n\nPeter\nmale\n1\nTRUE\n1\n\n\nPaul\nmale\n2\nFALSE\n3\n\n\nMary\nfemale\n2\nTRUE\n6\n\n\nJanette\nfemale\n3\nTRUE\n8\n\n\n\n\n\n\n\nWe would typically create such a data set in software such as Excel, and then import the data into R. It is informative, however, to create such a data set in R as an exercise. In the paragraphs that follow we do that by first creating five vectors (each with four elements) and then concatenating (combining) the vectors into a data frame.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors and data frames</span>"
    ]
  },
  {
    "objectID": "Vectors_dataframes.html#creating-a-data-frame-in-r",
    "href": "Vectors_dataframes.html#creating-a-data-frame-in-r",
    "title": "3  Vectors and data frames",
    "section": "3.3 Creating a data frame in R",
    "text": "3.3 Creating a data frame in R",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors and data frames</span>"
    ]
  },
  {
    "objectID": "Vectors_dataframes.html#character-vectors",
    "href": "Vectors_dataframes.html#character-vectors",
    "title": "3  Vectors and data frames",
    "section": "3.4 Character vectors",
    "text": "3.4 Character vectors\nFirst, we create character vectors for name and sex. Note that each element in a character vector is enclosed in quotation marks. We type the elements of the vector inside round brackets and we separate the elements with commas. The letter “c” directly in front of the square brackets instructs R to concatenate or combine the different elements into a single vector.\n\nc(\"Peter\", \"Paul\", \"Mary\", \"Janette\") \n\n[1] \"Peter\"   \"Paul\"    \"Mary\"    \"Janette\"\n\nc(\"male\", \"male\", \"female\", \"female\")\n\n[1] \"male\"   \"male\"   \"female\" \"female\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors and data frames</span>"
    ]
  },
  {
    "objectID": "Vectors_dataframes.html#logical-vectors",
    "href": "Vectors_dataframes.html#logical-vectors",
    "title": "3  Vectors and data frames",
    "section": "3.5 Logical vectors",
    "text": "3.5 Logical vectors\nSecond, we create a logical vector for the graduate variable. Note that we do not enclose TRUE and FALSE with quotation marks.\n\nc(TRUE, FALSE, TRUE, TRUE)\n\n[1]  TRUE FALSE  TRUE  TRUE",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors and data frames</span>"
    ]
  },
  {
    "objectID": "Vectors_dataframes.html#numeric-vectors",
    "href": "Vectors_dataframes.html#numeric-vectors",
    "title": "3  Vectors and data frames",
    "section": "3.6 Numeric vectors",
    "text": "3.6 Numeric vectors\nThird, we create numeric vectors for the division and experience variables. Note that the numbers in our two numeric vectors (experience and division) serve different purposes. The numbers in the experience vector represents differences in quantity (i.e. differences in years experience). By contrast, however, the numbers in the division column indicates differences in type or quality. For instance, in the division vector the numbers 1, 2 and 3 are used to indicate the divisions (finance, HR or marketing) in which Peter, Paul, Mary and Janette work. The numbers indicate differences in type, but not differences in magnitude. Following Stevens (19??), we could say that the division vector is “measured” on a nominal level scale, whereas experience is “measured” on an interval level scale.\n\n## Experience\nc(1, 3, 6, 8)\n\n[1] 1 3 6 8\n\n## Division\nc(1, 2, 2, 3)\n\n[1] 1 2 2 3",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors and data frames</span>"
    ]
  },
  {
    "objectID": "Vectors_dataframes.html#storing-vectors-as-objects",
    "href": "Vectors_dataframes.html#storing-vectors-as-objects",
    "title": "3  Vectors and data frames",
    "section": "3.7 Storing vectors as objects",
    "text": "3.7 Storing vectors as objects\nWe can store a vector for further use by assigning it to an “object” that we give any name of our choosing. We can then use that name to access the vector. Below we assign the character vector with the names of the four participants to an object that we call “name”, the character vector containing information about the sex of the participants to an object we call “sex”, the numeric vector containing information about years experience to an object we call “experience”, the numeric vector containing information about the division in which a person works as an object we call “division”, and the logical vector containing information about graduate status as an object we call “graduate”.\nWe can access the contents of any object by typing its name and then running it.\n\nname &lt;- c(\"Peter\", \"Paul\", \"Mary\", \"Janette\") \n\nsex  &lt;- c(\"male\", \"male\", \"female\", \"female\") \n\ngraduate &lt;- c(TRUE, FALSE, TRUE, TRUE) \n\nexperience &lt;- c(1, 3, 6, 8)\n\ndivision &lt;- c(1, 2, 2, 3)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors and data frames</span>"
    ]
  },
  {
    "objectID": "Vectors_dataframes.html#combining-vectors-into-a-data-frame",
    "href": "Vectors_dataframes.html#combining-vectors-into-a-data-frame",
    "title": "3  Vectors and data frames",
    "section": "3.8 Combining vectors into a data frame",
    "text": "3.8 Combining vectors into a data frame\nWe can combine vectors into a data frame, on condition that the vectors are of the same length. The order in which we type the names of the vectors determines their positions in the data frame. Each of the vectors becomes a column in the data frame.\nBelow we combine our five vectors into a data frame that we call df (note that we could choose any name we want).\n\ndf &lt;- data.frame(name, sex, division, graduate, experience)\n\ndf\n\n     name    sex division graduate experience\n1   Peter   male        1     TRUE          1\n2    Paul   male        2    FALSE          3\n3    Mary female        2     TRUE          6\n4 Janette female        3     TRUE          8\n\nstr(df)\n\n'data.frame':   4 obs. of  5 variables:\n $ name      : chr  \"Peter\" \"Paul\" \"Mary\" \"Janette\"\n $ sex       : chr  \"male\" \"male\" \"female\" \"female\"\n $ division  : num  1 2 2 3\n $ graduate  : logi  TRUE FALSE TRUE TRUE\n $ experience: num  1 3 6 8",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors and data frames</span>"
    ]
  },
  {
    "objectID": "Vectors_dataframes.html#changing-a-character-or-numeric-vector-to-a-factor",
    "href": "Vectors_dataframes.html#changing-a-character-or-numeric-vector-to-a-factor",
    "title": "3  Vectors and data frames",
    "section": "3.9 Changing a character or numeric vector to a factor",
    "text": "3.9 Changing a character or numeric vector to a factor\nThe sex variable is stored as a character vector, whereas the division variable is stored as a numeric vector. Both of these variables indicate membership to different levels of a categorical variable (i.e. both are nominal level variables). For instance, the words “male” and “female” are different levels of the categorical variable “sex”. Similarly, the numbers 1, 2 and 3 are used to indicate different levels, i.e. “finance”, “HR”, and “marketing” of the categorical variable “division”. We need to instruct R to treat the sex and division variables as categorical variables. In R, a categorical variables is referred to as a factor.\nIn the example below we use the factor() function to instruct R to treat the sex and division variables as factors. Note how we use the $ symbol to gain access to a vector in a data frame.\n\nlibrary(tidyverse)\ndf$sex &lt;- factor(df$sex)\n\n\ndf$division &lt;- factor(df$division, \n                      levels = c(1, 2, 3),\n                      labels = c(\"Finance\", \n                                 \"HR\",\n                                 \"Marketing\"))\n\nstr(df)\n\n'data.frame':   4 obs. of  5 variables:\n $ name      : chr  \"Peter\" \"Paul\" \"Mary\" \"Janette\"\n $ sex       : Factor w/ 2 levels \"female\",\"male\": 2 2 1 1\n $ division  : Factor w/ 3 levels \"Finance\",\"HR\",..: 1 2 2 3\n $ graduate  : logi  TRUE FALSE TRUE TRUE\n $ experience: num  1 3 6 8\n\n\nWe use the str() function to inspect the structure of the data frame. The division variable now is a factor and no longer a numeric vector. We can now use the gt() function of the gt package to print a nice table of our data frame.\n\nlibrary(dplyr)\nlibrary(gt)\ndf %&gt;% \n  gt()\n\n\n\n\n\n\n\nname\nsex\ndivision\ngraduate\nexperience\n\n\n\n\nPeter\nmale\nFinance\nTRUE\n1\n\n\nPaul\nmale\nHR\nFALSE\n3\n\n\nMary\nfemale\nHR\nTRUE\n6\n\n\nJanette\nfemale\nMarketing\nTRUE\n8",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors and data frames</span>"
    ]
  },
  {
    "objectID": "Vectors_dataframes.html#exercise",
    "href": "Vectors_dataframes.html#exercise",
    "title": "3  Vectors and data frames",
    "section": "3.10 Exercise",
    "text": "3.10 Exercise\nConsider the data set below. Recreate this data set in R by first creating five separate vectors and then store it as a data frame with the name mydata. Be sure to instruct R to treat area, sex and rank as factors. Give appropriate labels to the levels of the sex and rank variables. Finally, use the str() function to inspect the structure of the data frame and the gt() function to make nice table.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors and data frames</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html",
    "href": "ReadingStoringData.html",
    "title": "4  Reading and storing data in R",
    "section": "",
    "text": "4.1 Setting the working directory\nThe first step in any data analysis session with R is to set the working directory. The working directory is a folder that files related to the project that you are working on. This directory contains your data and any objects and files that you create and store while working with R.\nYou should create a project folder that will serve as your working directory on your computer or on an external drive. It is strongly recommended to keep the name of this folder short and straightforward. It is also good practice to create a “data” subfolder inside the project folder. Data files should be stored in this subfolder.\nYou can set the working directory via the Rstudio menu: Session/Set Working Directory/Choose Directory. R uses the setwd() function to set the working directory. The entire command, which includes the path to the working directory will be printed in the console window. You should copy this command and then paste it into the first line of the script window. The next time that you start R and want to set the working directory, you can highlight the first row of the script and run it.\nIn the example below I first used the menus to set the working directory to “C:/Users/deondb/OneDrive - Stellenbosch University/myR/Masters2021”. Next, I copied the line of code in the console window and pasted it into the first line of the script window. By running the line of code in the script window I can set the working directory in future sessions without having to return to the menu.\nI also included the getwd() function, which is used to see the active working directory. Users sometimes change the working directory during an R session and getwd() can be used at any time to remind you of the current working directory.\nsetwd(\"C:/Users/deondb/OneDrive - Stellenbosch University/myR/Masters2021\")\ngetwd()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html#reading-a-.csv-file-and-storing-it-as-an-object",
    "href": "ReadingStoringData.html#reading-a-.csv-file-and-storing-it-as-an-object",
    "title": "4  Reading and storing data in R",
    "section": "4.2 Reading a .csv file and storing it as an object",
    "text": "4.2 Reading a .csv file and storing it as an object\nData will typically be captured in a spreadsheet programme like Excel. One of the safest ways to get data into R is to first–in the working directory folder–store the Excel file as a comma separated values file (.csv), second to read the .csv file in R, and third to store the data as an object in R.\nWe accomplish the second and third steps jointly by using the read.csv() function. In the example below we read the data in the file labeled “SA_Swiss.csv” from the “data”subfolder and then store it to an object (a data frame) with the name mydata. Note that we can give the data frame any name we want (but the name should not start with a number and it should not contain any special characters such as @, #, $, %, !, etc). The name also should not contain spaces. Two words can be used in a name if they are separated by an underscore or by a full stop (e.g. “my.data” or “my_data”). It is good practice to keep the name short and straightforward, but as informative as possible.\n\nmydata &lt;- read.csv(\"data/SA_Swiss.csv\")\n\nOn some machines Excel uses a semi-colon rather than a comma to separate values. In such cases you should use the read.csv2() function rather than the read.csv() function.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html#check-that-the-data-has-been-properly-read",
    "href": "ReadingStoringData.html#check-that-the-data-has-been-properly-read",
    "title": "4  Reading and storing data in R",
    "section": "4.3 Check that the data has been properly read",
    "text": "4.3 Check that the data has been properly read\nYou can view the contents of the data frame with the View() function, which will open a spreadsheet in the script window. Scrolling through the spreadsheet will reveal if the data were properly read.\n\nView(mydata)\n\nYou can also request that R print the first few lines of the data frame by using the head() function.\n\nhead(mydata)\n\n  GWS1 GWS2 GWS3 GWS4 GWS5 GWS6 GWS7 GWS8 GWS9 UWES1 UWES2 UWES3 UWES4 UWES5\n1    2    1    2    1    1    1    1    1    1     5     5     5     5     5\n2    3    3    2    3    3    4    4    3    3     4     4     4     4     4\n3    2    2    2    2    2    3    4    2    3     5     5     6     6     5\n4    2    2    2    3    3    3    1    3    3     4     4     4     4     3\n5    1    1    1    2    2    1    1    1    1     6     6     6     6     5\n6    2    2    2    2    2    3    3    2    1     3     3     4     4     4\n  UWES6 UWES7 UWES8 UWES9 MBI1 MBI2 MBI3 MBI4 MBI5 MBI6 MBI7 MBI8 MBI9 MBI10\n1     5     5     4     4    1    1    1    1    1    1    1    1    1     1\n2     5     5     5     5    2    2    2    1    1    3    3    1    2     2\n3     6     6     5     4    1    1    2    1    1    1    1    1    1     1\n4     4     4     4     4    1    1    1    1    1    2    1    1    1     1\n5     5     6     6     3    1    1    1    1    1    1    1    1    1     1\n6     4     4     4     4    1    1    3    2    1    2    2    2    3     2\n  MBI11 MBI12 MBI13 MBI14 MBI15 MBI16 GHQ1 GHQ2 GHQ3 GHQ4 GHQ5 GHQ6 GHQ7 GHQ8\n1     4     3     2     1     1     1    1    1    2    2    1    4    1    1\n2     3     3     3     3     3     3    1    2    1    1    2    1    2    2\n3     3     3     3     2     3     2    1    2    1    1    3    2    2    1\n4     4     4     4     4     4     4    1    2    2    2    2    1    2    2\n5     7     7     7     7     7     7    2    1    1    1    1    4    1    2\n6     4     3     3     3     4     4    2    1    1    1    2    1    2    2\n  GHQ9 GHQ10 GHQ11 GHQ12 Country Age Gender\n1    1     1     1     1       2  35      1\n2    2     2     1     1       2  35      0\n3    2     1     1     2       2  22      0\n4    1     1     1     2       2  38      1\n5    1     1     1     1       2  20      0\n6    1     1     1     1       2  25      1\n\n\nFinally, it is good practice to use the names() function to print to the console window the names of the variables in the data frame. Each row of the printed names will start with a number in square brackets and that number represents the serial position in the data frame of the first variable in that row.\n\nnames(mydata)\n\n [1] \"GWS1\"    \"GWS2\"    \"GWS3\"    \"GWS4\"    \"GWS5\"    \"GWS6\"    \"GWS7\"   \n [8] \"GWS8\"    \"GWS9\"    \"UWES1\"   \"UWES2\"   \"UWES3\"   \"UWES4\"   \"UWES5\"  \n[15] \"UWES6\"   \"UWES7\"   \"UWES8\"   \"UWES9\"   \"MBI1\"    \"MBI2\"    \"MBI3\"   \n[22] \"MBI4\"    \"MBI5\"    \"MBI6\"    \"MBI7\"    \"MBI8\"    \"MBI9\"    \"MBI10\"  \n[29] \"MBI11\"   \"MBI12\"   \"MBI13\"   \"MBI14\"   \"MBI15\"   \"MBI16\"   \"GHQ1\"   \n[36] \"GHQ2\"    \"GHQ3\"    \"GHQ4\"    \"GHQ5\"    \"GHQ6\"    \"GHQ7\"    \"GHQ8\"   \n[43] \"GHQ9\"    \"GHQ10\"   \"GHQ11\"   \"GHQ12\"   \"Country\" \"Age\"     \"Gender\" \n\ndata.frame(names(mydata))\n\n   names.mydata.\n1           GWS1\n2           GWS2\n3           GWS3\n4           GWS4\n5           GWS5\n6           GWS6\n7           GWS7\n8           GWS8\n9           GWS9\n10         UWES1\n11         UWES2\n12         UWES3\n13         UWES4\n14         UWES5\n15         UWES6\n16         UWES7\n17         UWES8\n18         UWES9\n19          MBI1\n20          MBI2\n21          MBI3\n22          MBI4\n23          MBI5\n24          MBI6\n25          MBI7\n26          MBI8\n27          MBI9\n28         MBI10\n29         MBI11\n30         MBI12\n31         MBI13\n32         MBI14\n33         MBI15\n34         MBI16\n35          GHQ1\n36          GHQ2\n37          GHQ3\n38          GHQ4\n39          GHQ5\n40          GHQ6\n41          GHQ7\n42          GHQ8\n43          GHQ9\n44         GHQ10\n45         GHQ11\n46         GHQ12\n47       Country\n48           Age\n49        Gender",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html#missing-data",
    "href": "ReadingStoringData.html#missing-data",
    "title": "4  Reading and storing data in R",
    "section": "4.4 Missing data",
    "text": "4.4 Missing data\nIt is relatively common to find missing data in a data set. In Excel, missing data can be represented by an empty cell, by the letters NA, or by a special code such as -999. In the first two cases R will recognise the empty cells or cells containing NAs as missing data. In the third case it is necessary to indicate to read.csv() that the special code represents missing data. Note that in an R data frame missing data will always be represented by NA (an abbreviation for “not available”).\n\nSA_Swiss &lt;- read.csv(\"data/SA_Swiss.csv\", na.strings=\"-999\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html#reading-an-excel-file",
    "href": "ReadingStoringData.html#reading-an-excel-file",
    "title": "4  Reading and storing data in R",
    "section": "4.5 Reading an Excel file",
    "text": "4.5 Reading an Excel file\nIt is possible to directly read an Excel file without first storing it in csv format. This requires that the readxl package be installed and activated (the package only needs to be installed once). Note how a special code such as -999 can be used to indicate missing data. Note that empty cells or NA can also be used to indicate missing data in the Excel file, in which case the read_excel() function will automatically recognise the missing data.\n\n#install.packages(\"readxl\"  , repos = \"https://cloud.r-project.org/\")\nlibrary(readxl)\nmydata &lt;- read_excel(\"data/SA_Swiss.xlsx\", na = \"-999\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html#reading-an-spss-file",
    "href": "ReadingStoringData.html#reading-an-spss-file",
    "title": "4  Reading and storing data in R",
    "section": "4.6 Reading an SPSS file",
    "text": "4.6 Reading an SPSS file\nIt is possible to read an SPSS data file without first storing it in csv format. This requires that the haven package be installed and activated (the package only needs to be installed once). The read_spss() function will automatically recognise missing data in an SPSS file, on condition that special codes have been assigned missing data status in the SPSS file.\n\n#install.packages(\"haven\"  , repos = \"https://cloud.r-project.org/\")\nlibrary(haven)\nmydata &lt;- read_spss(\"data/SA_Swiss.sav\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html#reading-data-from-the-clipboard",
    "href": "ReadingStoringData.html#reading-data-from-the-clipboard",
    "title": "4  Reading and storing data in R",
    "section": "4.7 Reading data from the clipboard",
    "text": "4.7 Reading data from the clipboard\nAt times it might be convenient to highlight and copy data in Excel and then use the read.clipboard() function from the psychTools package to read and store the copied data as a data frame. Whereas this is easy and quick to do it is not recommended for general practice, because the saved code will not explicitly state where the data were obtained from (other than the clipboard). In the example code below I copied some data in Excel and then used the read.clipboard() function to store it as a data frame called “tempdata”. To execute the code users will have to first copy data in Excel, then remove the hash tags at the beginning of the lines, and then run the code.\n\nlibrary(psychTools)\ntempdata &lt;- read.clipboard()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html#exercise",
    "href": "ReadingStoringData.html#exercise",
    "title": "4  Reading and storing data in R",
    "section": "4.8 Exercise",
    "text": "4.8 Exercise\n\nCreate a folder on your machine that will be the working directory of your project\nCreate a data subfolder in the working directory\nStore the SA_Swiss.csv, SA_Swiss.xlsx, SA_Swiss.sav and playdata.csv files in the data subfolder\nSet the working directory to the project folder\nImport the SA_Swiss.csv data set and store it as mydata_csv.\nImport the SA_Swiss.xlsx data set and store it as mydata_xl.\nImport the SA_Swiss.sav data set and store it as mydata_spss.\nOpen the playdata.csv file in Excel. Copy the data set and then use the read.clipboard() function to read and store the data frame as playdata_clip.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "DataManagement.html",
    "href": "DataManagement.html",
    "title": "5  Data management in R",
    "section": "",
    "text": "5.1 Reading the data and storing it as a data frame\nOur first step is to read the data and to store it as a data frame. This needs to be done at the start of each session. If this had been done earlier in the session we do not need to do it again.\nmydata &lt;- read.csv(\"data/SA_Swiss.csv\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data management in *R*</span>"
    ]
  },
  {
    "objectID": "DataManagement.html#inspecting-the-structure-of-a-data-frame",
    "href": "DataManagement.html#inspecting-the-structure-of-a-data-frame",
    "title": "5  Data management in R",
    "section": "5.2 Inspecting the structure of a data frame",
    "text": "5.2 Inspecting the structure of a data frame\nWhen data are imported from a csv or Excel file, R will (usually) automatically classify variables that contain only numbers as numeric, whereas variables that contain text will (usually) be classified as categorical. In R, a categorical variable is referred to as a factor. There are two types of categorical variables or factors, namely nominal and ordinal.\nWe can inspect the classification of variables in a data frame with the str() function. In the example below we see that the items of the GWSS, the UWES, the MBI, and the GHQ are classified as integers, which is a type of numeric variable. We also note that Gender and Country are classified as integer (numeric) variables. The latter classification is problematic, in the sense that Gender and Country are actually categorical (nominal) variables where the numbers 0 and 1 for Gender indicate “Man” and “Woman”, respectively, and the numbers 1 and 2 for Country indicate “South Africa” and “Switzerland”, respectively.\nChecking if categorical variables were imported as factors is an important and essential step in the data analysis process. Similarly, converting variables that were incorrectly imported as numerical to factor is an essential step.\n\nstr(mydata)\n\n'data.frame':   1377 obs. of  49 variables:\n $ GWS1   : int  2 3 2 2 1 2 1 3 2 2 ...\n $ GWS2   : int  1 3 2 2 1 2 1 3 1 2 ...\n $ GWS3   : int  2 2 2 2 1 2 1 2 1 2 ...\n $ GWS4   : int  1 3 2 3 2 2 1 2 1 2 ...\n $ GWS5   : int  1 3 2 3 2 2 1 2 1 2 ...\n $ GWS6   : int  1 4 3 3 1 3 1 1 1 2 ...\n $ GWS7   : int  1 4 4 1 1 3 2 2 1 2 ...\n $ GWS8   : int  1 3 2 3 1 2 1 2 1 1 ...\n $ GWS9   : int  1 3 3 3 1 1 2 1 1 1 ...\n $ UWES1  : int  5 4 5 4 6 3 3 5 5 5 ...\n $ UWES2  : int  5 4 5 4 6 3 4 5 5 5 ...\n $ UWES3  : int  5 4 6 4 6 4 6 5 4 6 ...\n $ UWES4  : int  5 4 6 4 6 4 6 5 3 6 ...\n $ UWES5  : int  5 4 5 3 5 4 5 6 5 5 ...\n $ UWES6  : int  5 5 6 4 5 4 5 4 5 6 ...\n $ UWES7  : int  5 5 6 4 6 4 6 7 5 6 ...\n $ UWES8  : int  4 5 5 4 6 4 5 5 4 7 ...\n $ UWES9  : int  4 5 4 4 3 4 5 5 4 3 ...\n $ MBI1   : int  1 2 1 1 1 1 1 1 3 1 ...\n $ MBI2   : int  1 2 1 1 1 1 1 1 2 2 ...\n $ MBI3   : int  1 2 2 1 1 3 1 5 2 6 ...\n $ MBI4   : int  1 1 1 1 1 2 1 2 2 1 ...\n $ MBI5   : int  1 1 1 1 1 1 1 1 2 1 ...\n $ MBI6   : int  1 3 1 2 1 2 2 2 1 1 ...\n $ MBI7   : int  1 3 1 1 1 2 2 3 1 1 ...\n $ MBI8   : int  1 1 1 1 1 2 1 2 1 2 ...\n $ MBI9   : int  1 2 1 1 1 3 1 2 1 2 ...\n $ MBI10  : int  1 2 1 1 1 2 1 2 1 2 ...\n $ MBI11  : int  4 3 3 4 7 4 2 2 2 2 ...\n $ MBI12  : int  3 3 3 4 7 3 2 1 2 1 ...\n $ MBI13  : int  2 3 3 4 7 3 2 1 1 1 ...\n $ MBI14  : int  1 3 2 4 7 3 3 1 1 1 ...\n $ MBI15  : int  1 3 3 4 7 4 3 2 2 2 ...\n $ MBI16  : int  1 3 2 4 7 4 2 2 1 1 ...\n $ GHQ1   : int  1 1 1 1 2 2 2 2 2 2 ...\n $ GHQ2   : int  1 2 2 2 1 1 1 1 1 1 ...\n $ GHQ3   : int  2 1 1 2 1 1 1 2 2 2 ...\n $ GHQ4   : int  2 1 1 2 1 1 2 2 2 2 ...\n $ GHQ5   : int  1 2 3 2 1 2 1 1 1 2 ...\n $ GHQ6   : int  4 1 2 1 4 1 1 1 1 1 ...\n $ GHQ7   : int  1 2 2 2 1 2 2 2 2 3 ...\n $ GHQ8   : int  1 2 1 2 2 2 2 2 2 2 ...\n $ GHQ9   : int  1 2 2 1 1 1 1 1 2 1 ...\n $ GHQ10  : int  1 2 1 1 1 1 1 1 2 1 ...\n $ GHQ11  : int  1 1 1 1 1 1 1 1 1 1 ...\n $ GHQ12  : int  1 1 2 2 1 1 2 2 2 1 ...\n $ Country: int  2 2 2 2 2 2 2 2 2 2 ...\n $ Age    : num  35 35 22 38 20 25 58 26 28 34 ...\n $ Gender : int  1 0 0 1 0 1 1 0 1 0 ...\n\n\nTo ensure proper statistical analyses, we need to convert Gender and Country from numeric variables to factors. The code in the example below instructs R that the Gender variable in the mydata data frame should be treated as a factor, and that the newly converted Gender variable should be stored with the same name as the existing one (i.e. the new variable overwrites the old variable). The second line of code is used to check whether the newly overwritten Gender variable (mydata$Gender) actually is a factor.\n\nmydata$Gender &lt;- factor(mydata$Gender)\nis.factor(mydata$Gender)\n\n[1] TRUE\n\n\nIt is good practice, but not necessary, to assign labels to the numbers of a categorical variable or factor. In the present example we could assign the label “Man” to the value of 0, and the label “Woman” to the value of 1. To assign the labels we (a) instruct R what the levels of the categorical variable are, and (b) what the labels of each level are.\n\nmydata$Gender &lt;- factor(mydata$Gender, levels = c(\"0\", \"1\"), labels = c(\"Man\", \"Woman\"))\nis.factor(mydata$Gender)\n\n[1] TRUE\n\n\nWe now do the same with the Country variable (mydata$Country).\n\nmydata$Country &lt;- factor(mydata$Country, levels = c(\"1\", \"2\"), labels = c(\"ZAR\", \"CHE\"))\nis.factor(mydata$Country)\n\n[1] TRUE\n\n\nWe can now ask for a summary of basic descriptive statistics of all the variables in the data frame by using the summary() function. This function will return for each numeric variable the minimum and maximum values, the first and third quartiles, the median, and the mean. For each categorical variable or factor the frequencies of the different levels of the variable are returned.\n\nsummary(mydata)\n\n      GWS1            GWS2            GWS3            GWS4      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  \n Median :3.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :2.601   Mean   :2.145   Mean   :2.226   Mean   :2.243  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000  \n Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :5.000  \n      GWS5            GWS6            GWS7            GWS8      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:2.000   1st Qu.:1.000  \n Median :2.000   Median :2.000   Median :3.000   Median :2.000  \n Mean   :2.041   Mean   :2.078   Mean   :2.667   Mean   :1.969  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000  \n Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :5.000  \n      GWS9           UWES1           UWES2           UWES3          UWES4     \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.00   Min.   :1.00  \n 1st Qu.:1.000   1st Qu.:4.000   1st Qu.:4.000   1st Qu.:4.00   1st Qu.:4.00  \n Median :2.000   Median :5.000   Median :5.000   Median :5.00   Median :5.00  \n Mean   :1.993   Mean   :4.698   Mean   :4.892   Mean   :5.24   Mean   :4.99  \n 3rd Qu.:3.000   3rd Qu.:6.000   3rd Qu.:6.000   3rd Qu.:6.00   3rd Qu.:6.00  \n Max.   :5.000   Max.   :7.000   Max.   :7.000   Max.   :7.00   Max.   :7.00  \n     UWES5           UWES6           UWES7           UWES8      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:4.000   1st Qu.:5.000   1st Qu.:5.000   1st Qu.:4.000  \n Median :5.000   Median :6.000   Median :6.000   Median :5.000  \n Mean   :4.808   Mean   :5.373   Mean   :5.586   Mean   :4.914  \n 3rd Qu.:6.000   3rd Qu.:6.000   3rd Qu.:7.000   3rd Qu.:6.000  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000  \n     UWES9            MBI1            MBI2            MBI3      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:4.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:2.000  \n Median :5.000   Median :2.000   Median :2.000   Median :4.000  \n Mean   :4.754   Mean   :2.549   Mean   :2.741   Mean   :3.774  \n 3rd Qu.:6.000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:5.000  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000  \n      MBI4            MBI5            MBI6            MBI7      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:3.000  \n Median :3.000   Median :2.000   Median :3.000   Median :4.000  \n Mean   :2.873   Mean   :2.603   Mean   :3.001   Mean   :3.741  \n 3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:5.000  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000  \n      MBI8            MBI9           MBI10           MBI11      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000  \n Median :3.000   Median :3.000   Median :3.000   Median :3.000  \n Mean   :3.165   Mean   :2.895   Mean   :2.914   Mean   :2.675  \n 3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:3.000  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000  \n     MBI12           MBI13           MBI14           MBI15      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:1.000   1st Qu.:2.000   1st Qu.:2.000  \n Median :3.000   Median :2.000   Median :3.000   Median :3.000  \n Mean   :2.699   Mean   :2.261   Mean   :3.103   Mean   :2.788  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000  \n     MBI16            GHQ1            GHQ2            GHQ3      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:1.000   1st Qu.:2.000  \n Median :2.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :2.466   Mean   :2.104   Mean   :1.987   Mean   :1.977  \n 3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :7.000   Max.   :4.000   Max.   :4.000   Max.   :4.000  \n      GHQ4            GHQ5            GHQ6            GHQ7      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:1.000   1st Qu.:2.000  \n Median :2.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :1.954   Mean   :2.192   Mean   :1.938   Mean   :2.133  \n 3rd Qu.:2.000   3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :4.000  \n      GHQ8            GHQ9           GHQ10           GHQ11      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  \n Median :2.000   Median :2.000   Median :1.000   Median :1.000  \n Mean   :2.004   Mean   :1.933   Mean   :1.676   Mean   :1.441  \n 3rd Qu.:2.000   3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :4.000  \n     GHQ12       Country        Age          Gender   \n Min.   :1.000   ZAR:584   Min.   :16.00   Man  :607  \n 1st Qu.:2.000   CHE:793   1st Qu.:26.00   Woman:770  \n Median :2.000             Median :33.00              \n Mean   :1.992             Mean   :36.44              \n 3rd Qu.:2.000             3rd Qu.:47.00              \n Max.   :4.000             Max.   :65.00",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data management in *R*</span>"
    ]
  },
  {
    "objectID": "Subsetting.html",
    "href": "Subsetting.html",
    "title": "6  Subsetting a data frame using the dplyr package",
    "section": "",
    "text": "6.1 The %&gt;% operator of the dplyr package\nThe dplyr package makes it easy to select cases and variables by making use of the %&gt;% operator (the pipe operator). The meaning of %&gt;% is “and then”. One starts by typing the name of the data frame followed by %&gt;% and then the criteria on the basis of which persons and/or variables are selected. For instance, to select the variables in columns 1 to 5 of the mybfi data frame we would type mybfi %&gt;% select(1:5), which reads as start with the mybfi data frame AND THEN select the variables in columns 1 to 5. A keyboard shortcut for typing the pipe operator is Shift-Ctrl-M.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#filtering-cases-with-the-dplyr-package",
    "href": "Subsetting.html#filtering-cases-with-the-dplyr-package",
    "title": "6  Subsetting a data frame using the dplyr package",
    "section": "6.2 Filtering cases with the dplyr package",
    "text": "6.2 Filtering cases with the dplyr package\n\n6.2.1 Filtering rows by consecutive row numbers\nThe slice() function of the dplyr package can be used to select a specified set of cases that fall in consecutive rows from a data frame. In the example below we select the cases that are in rows 1 to 10 of the mybfi data frame.\n\nmybfi %&gt;% slice(1:10)\n\n   A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1   1  6  6  4  5  2  2  5  4  1      2         5  31\n2   2  4  4  3  4  3  4  4  3  4      2         4  22\n3   1  6  5  6  5  1  2  2  4  4      2         3  40\n4   1  5  6  6  4  1  2  4  3  2      2         3  36\n5   2  5  6  6  6  2  2  4  4  2      2         5  30\n6   1  5  6  6  5  1  1  2  1  1      2         1  53\n7   2  5  6  5  6  2  4  2  3  2      2         3  31\n8   2  6  4  6  5  4  5  4  2  5      2         3  18\n9   1  6  4  6  5  2  2  1  6  4      2         3  43\n10  3  4  5  6  3  5  5  6  5  4      2         3  21\n\n\n\n\n6.2.2 Filtering cases by non-consecutive row numbers\nTo select cases that are in non-consecutive rows we need to type their row numbers. In the example below we select the cases that are in rows 1, 3, 5, and 7 to 10 of the mybfi data frame.\n\nmybfi %&gt;% slice(1, 3, 5, 7:10)\n\n  A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1  1  6  6  4  5  2  2  5  4  1      2         5  31\n2  1  6  5  6  5  1  2  2  4  4      2         3  40\n3  2  5  6  6  6  2  2  4  4  2      2         5  30\n4  2  5  6  5  6  2  4  2  3  2      2         3  31\n5  2  6  4  6  5  4  5  4  2  5      2         3  18\n6  1  6  4  6  5  2  2  1  6  4      2         3  43\n7  3  4  5  6  3  5  5  6  5  4      2         3  21\n\n\n\n\n6.2.3 Filtering cases that meet a certain condition\nThe filter() function of the dplyr package can be used to filter cases that meet a specific condition. In the examples below we (a) select the men, (b) select people with education levels greater than 3, (c) select cases whose age is equal to 30, (d) select cases whose age is smaller than or equal to 30, and (e) select cases whose age is greater than or equal to 31.\n\nmybfi %&gt;% filter(gender == 1)\n\nmybfi %&gt;% filter(education &gt; 3)\n\nmybfi %&gt;% filter(age == 30)\n\nmybfi %&gt;% filter(age &lt;= 30)\n\nmybfi %&gt;% filter(age &gt;= 31)\n\n\n\n6.2.4 Filtering cases that don’t meet a certain condition\nThe ! symbol can be used to select cases that do not meet a certain condition. In the examples below we (a) select people who are not men, (b) select people who do not have education levels greater than 3, and (c) select people whose age is not greater than 30.\n\nmybfi %&gt;% filter(!gender == 1)\n\nmybfi %&gt;% filter(!education &gt; 3)\n\nmybfi %&gt;% filter(!age &gt; 30)\n\n\n\n6.2.5 Filtering cases on the basis of two or more conditions\nWe can also select cases on the basis of two or more conditions. The & (“and”) and | (“or”) operators are used to specify the conditions. In the examples below we (a) select cases who are men AND are older than 60, and (b) select cases who are older than 65 OR whose education level is equal to one. The second example will select ALL the people who are older than 65 and ALL the people whose education level is equal to one.\n\nmybfi %&gt;% filter(gender == 1 & age &gt; 60)\n\n  A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1  4  3  1  1  5  4  2  1  2  1      1         5  74\n2  3  2  2  6  5  1  1  1  1  1      1         5  63\n3  1  5  6  5  6  2  2  2  2  2      1         5  68\n\nmybfi %&gt;% \n  filter(age &gt; 65 | education == 1)\n\n   A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1   1  5  6  6  5  1  1  2  1  1      2         1  53\n2   4  5  4  5  4  3  2  2  4  5      1         1  49\n3   3  5  4  4  4  2  3  2  3  2      1         1  53\n4   3  5  5  6  6  4  3  2  3  1      1         1  39\n5   3  6  6  5  6  2  5  4  1  2      1         1  40\n6   1  1  1  1  5  1  2  6  4  6      2         1  18\n7   3  2  5  5  3  2  2  2  1  3      2         1  40\n8   2  5  3  5  5  2  2  1  1  5      1         1  51\n9   3  4  6  6  6  5  5  4  4  6      1         1  25\n10  5  4  4  4  3  2  2  1  3  1      1         1  24\n11  1  6  5  6  4  5  5  5  1  4      2         1  23\n12  3  3  3  5  5  3  5  2  4  4      1         1  18\n13  4  6  1  6  6  4  3  6  5  6      2         1  20\n14  1  4  3  6  3  5  6  4  5  5      2         1  16\n15  2  5  5  2  4  5  4  6  2  4      2         1  17\n16  3  4  5  5  3  3  2  3  4  2      2         1  20\n17  3  3  4  2  3  3  3  2  3  3      1         1  19\n18  2  3  4  2  5  1  5  1  1  1      2         1  18\n19  1  6  3  4  2  4  1  3  2  5      1         1  16\n20  2  3  4  3  3  2  4  2  5  3      2         1  19\n21  2  5  5  6  5  1  2  2  2  2      2         1  15\n22  2  4  6  5  6  2  5  4  3  4      1         1  26\n23  6  4  4  6  6  3  6  5  4  4      2         1  23\n24  3  5  4  5  5  1  1  1  1  1      2         1  21\n25  3  5  4  5  5  3  4  2  2  1      2         1  22\n26  2  4  5  4  2  5  5  6  4  4      2         1  19\n27  3  4  4  5  4  3  3  4  2  4      2         1  24\n28  1  3  4  4  4  1  1  1  1  4      2         2  86\n29  4  3  1  1  5  4  2  1  2  1      1         5  74\n30  5  6  5  5  6  1  4  1  3  2      2         1  18\n31  3  4  5  6  5  1  2  1  2  1      1         1  20\n32  5  5  5  6  6  4  1  2  2  5      2         1  44\n33  4  4  6  6  5  1  2  3  6  2      1         1  23\n34  2  4  4  6  4  3  3  2  3  3      2         1  32\n35  1  5  3  2  3  5  5  5  5  3      2         1  43\n36  2  5  5  5  6  2  2  2  2  2      1         1  19\n37  1  6  5  2  5  1  1  1  1  1      2         1  18\n38  1  6  6  5  5  3  3  3  3  4      2         1  34\n39  3  5  5  6  5  1  2  1  1  1      2         1  24\n40  1  6  6  6  5  4  2  2  3  4      1         1  34\n41  6  6  5  5  4  1  4  1  1  1      2         1  21\n42  2  4  4  3  5  5  5  6  4  6      2         1  21\n43  2  6  5  4  6  2  5  2  1  2      2         1  16\n44  1  4  5  6  4  4  5  3  6  4      1         1  18\n45  4  1  1  1  3  6  6  5  5  5      2         1  20\n46  4  5  2  2  1  2  4  2  2  3      1         1  17\n47  6  6  6  6  6  6  6  6  6  6      1         1  18\n48  5  4  2  4  5  2  5  5  5  5      1         1  38\n49  3  3  5  5  5  3  5  3  3  4      2         1  18\n50  4  5  5  6  5  2  3  3  1  1      1         1  18\n51  3  5  2  1  2  4  6  3  2  3      2         1  17\n52  4  4  4  3  4  2  2  3  3  3      1         1  19\n53  3  5  2  2  5  2  2  5  2  2      1         1  21\n54  1  5  6  5  6  2  2  2  2  2      1         5  68\n55  1  6  5  6  3  6  6  6  2  6      2         1  29\n56  3  4  4  6  3  3  5  4  3  3      2         1  42\n57  2  3  4  6  5  3  2  3  2  1      2         1  54\n58  1  6  5  5  5  1  2  1  4  4      2         1  58\n59  4  2  1  1  2  6  6  4  6  6      2         1  20\n60  1  5  5  6  5  3  3  2  1  2      2         1  22\n61  1  5  6  6  6  5  1  5  5  2      1         1  31\n62  4  5  2  1  4  4  5  2  4  6      2         1  18\n63  1  5  6  4  5  1  3  3  2  2      1         1  22\n64  2  5  5  2  5  2  4  2  4  3      2         1  46\n65  6  6  6  6  6  1  6  1  6  1      1         1  31\n66  4  4  4  4  4  4  4  4  4  4      2         1  20\n67  5  2  1  1  1  1  1  1  1  1      1         1  18\n68  2  5  2  5  5  1  4  2  5  2      1         1  35\n69  3  4  3  6  3  3  3  3  4  4      1         1  46\n70  3  6  4  6  4  1  1  1  1  1      1         1  21\n71  5  4  6  2  4  5  6  5  4  5      1         1  19\n72  2  6  5  4  6  4  5  5  6  6      2         1  20\n73  1  5  6  5  2  1  2  5  2  1      2         1  27\n74  2  5  5  5  5  1  5  4  4  2      1         1  35\n75  5  6  6  1  6  2  3  6  6  4      1         1  16\n76  1  6  6  6  6  2  2  5  2  1      1         1  17\n77  2  4  6  4  5  2  4  1  1  1      2         1  31\n78  2  5  6  3  5  5  4  1  2  2      1         1  18\n79  2  5  5  5  5  3  2  3  2  2      1         1  19\n80  2  6  6  6  5  1  1  1  2  1      2         5  66\n81  1  5  6  4  5  1  2  2  2  4      2         1  18\n82  5  6  5  6  4  1  1  2  2  1      1         1  18\n\n\n\n\n6.2.6 Storing the filtered cases in a new data frame\nIn the example below we start with the mybfi data frame, AND THEN we select the cases who are older than 28 AND have an education level of three or higher, AND THEN we select the first 15 rows of the cases who met the condition. Finally, we assign these cases to a new data frame that is labeled df_new. This data frame will contain 15 cases that each are older than 28 and have an education level of three or higher.\n\ndf_new &lt;- mybfi %&gt;% \n  filter(age &gt; 28 & education &gt;= 3) %&gt;% \n  slice(1:15)\n\ndf_new\n\n   A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1   1  6  6  4  5  2  2  5  4  1      2         5  31\n2   1  6  5  6  5  1  2  2  4  4      2         3  40\n3   1  5  6  6  4  1  2  4  3  2      2         3  36\n4   2  5  6  6  6  2  2  4  4  2      2         5  30\n5   2  5  6  5  6  2  4  2  3  2      2         3  31\n6   1  6  4  6  5  2  2  1  6  4      2         3  43\n7   1  5  5  6  5  4  4  4  3  2      2         3  30\n8   2  3  2  1  4  1  5  1  2  1      1         5  48\n9   1  6  6  6  6  1  1  3  2  1      1         3  39\n10  1  5  5  6  6  4  5  3  2  5      2         5  34\n11  1  5  5  5  5  5  6  4  5  6      1         4  52\n12  3  4  5  6  4  4  4  5  5  3      2         5  31\n13  5  5  5  6  2  5  4  5  2  4      2         3  46\n14  1  6  6  6  6  1  1  1  2  2      1         3  30\n15  4  4  4  3  3  4  4  5  3  2      1         4  32",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#selecting-variables-with-the-dplyr-package",
    "href": "Subsetting.html#selecting-variables-with-the-dplyr-package",
    "title": "6  Subsetting a data frame using the dplyr package",
    "section": "6.3 Selecting variables with the dplyr package",
    "text": "6.3 Selecting variables with the dplyr package\nThe select() function of the dplyr package makes it easy to select variables with either the column numbers of the variables or their names. In the examples below we start with the mybfi data frame AND THEN: (a) select the variables A1 to A5 (i.e. A1, A2, A3, A4 and A5), (b) select the variables A1, A3, N1 and N3, (c) select any variable that starts with “A” (i.e. A1, A2, A3, A4 and A5), (d) select any variable that does not start with “A”, (e) select any variable that ends with “r” (i.e. gender), (f) select the variables in columns 6 to 10 (i.e. N1, N2, N3, N4 and N5) (f) select the variables in columns 1, 3, 6, and 8 (i.e. A1, A3, N1, and N3).\n\nlibrary(dplyr)\n\n\nmybfi %&gt;% select(A1:A5)\nmybfi %&gt;% select(A1, A3, N1, N3)\nmybfi %&gt;% select(starts_with(\"A\"))\nmybfi %&gt;% select(!starts_with(\"A\"))\nmybfi %&gt;% select(ends_with(\"r\"))\nmybfi %&gt;% select(6:10)\nmybfi %&gt;% select(1, 3, 6, 8)\n\n\n6.3.1 Combining select and slice\nIn the example below we first select the variables A1 to A5 AND THEN only only the cases that are in rows 1, 3 and 5.\n\nmybfi %&gt;%\n  select(A1:5) %&gt;% \n  slice(1,3,5)\n\n\n\n6.3.2 Combining select and filter\nIn the example below we first select all the women AND THEN only the first 10 rows AND THEN we select variables A1 to A5.\n\nmybfi %&gt;%\n  filter(gender == 2) %&gt;% \n  slice(1:10) %&gt;% \n  select(A1:A5)\n\n\n\n6.3.3 Randomly select rows from a data frame without replacement\nWe can use the sample_n() function of the dplyr package to draw a random sample of cases (without replacement) from a data frame. In the example below we randomly sample (without replacement) ten cases from the mybfi data frame.\n\nmybfi %&gt;% \n  sample_n(10)\n\n   A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1   3  5  2  4  2  3  3  2  3  1      1         5  24\n2   4  3  5  6  4  1  4  3  4  2      1         4  21\n3   4  4  5  6  5  1  2  1  5  4      1         4  43\n4   1  6  6  6  6  3  4  2  3  3      2         3  42\n5   1  4  4  4  5  5  4  6  4  4      2         3  22\n6   2  3  5  6  5  2  1  4  1  1      1         4  25\n7   1  6  5  4  5  1  1  2  2  3      2         4  21\n8   2  5  5  6  5  4  4  4  3  3      2         3  20\n9   1  6  6  6  6  1  1  1  1  1      1         5  53\n10  2  5  6  6  5  1  3  3  3  5      2         3  20\n\n\n\n\n6.3.4 Randomly select rows from a data frame with replacement\nTo randomly select cases with replacement we need to add the argument replace = TRUE.\n\nmybfi %&gt;% \n  sample_n(10, replace = TRUE)\n\n   A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1   1  6  6  6  6  2  6  5  6  5      1         3  28\n2   2  4  5  6  5  2  5  6  4  2      2         5  25\n3   3  4  5  4  5  4  3  2  4  2      1         3  19\n4   2  4  2  1  3  1  4  2  1  1      2         3  25\n5   4  4  5  6  5  3  2  2  2  4      1         3  27\n6   2  5  4  4  4  5  5  4  4  4      2         3  18\n7   1  6  6  6  6  1  1  1  1  1      2         5  34\n8   3  6  5  6  5  1  1  2  1  4      1         4  22\n9   1  5  6  6  4  1  2  4  3  2      2         3  36\n10  2  4  4  1  2  6  6  6  6  6      2         3  19\n\n\n\n\n6.3.5 Randomly select a fraction of rows without replacement\nWe can also specify a fraction of cases to be randomly selected. In the example below we randomly select 1% of the cases in the mybfi data frame.\n\nmybfi %&gt;% \n  sample_frac(.01)\n\n   A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1   6  2  5  6  6  1  1  6  2  6      2         2  19\n2   2  5  5  2  5  1  1  4  4  1      2         4  44\n3   2  5  4  5  2  6  6  6  6  5      2         3  37\n4   2  4  3  3  3  1  2  1  3  1      1         5  25\n5   5  5  5  6  6  1  1  1  1  2      2         2  38\n6   2  2  6  2  2  5  5  5  5  1      1         2  19\n7   1  6  6  6  6  6  6  6  6  6      2         3  25\n8   2  4  4  6  5  2  5  2  2  1      1         3  60\n9   6  5  5  6  6  1  1  4  3  1      1         3  17\n10  4  5  4  6  4  3  4  3  1  4      2         3  19\n\n\n\n\n6.3.6 Randomly select a fraction of rows with replacement\nTo randomly select a fraction of cases with replacement we need to add the argument replace = TRUE.\n\nmybfi %&gt;% \n  sample_frac(0.01, replace = TRUE)\n\n   A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1   2  5  6  5  4  6  6  6  6  4      2         3  23\n2   2  5  6  6  5  4  4  6  4  5      2         5  19\n3   1  5  6  6  5  4  4  4  2  1      2         3  18\n4   1  6  5  6  4  2  3  1  1  1      1         3  20\n5   4  5  4  5  2  5  5  5  3  5      2         3  24\n6   3  4  3  1  3  3  4  5  3  2      1         5  25\n7   1  6  5  6  4  5  5  5  1  4      2         1  23\n8   2  5  5  6  5  1  2  2  5  5      2         5  32\n9   3  4  3  1  3  3  4  5  3  2      1         5  25\n10  1  4  3  4  5  3  2  5  2  4      2         4  23\n\n\n\n\n6.3.7 Randomly split a data frame into two parts\nThe code below can be used to randomly split a data frame into two. We need to indicate the number of people we want in each group. In the example below we randomly split the mybfi data frame into two, where the first sample (mybfi1) contains 800 cases and the second (mybfi2) 200 cases.\n\n## Specify exactly how many persons you want in the two groups, respectively.\n## Here we want n = 800 in the first group and n = 200 in the second.\nv          &lt;- as.vector(c(rep(TRUE, 800), rep(FALSE, 200))) \n\nselection  &lt;- sample(v) \n\n\nmybfi1     &lt;- mybfi %&gt;%\n  filter(selection) \n\nmybfi2     &lt;- mybfi %&gt;% \n  filter(!selection) \n  \n\nnrow(mybfi1)\n\n[1] 800\n\nnrow(mybfi2)\n\n[1] 200",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#filtering-cases-and-selecting-variables-with-square-brackets",
    "href": "Subsetting.html#filtering-cases-and-selecting-variables-with-square-brackets",
    "title": "6  Subsetting a data frame using the dplyr package",
    "section": "6.4 Filtering cases and selecting variables with square brackets",
    "text": "6.4 Filtering cases and selecting variables with square brackets\n\n6.4.1 Selecting consecutive rows by their row numbers with square brackets\nWe can select rows of a data frame by their row numbers using square brackets. Any particular cell in a data frame can be indexed by its row number and its column number, for instance the score of the person in row one on the variable in column one has the index mybfi[1, 1]. We first type the name of the data frame and then the square brackets with the row and column numbers. Note that we place a comma in the square brackets to separate the row and column numbers. The value before the comma pertains to the row number, whereas the value after the comma pertains to the column number. So, the score of the person in row five on the variable in column six of the mybfi data frame is indexed by mybfi[5, 6].\nIf we want to select a row and include all the columns we leave the space after the comma empty. So if we want to use the scores of the person in the sixth row of the data frame on all the variables we would do it as follows: mybfi[6, ].\nIn the example below we select the persons in rows 1 to 20, using all the variables.\n\nhead(mybfi[1:10, ])\n\n  A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1  1  6  6  4  5  2  2  5  4  1      2         5  31\n2  2  4  4  3  4  3  4  4  3  4      2         4  22\n3  1  6  5  6  5  1  2  2  4  4      2         3  40\n4  1  5  6  6  4  1  2  4  3  2      2         3  36\n5  2  5  6  6  6  2  2  4  4  2      2         5  30\n6  1  5  6  6  5  1  1  2  1  1      2         1  53\n\n\n\n\n6.4.2 Selecting non-consecutive rows by their row numbers with square brackets\nWe can select non-consecutive rows of a data frame by concatenating the row numbers inside round brackets, preceded by the letter “c”. For instance, to select rows 2, 4, and 7 of the mybfi data frame we type mybfi[c(2, 4, 7), ]. Note that the space after the comma indicates that we want to include all the variables.\nIn the example below we select rows 1, 3, 5, 7 and 9 from the bfi data frame.\n\nhead(mybfi[c(1, 3, 5, 7, 9), ])\n\n  A1 A2 A3 A4 A5 N1 N2 N3 N4 N5 gender education age\n1  1  6  6  4  5  2  2  5  4  1      2         5  31\n3  1  6  5  6  5  1  2  2  4  4      2         3  40\n5  2  5  6  6  6  2  2  4  4  2      2         5  30\n7  2  5  6  5  6  2  4  2  3  2      2         3  31\n9  1  6  4  6  5  2  2  1  6  4      2         3  43",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#selecting-columns-variables-from-a-data-frame",
    "href": "Subsetting.html#selecting-columns-variables-from-a-data-frame",
    "title": "6  Subsetting a data frame using the dplyr package",
    "section": "6.5 Selecting columns (variables) from a data frame",
    "text": "6.5 Selecting columns (variables) from a data frame\n\n6.5.1 Selecting a variable with $ notation\nWe can select a variable from a data frame using dollar notation by first typing the name of the data frame, second typing a $ symbol, and then typing the name of the variable. For instance, we select variable N1 of the mybfi data frame as follows: mybfi$N1\nIn this example we are selecting the variable A1 from the mybfi data frame. Next, I find the mean and standard deviation of variable A1.\n\nhead(mybfi$A1)\n\n[1] 1 2 1 1 2 1\n\nmean(mybfi$A1)\n\n[1] 2.317\n\nsd(mybfi$A1)\n\n[1] 1.366887\n\n\n\n\n6.5.2 Selecting a variable by its column number with square brackets\nWe can select a variable by its column number in the data frame using square brackets. Any particular cell in a data frame can be indexed by its row number and its column number, for instance the score of the person in row one on the variable in column one has the index mybfi[1, 1]. We first type the name of the data frame and then the square brackets with the row and column numbers. Note that we place a comma in the square brackets to separate the row and column numbers. The value before the comma pertains to the row number, whereas the value after the comma pertains to the column number. So, the score of the person in row five on the variable in column six of the mybfi data frame is indexed by mybfi[5, 6].\nIf we want to select a variable and include all the rows we leave the space before the comma empty. So if we want to use the scores of all the persons in the data frame on the variable in column six we would do it as follows: mybfi[, 6].\nIn the example below we select variable A1, which is in the first column, using all the rows.\n\nhead(mybfi[, 1]) \n\n[1] 1 2 1 1 2 1\n\nmean(mybfi[, 1])\n\n[1] 2.317\n\nsd(mybfi[, 1])\n\n[1] 1.366887\n\n\n\n\n6.5.3 Selecting a variable by its name with square brackets\nWe can also use square brackets to select a variable by its name. For instance, to select variable N1 of the mybfi data frame, we type mybfi[, “N1”. Note that we have to type the name of the variable inside quotation marks.\nIn the example below we select variable A1 from the mybfi data frame.\n\nhead(mybfi[, \"A1\"])\n\n[1] 1 2 1 1 2 1\n\nmean(mybfi[, \"A1\"])\n\n[1] 2.317\n\nsd(mybfi[, \"A1\"])\n\n[1] 1.366887\n\n\n\n\n6.5.4 Selecting two or more contiguous variables by their column numbers with square brackets\nIf the columns we want to select are contiguous–i.e. next to each other or together in sequence–we can type the first and last column numbers separated by a colon. For instance, to select the variables in columns three to six of the mybfi data frame we would type mybfi[, 3:6].\nIn the example below we select the variables in columns one to five. Next we ask for descriptive statistics of these variables.\n\nhead(mybfi[, 1:5])\n\n  A1 A2 A3 A4 A5\n1  1  6  6  4  5\n2  2  4  4  3  4\n3  1  6  5  6  5\n4  1  5  6  6  4\n5  2  5  6  6  6\n6  1  5  6  6  5\n\npsych::describe(mybfi[, 1:5])\n\n   vars    n mean   sd median trimmed  mad min max range  skew kurtosis   se\nA1    1 1000 2.32 1.37      2    2.12 1.48   1   6     5  0.97     0.09 0.04\nA2    2 1000 4.81 1.16      5    4.98 1.48   1   6     5 -1.15     1.22 0.04\nA3    3 1000 4.69 1.25      5    4.87 1.48   1   6     5 -1.08     0.75 0.04\nA4    4 1000 4.77 1.48      5    5.02 1.48   1   6     5 -1.14     0.27 0.05\nA5    5 1000 4.64 1.26      5    4.80 1.48   1   6     5 -0.90     0.23 0.04\n\ncor(mybfi[, 1:5])\n\n           A1         A2         A3         A4         A5\nA1  1.0000000 -0.3297446 -0.2407081 -0.1508566 -0.1860256\nA2 -0.3297446  1.0000000  0.4753208  0.3681640  0.3827802\nA3 -0.2407081  0.4753208  1.0000000  0.3957704  0.5281988\nA4 -0.1508566  0.3681640  0.3957704  1.0000000  0.3372695\nA5 -0.1860256  0.3827802  0.5281988  0.3372695  1.0000000\n\n\n\n\n6.5.5 Selecting two or more non-contiguous variables by their column numbers with square brackets\n\nhead(mybfi[, c(1,3,5)])\n\n  A1 A3 A5\n1  1  6  5\n2  2  4  4\n3  1  5  5\n4  1  6  4\n5  2  6  6\n6  1  6  5\n\n\n\n\n6.5.6 Selecting two or more variables by their names with square brackets\n\nhead(mybfi[, c(\"A1\", \"A3\", \"A5\")])\n\n  A1 A3 A5\n1  1  6  5\n2  2  4  4\n3  1  5  5\n4  1  6  4\n5  2  6  6\n6  1  6  5",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#excercises",
    "href": "Subsetting.html#excercises",
    "title": "6  Subsetting a data frame using the dplyr package",
    "section": "6.6 Excercises",
    "text": "6.6 Excercises\n\nUse the dplyr package with the pipe operator (%&gt;%) to randomly select 500 cases from the mybfi data frame and then find the correlations of the five agreeableness variables.\n\nHere is an example where I randomly select 300 cases AND THEN select items N1, N3, and N5, AND THEN calculate their correlations.\n\nmybfi %&gt;% \n  sample_n(300) %&gt;% \n  select(N1, N3, N5) %&gt;% \n  cor()\n\n\nRandomly split (without replacement) the mybfi data frame into two data frames where the first data frame contains 600 cases and the second 400 cases. Store the first data frame as mybfi1 and the second as mybfi2. Next, use the pipe operator to select variables A1, A3, N2 and N4, and then find the correlations of these variables. Do this for each of the two data frames. Note that the corresponding correlations are not exactly the same across the two data sets, which illustrates the role of sampling error.\n\n\nUse the dplyr package to select the rows of all the women (coded as 2 in the gender column). Then find the correlations of the variables N1 to N5. Do the same for the men.\n\n\nUse the dplyr package to select the rows of the men who (a) have education levels of 3 or above and (b) are older than 40. Then find the correlations of variables N1 to N5.\n\nThe “SA_Swiss.csv” file contains data for 1377 people with respect to the items of the General Work Stress Scale (GWS), the Utrecht Work Engagement Scale (UWES), the Maslach Burnout Inventory (MBI), and the General Health Questionnaire (GHQ), country, and gender.\nThe country variable has two levels: 1 = Swiss, and 2 = South Africa. The gender variable also has two levels: 0 = men and 1 = women. The items of the GWS are labelled GWS1 to GWS9, the UWES items are labelled UWES1 to UWES9, the MBI items are labelled MBI1 to MBI15, and the GHQ items are labelled GHQ1 to GHQ12.\n\nImport the data and store it as a data frame labelled “df”\nInspect the names of all the variables in df\nStore the country and gender variables as factors\nUse the describe function of the psych package to (a) find the column numbers of all the variables in df, and (b) to inspect descriptive statistics of each item\nUse the slice() function of the dplyr package to find descriptive statistics for the people that are in rows 50 to 100 of the df data frame (you will have to “activate” the psych package to use the describe() function)\n\nExample: Say we have a data frame labelled mydata. We can find descriptive statistics of the first 10 rows of mydata with the slice() and describe() functions as follows:\n\nlibrary(psych)\nlibrary(dplyr)  \nmydata %&gt;% \n  slice(1:10) %&gt;% \n  describe()\n\n\nUse the slice() function of the dplyr package to find descriptive statistics for the people that are in rows 50 to 60 and rows 70 to 80 of the df data frame\nUse the filter() function of the dplyr package to find descriptive statistics for all the women\nUse the filter() function of the dplyr package to find descriptive statistics for all the participants who are older than 39\nUse the filter() function of the dplyr package to find descriptive statistics for all the participants who are not from Switzerland\nUse the filter() function of the dplyr package to find descriptive statistics for all the women who are from Switzerland\nUse the filter() function of the dplyr package to find descriptive statistics for all the men who are from South Africa and are 30 years or younger\nUse the filter() function of the dplyr package to store the data of all the men in data frame labelled df_men and the data of the women in a data frame labelled df_women\nUse the select() function of the dplyr package to find descriptive statistics of the UWES items by name for the men\nUse the select() function of the dplyr package to find descriptive statistics of the GHQ items by number for the women\nUse the select() function of the dplyr package to find descriptive statistics of the GHQ items by number for the women\nUse the filter() and select() functions of the dplyr package to find descriptive statistics of all persons who are older than 50 on the GWS items\nUse the sample_n() function of the dplyr package to randomly select (without replacement) 20 cases and then find descriptive statistics for them\nUse the sample_frac() function of the dplyr package to randomly select (without replacement) 20% of the cases. Store these data in a new data frame labelled df_20 and then find descriptive statistics of this data frame\nRandomly split the df data frame into two, such that the one part contains 700 cases and the second part contains 677 cases. Store the new data frames as df_700 and df_677, respectively. Next, find descriptive statistics for each of the two data frames",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Subsetting a data frame using the dplyr package</span>"
    ]
  },
  {
    "objectID": "Estimating_correlations.html",
    "href": "Estimating_correlations.html",
    "title": "7  Estimating correlations with lavaan",
    "section": "",
    "text": "7.1 Finding the Pearson correlations of a set of variables\nSay we want to find the correlations of the five Agreeableness items, which are located in columns 1 to 5 in the bfi data frame. We can use the lavCor() function of the lavaan package to easily find the correlations as follows: lavCor(bfi[1:5]).\nIn the example below we find the Pearson correlations of the five Neuroticism items, which are located in columns 16 to 20.\nlavCor(bfi[16:20])\n\n      N1    N2    N3    N4    N5\nN1 1.000                        \nN2 0.706 1.000                  \nN3 0.556 0.545 1.000            \nN4 0.399 0.390 0.518 1.000      \nN5 0.377 0.352 0.428 0.398 1.000",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Estimating correlations with lavaan</span>"
    ]
  },
  {
    "objectID": "Estimating_correlations.html#the-statistical-signifcance-of-the-correlations",
    "href": "Estimating_correlations.html#the-statistical-signifcance-of-the-correlations",
    "title": "7  Estimating correlations with lavaan",
    "section": "7.2 The statistical signifcance of the correlations",
    "text": "7.2 The statistical signifcance of the correlations\nWe can also obtain information about (a) the number of cases whose data were included in estimating the correlations (by default cases with missing data are removed listwise), (b) the covariances and variances of the items, and (c) the statistical significance of the correlations. To do this we need to specify that we want the output in lavaan format and that we want to estimate the standard errors of the covariances. We could do this for the five Agreeableness items as follows: Agree_cors &lt;- lavCor(bfi[1:5], se = standard, output = \"lavaan\"). Note that we store the results in an object that here is named Agree_cors. Finally, we ask for a summary of Agree_cors and specify that we want to see the standardized parameters (i.e. the correlations). We could do that as follows: summary(Agree_cors, standardized = TRUE)\nIn the example below we find the correlations of the five Neuroticism items, request standard errors, specify that the output should be in lavaan format, and store the results as Neur_cors.\n\nNeur_cors &lt;- lavCor(bfi[16:20], output = \"lavaan\", se = \"standard\")\nsummary(Neur_cors, standardized = TRUE)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n\n                                                  Used       Total\n  Number of observations                          2694        2800\n\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  N1 ~~                                                                 \n    N2                1.694    0.057   29.927    0.000    1.694    0.706\n    N3                1.399    0.055   25.220    0.000    1.399    0.556\n    N4                0.986    0.051   19.217    0.000    0.986    0.399\n    N5                0.961    0.053   18.302    0.000    0.961    0.377\n  N2 ~~                                                                 \n    N3                1.332    0.054   24.854    0.000    1.332    0.545\n    N4                0.937    0.050   18.869    0.000    0.937    0.390\n    N5                0.872    0.051   17.247    0.000    0.872    0.352\n  N3 ~~                                                                 \n    N4                1.304    0.055   23.875    0.000    1.304    0.518\n    N5                1.110    0.054   20.422    0.000    1.110    0.428\n  N4 ~~                                                                 \n    N5                1.014    0.053   19.176    0.000    1.014    0.398\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    N1                2.474    0.067   36.701    0.000    2.474    1.000\n    N2                2.329    0.063   36.701    0.000    2.329    1.000\n    N3                2.560    0.070   36.701    0.000    2.560    1.000\n    N4                2.474    0.067   36.701    0.000    2.474    1.000\n    N5                2.630    0.072   36.701    0.000    2.630    1.000",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Estimating correlations with lavaan</span>"
    ]
  },
  {
    "objectID": "Estimating_correlations.html#using-full-information-maximum-likelihood-to-deal-with-missing-data",
    "href": "Estimating_correlations.html#using-full-information-maximum-likelihood-to-deal-with-missing-data",
    "title": "7  Estimating correlations with lavaan",
    "section": "7.3 Using full information maximum likelihood to deal with missing data",
    "text": "7.3 Using full information maximum likelihood to deal with missing data\nNote that the lavaan output shows that only 2694 of the 2800 cases were used to find the correlations (this is due to some missing data). We can use the data of all 2800 cases by using full information maximum likelihood (fiml) to estimate the correlations. This technique uses all the information in the data set to estimate the correlations and no cases are removed.\nWe can use the same code as before, but add the specification that the missing data should be dealt with using full information maximum likelihood.\n\nlavCor(bfi[16:20], missing = \"fiml\")\n\n      N1    N2    N3    N4    N5\nN1 1.000                        \nN2 0.707 1.000                  \nN3 0.557 0.549 1.000            \nN4 0.398 0.391 0.519 1.000      \nN5 0.378 0.351 0.428 0.398 1.000\n\n\n\nNeur_cors_fiml &lt;- lavCor(bfi[16:20], missing = \"fiml\", se = \"standard\", output = \"lavaan\")\nsummary(Neur_cors_fiml, standardized = TRUE)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        20\n\n  Number of observations                          2800\n  Number of missing patterns                        11\n\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  N1 ~~                                                                 \n    N2                1.693    0.056   30.482    0.000    1.693    0.707\n    N3                1.400    0.055   25.676    0.000    1.400    0.557\n    N4                0.980    0.050   19.466    0.000    0.980    0.398\n    N5                0.961    0.052   18.607    0.000    0.961    0.378\n  N2 ~~                                                                 \n    N3                1.342    0.053   25.424    0.000    1.342    0.549\n    N4                0.937    0.049   19.188    0.000    0.937    0.391\n    N5                0.867    0.050   17.449    0.000    0.867    0.351\n  N3 ~~                                                                 \n    N4                1.305    0.054   24.278    0.000    1.305    0.519\n    N5                1.110    0.054   20.749    0.000    1.110    0.428\n  N4 ~~                                                                 \n    N5                1.012    0.052   19.445    0.000    1.012    0.398\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    N1                2.932    0.030   98.584    0.000    2.932    1.866\n    N2                3.508    0.029  121.510    0.000    3.508    2.300\n    N3                3.217    0.030  106.148    0.000    3.217    2.008\n    N4                3.185    0.030  106.899    0.000    3.185    2.029\n    N5                2.969    0.031   96.675    0.000    2.969    1.834\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    N1                2.467    0.066   37.311    0.000    2.467    1.000\n    N2                2.326    0.062   37.328    0.000    2.326    1.000\n    N3                2.566    0.069   37.378    0.000    2.566    1.000\n    N4                2.464    0.066   37.193    0.000    2.464    1.000\n    N5                2.620    0.070   37.225    0.000    2.620    1.000",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Estimating correlations with lavaan</span>"
    ]
  },
  {
    "objectID": "Spearman.html",
    "href": "Spearman.html",
    "title": "8  Single factor model",
    "section": "",
    "text": "library(psychTools)\nlibrary(lavaan)\nlibrary(flexplavaan)\n\nmod1 &lt;- '\nNeuroticism =~ N1 + N2 + N3 + N4 + N5\n'\n\nfit.mod1 &lt;- cfa(mod1, data = bfi)\nsummary(fit.mod1, standardized = TRUE, fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 26 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n                                                  Used       Total\n  Number of observations                          2694        2800\n\nModel Test User Model:\n                                                      \n  Test statistic                               360.932\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              4724.621\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.925\n  Tucker-Lewis Index (TLI)                       0.849\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -23078.504\n  Loglikelihood unrestricted model (H1)     -22898.038\n                                                      \n  Akaike (AIC)                               46177.007\n  Bayesian (BIC)                             46235.995\n  Sample-size adjusted Bayesian (SABIC)      46204.222\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.163\n  90 Percent confidence interval - lower         0.149\n  90 Percent confidence interval - upper         0.177\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.056\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Neuroticism =~                                                        \n    N1                1.000                               1.286    0.818\n    N2                0.952    0.023   40.826    0.000    1.225    0.803\n    N3                0.892    0.024   36.883    0.000    1.147    0.717\n    N4                0.677    0.024   27.818    0.000    0.872    0.554\n    N5                0.632    0.025   24.973    0.000    0.813    0.502\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                0.819    0.036   22.656    0.000    0.819    0.331\n   .N2                0.828    0.034   24.025    0.000    0.828    0.356\n   .N3                1.245    0.042   29.526    0.000    1.245    0.486\n   .N4                1.714    0.051   33.779    0.000    1.714    0.693\n   .N5                1.968    0.057   34.494    0.000    1.968    0.748\n    Neuroticism       1.655    0.070   23.756    0.000    1.000    1.000\n\nvisualize(fit.mod1, subset = 1:5)\n\n\n\n\n\n\n\nmeasurement_plot(fit.mod1, sample = 100)\n\n\n\n\n\n\n\nresidual_plots(fit.mod1)\n\n\n\n\n\n\n\n\n\nGWS2020 &lt;- read.csv(\"C:/Users/deondb/Downloads/GWS2020.csv\")\n\nmodgws &lt;- '\nStress =~ Item1 + Item2 + Item3 + Item4 + Item5 + Item6 + Item7 + Item8 + Item9\n'\n\nfit.modgws &lt;- cfa(modgws, data = GWS2020)\nsummary(fit.modgws, standardized = TRUE, fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 22 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        18\n\n  Number of observations                          1377\n\nModel Test User Model:\n                                                      \n  Test statistic                               808.572\n  Degrees of freedom                                27\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              6506.495\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.879\n  Tucker-Lewis Index (TLI)                       0.839\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -14826.170\n  Loglikelihood unrestricted model (H1)     -14421.884\n                                                      \n  Akaike (AIC)                               29688.341\n  Bayesian (BIC)                             29782.439\n  Sample-size adjusted Bayesian (SABIC)      29725.260\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.145\n  90 Percent confidence interval - lower         0.136\n  90 Percent confidence interval - upper         0.154\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.057\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Stress =~                                                             \n    Item1             1.000                               0.810    0.762\n    Item2             1.114    0.036   31.162    0.000    0.902    0.814\n    Item3             0.963    0.035   27.841    0.000    0.780    0.737\n    Item4             0.895    0.036   24.948    0.000    0.725    0.668\n    Item5             0.763    0.031   24.824    0.000    0.618    0.665\n    Item6             0.831    0.029   28.328    0.000    0.673    0.748\n    Item7             0.813    0.035   23.238    0.000    0.659    0.627\n    Item8             0.837    0.031   27.235    0.000    0.678    0.723\n    Item9             0.771    0.032   24.225    0.000    0.624    0.651\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Item1             0.474    0.021   22.527    0.000    0.474    0.419\n   .Item2             0.414    0.020   20.962    0.000    0.414    0.337\n   .Item3             0.511    0.022   23.054    0.000    0.511    0.457\n   .Item4             0.651    0.027   24.084    0.000    0.651    0.553\n   .Item5             0.481    0.020   24.118    0.000    0.481    0.557\n   .Item6             0.356    0.016   22.826    0.000    0.356    0.440\n   .Item7             0.671    0.027   24.515    0.000    0.671    0.607\n   .Item8             0.420    0.018   23.311    0.000    0.420    0.478\n   .Item9             0.530    0.022   24.278    0.000    0.530    0.576\n    Stress            0.656    0.041   16.177    0.000    1.000    1.000\n\nvisualize(fit.modgws, subset = c(1:9))\n\n\n\n\n\n\n\nmeasurement_plot(fit.modgws)\n\n\n\n\n\n\n\nresidual_plots(fit.modgws)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Single factor model</span>"
    ]
  },
  {
    "objectID": "Measurement_models.html",
    "href": "Measurement_models.html",
    "title": "9  Congeneric, tau-equivalent and parallel measurement models",
    "section": "",
    "text": "library(psychTools)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(flexplavaan)\n\n\nmod.congeneric &lt;- '\nN =~ N1 + N2 + N3 + N4 + N5\n'\n\nfit.mod.congeneric &lt;- cfa(mod.congeneric, data = bfi)\n\nvisualize(fit.mod.congeneric, subset = 1:5)\n\n\n\n\n\n\n\nmeasurement_plot(fit.mod.congeneric, sample = 300)\n\nNote: You didn't specify a reference for the ghost line, which means I get to chose it. That shows a lot of trust. I appreciate that. I won't let you down.\n\n\n\n\n\n\n\n\nresidual_plots(fit.mod.congeneric)\n\n\n\n\n\n\n\n\n\nmod.tau.eq &lt;- '\nN =~ a*N1 + a*N2 + a*N3 + a*N4 + a*N5\n'\n\nfit.mod.tau.eq &lt;- cfa(mod.tau.eq, data = bfi, std.lv = TRUE)\n\nvisualize(fit.mod.tau.eq, subset = 1:5)\n\n\n\n\n\n\n\nmeasurement_plot(fit.mod.tau.eq, sample = 300)\n\nNote: You didn't specify a reference for the ghost line, which means I get to chose it. That shows a lot of trust. I appreciate that. I won't let you down.\n\n\n\n\n\n\n\n\nresidual_plots(fit.mod.tau.eq)\n\n\n\n\n\n\n\n\n\nmod.parallel &lt;- '\nN =~ a*N1 + a*N2 + a*N3 + a*N4 + a*N5\n\nN1 ~~ b*N1\nN2 ~~ b*N2\nN3 ~~ b*N3\nN4 ~~ b*N4\nN5 ~~ b*N5\n'\n\nfit.mod.parallel &lt;- cfa(mod.parallel, data = bfi, std.lv = TRUE)\n\n\nvisualize(fit.mod.parallel, subset = 1:5)\n\n\n\n\n\n\n\nmeasurement_plot(fit.mod.parallel, sample = 300)\n\nNote: You didn't specify a reference for the ghost line, which means I get to chose it. That shows a lot of trust. I appreciate that. I won't let you down.\n\n\n\n\n\n\n\n\nresidual_plots(fit.mod.parallel)\n\n\n\n\n\n\n\n\n\nmodels.fit &lt;- compareFit(fit.mod.congeneric, fit.mod.tau.eq, fit.mod.parallel)\n\nsummary(models.fit)\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n                   Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)\nfit.mod.congeneric  5 46177 46236 360.93                                      \nfit.mod.tau.eq      9 46392 46428 584.30     223.36 0.14268       4  &lt; 2.2e-16\nfit.mod.parallel   13 46617 46628 816.60     232.31 0.14556       4  &lt; 2.2e-16\n                      \nfit.mod.congeneric    \nfit.mod.tau.eq     ***\nfit.mod.parallel   ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n####################### Model Fit Indices ###########################\n                      chisq df pvalue rmsea   cfi   tli  srmr        aic\nfit.mod.congeneric 360.932†  5   .000 .163  .925† .849  .056† 46177.007†\nfit.mod.tau.eq     584.296   9   .000 .154  .878  .864  .108  46392.372 \nfit.mod.parallel   816.601  13   .000 .151† .830  .869† .086  46616.677 \n                          bic\nfit.mod.congeneric 46235.995†\nfit.mod.tau.eq     46427.764 \nfit.mod.parallel   46628.475 \n\n################## Differences in Fit Indices #######################\n                                    df  rmsea    cfi   tli   srmr     aic\nfit.mod.tau.eq - fit.mod.congeneric  4 -0.009 -0.047 0.015  0.051 215.364\nfit.mod.parallel - fit.mod.tau.eq    4 -0.003 -0.048 0.004 -0.021 224.305\n                                        bic\nfit.mod.tau.eq - fit.mod.congeneric 191.769\nfit.mod.parallel - fit.mod.tau.eq   200.710",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Congeneric, tau-equivalent and parallel measurement models</span>"
    ]
  },
  {
    "objectID": "Multiple_factors.html",
    "href": "Multiple_factors.html",
    "title": "10  Multiple factors",
    "section": "",
    "text": "library(lavaan)\nlibrary(flexplavaan)\nlibrary(psychTools)\n\n\nmod3 &lt;- '\nNeuroticism   =~ N1 + N2 + N3 + N4 + N5\nAgreeableness =~ A1 + A2 + A3 + A4 + A5\n\nN1 ~~ N2\n'\n\nfit.mod3 &lt;- cfa(model = mod3, \n                data  = bfi)\n\nsummary(fit.mod3, \n        standardized = TRUE, \n        fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 39 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        22\n\n                                                  Used       Total\n  Number of observations                          2618        2800\n\nModel Test User Model:\n                                                      \n  Test statistic                               394.745\n  Degrees of freedom                                33\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              7465.706\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.951\n  Tucker-Lewis Index (TLI)                       0.934\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -43229.863\n  Loglikelihood unrestricted model (H1)     -43032.491\n                                                      \n  Akaike (AIC)                               86503.726\n  Bayesian (BIC)                             86632.870\n  Sample-size adjusted Bayesian (SABIC)      86562.969\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.065\n  90 Percent confidence interval - lower         0.059\n  90 Percent confidence interval - upper         0.071\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.048\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Neuroticism =~                                                        \n    N1                1.000                               1.069    0.681\n    N2                0.940    0.024   38.841    0.000    1.005    0.658\n    N3                1.206    0.041   29.617    0.000    1.289    0.807\n    N4                0.944    0.035   26.800    0.000    1.009    0.643\n    N5                0.833    0.035   23.538    0.000    0.890    0.549\n  Agreeableness =~                                                      \n    A1                1.000                               0.542    0.387\n    A2               -1.413    0.086  -16.450    0.000   -0.765   -0.651\n    A3               -1.838    0.109  -16.933    0.000   -0.996   -0.761\n    A4               -1.343    0.091  -14.789    0.000   -0.728   -0.488\n    A5               -1.487    0.091  -16.357    0.000   -0.806   -0.638\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .N1 ~~                                                                 \n   .N2                0.621    0.040   15.696    0.000    0.621    0.471\n  Neuroticism ~~                                                        \n    Agreeableness     0.117    0.016    7.304    0.000    0.203    0.203\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                1.319    0.048   27.230    0.000    1.319    0.536\n   .N2                1.319    0.047   28.112    0.000    1.319    0.567\n   .N3                0.890    0.048   18.678    0.000    0.890    0.349\n   .N4                1.447    0.049   29.606    0.000    1.447    0.587\n   .N5                1.836    0.057   32.264    0.000    1.836    0.699\n   .A1                1.668    0.049   34.239    0.000    1.668    0.850\n   .A2                0.795    0.029   27.568    0.000    0.795    0.576\n   .A3                0.719    0.035   20.374    0.000    0.719    0.420\n   .A4                1.692    0.052   32.681    0.000    1.692    0.762\n   .A5                0.943    0.033   28.181    0.000    0.943    0.592\n    Neuroticism       1.142    0.066   17.384    0.000    1.000    1.000\n    Agreeableness     0.293    0.033    9.006    0.000    1.000    1.000\n\nvisualize(fit.mod3, \n          subset = 1:10)\n\n\n\n\n\n\n\nmeasurement_plot(fit.mod3, \n                 sample = 300)\n\n$Neuroticism\n\n\n\n\n\n\n\n\n\n\n$Agreeableness\n\n\n\n\n\n\n\n\nresidual_plots(fit.mod3)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multiple factors</span>"
    ]
  },
  {
    "objectID": "Higher_order.html",
    "href": "Higher_order.html",
    "title": "11  Higher order models",
    "section": "",
    "text": "11.1 A second-order factor analysis of nine cognitive variables\nlibrary(lavaan)\n\nThis is lavaan 0.6-19\nlavaan is FREE software! Please report any bugs.\n\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\n\nThe following object is masked from 'package:lavaan':\n\n    cor2cov\n\nlibrary(semPlot)\nmod &lt;- '\nComprehension =~ Sentences + Vocabulary + Sent.Completion\nFluency       =~ First.Letters + Four.Letter.Words + Suffixes\nReasoning     =~ Letter.Series + Pedigrees + Letter.Group\n'\n\nfit.mod &lt;- cfa(mod, sample.cov = Thurstone, sample.nobs = 500, std.lv = TRUE)\nsummary(fit.mod, standardized = TRUE, fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 25 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                90.086\n  Degrees of freedom                                24\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              2598.803\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.974\n  Tucker-Lewis Index (TLI)                       0.961\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5126.360\n  Loglikelihood unrestricted model (H1)      -5081.317\n                                                      \n  Akaike (AIC)                               10294.720\n  Bayesian (BIC)                             10383.227\n  Sample-size adjusted Bayesian (SABIC)      10316.572\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.074\n  90 Percent confidence interval - lower         0.058\n  90 Percent confidence interval - upper         0.091\n  P-value H_0: RMSEA &lt;= 0.050                    0.007\n  P-value H_0: RMSEA &gt;= 0.080                    0.297\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.044\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Comprehension =~                                                      \n    Sentences         0.904    0.035   25.627    0.000    0.904    0.905\n    Vocabulary        0.913    0.035   26.054    0.000    0.913    0.914\n    Sent.Completin    0.855    0.036   23.467    0.000    0.855    0.856\n  Fluency =~                                                            \n    First.Letters     0.835    0.040   21.117    0.000    0.835    0.836\n    Four.Lttr.Wrds    0.796    0.040   19.821    0.000    0.796    0.797\n    Suffixes          0.702    0.042   16.793    0.000    0.702    0.703\n  Reasoning =~                                                          \n    Letter.Series     0.780    0.042   18.651    0.000    0.780    0.781\n    Pedigrees         0.719    0.043   16.851    0.000    0.719    0.720\n    Letter.Group      0.703    0.043   16.362    0.000    0.703    0.703\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Comprehension ~~                                                      \n    Fluency           0.643    0.033   19.542    0.000    0.643    0.643\n    Reasoning         0.670    0.033   20.153    0.000    0.670    0.670\n  Fluency ~~                                                            \n    Reasoning         0.637    0.038   16.700    0.000    0.637    0.637\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Sentences         0.181    0.019    9.788    0.000    0.181    0.181\n   .Vocabulary        0.165    0.018    9.121    0.000    0.165    0.165\n   .Sent.Completin    0.267    0.022   12.296    0.000    0.267    0.267\n   .First.Letters     0.301    0.033    9.075    0.000    0.301    0.302\n   .Four.Lttr.Wrds    0.364    0.034   10.635    0.000    0.364    0.365\n   .Suffixes          0.505    0.039   13.043    0.000    0.505    0.506\n   .Letter.Series     0.390    0.039   10.102    0.000    0.390    0.390\n   .Pedigrees         0.480    0.040   11.876    0.000    0.480    0.481\n   .Letter.Group      0.504    0.041   12.249    0.000    0.504    0.505\n    Comprehension     1.000                               1.000    1.000\n    Fluency           1.000                               1.000    1.000\n    Reasoning         1.000                               1.000    1.000\nsemPaths(fit.mod,\n         ###color       = \"black\",\n         what           = c(\"std\"),\n         weighted       = FALSE,\n         sizeLat        = 14,\n         sizeLat2       = 10,          \n         shapeLat       = \"ellipse\",\n         sizeMan        = 8,\n         nCharNodes     = 3,\n         edge.label.cex = 0.8,\n         label.cex      = 1, \n         edge.color     = \"black\",\n         mar            = c(5,5,5,5),\n         style          = \"lisrel\")\nmod &lt;- '\nComprehension =~ Sentences + Vocabulary + Sent.Completion\nFluency       =~ First.Letters + Four.Letter.Words + Suffixes\nReasoning     =~ Letter.Series + Pedigrees + Letter.Group\n\nGeneral       =~ Comprehension + Fluency + Reasoning\n'\n\nfit.mod &lt;- cfa(mod, sample.cov = Thurstone, sample.nobs = 500, std.lv = TRUE)\nsummary(fit.mod, standardized = TRUE, fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 36 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                90.086\n  Degrees of freedom                                24\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              2598.803\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.974\n  Tucker-Lewis Index (TLI)                       0.961\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5126.360\n  Loglikelihood unrestricted model (H1)      -5081.317\n                                                      \n  Akaike (AIC)                               10294.720\n  Bayesian (BIC)                             10383.227\n  Sample-size adjusted Bayesian (SABIC)      10316.572\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.074\n  90 Percent confidence interval - lower         0.058\n  90 Percent confidence interval - upper         0.091\n  P-value H_0: RMSEA &lt;= 0.050                    0.007\n  P-value H_0: RMSEA &gt;= 0.080                    0.297\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.044\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Comprehension =~                                                      \n    Sentences         0.515    0.041   12.573    0.000    0.904    0.905\n    Vocabulary        0.520    0.041   12.607    0.000    0.913    0.914\n    Sent.Completin    0.487    0.040   12.309    0.000    0.855    0.856\n  Fluency =~                                                            \n    First.Letters     0.521    0.040   13.106    0.000    0.835    0.836\n    Four.Lttr.Wrds    0.497    0.038   12.933    0.000    0.796    0.797\n    Suffixes          0.438    0.036   12.022    0.000    0.702    0.703\n  Reasoning =~                                                          \n    Letter.Series     0.452    0.043   10.533    0.000    0.780    0.781\n    Pedigrees         0.417    0.040   10.310    0.000    0.719    0.720\n    Letter.Group      0.407    0.040   10.210    0.000    0.703    0.703\n  General =~                                                            \n    Comprehension     1.444    0.167    8.643    0.000    0.822    0.822\n    Fluency           1.254    0.138    9.110    0.000    0.782    0.782\n    Reasoning         1.407    0.175    8.033    0.000    0.815    0.815\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Sentences         0.181    0.019    9.788    0.000    0.181    0.181\n   .Vocabulary        0.165    0.018    9.121    0.000    0.165    0.165\n   .Sent.Completin    0.267    0.022   12.296    0.000    0.267    0.267\n   .First.Letters     0.301    0.033    9.075    0.000    0.301    0.302\n   .Four.Lttr.Wrds    0.364    0.034   10.635    0.000    0.364    0.365\n   .Suffixes          0.505    0.039   13.043    0.000    0.505    0.506\n   .Letter.Series     0.390    0.039   10.102    0.000    0.390    0.390\n   .Pedigrees         0.480    0.040   11.876    0.000    0.480    0.481\n   .Letter.Group      0.504    0.041   12.249    0.000    0.504    0.505\n   .Comprehension     1.000                               0.324    0.324\n   .Fluency           1.000                               0.389    0.389\n   .Reasoning         1.000                               0.336    0.336\n    General           1.000                               1.000    1.000\nsemPaths(fit.mod,\n         ###color       = \"black\",\n         what           = c(\"std\"),\n         weighted       = FALSE,\n         sizeLat        = 14,\n         sizeLat2       = 10,          \n         shapeLat       = \"ellipse\",\n         sizeMan        = 8,\n         nCharNodes     = 3,\n         edge.label.cex = 0.8,\n         label.cex      = 1, \n         edge.color     = \"black\",\n         mar            = c(5,5,5,5),\n         style          = \"lisrel\")\nmod &lt;- '\nComprehension =~ Sentences + Vocabulary + Sent.Completion\nFluency       =~ First.Letters + Four.Letter.Words + Suffixes\nReasoning     =~ Letter.Series + Pedigrees + Letter.Group\n\nG =~ Sentences + Vocabulary + Sent.Completion +\n     First.Letters + Four.Letter.Words + Suffixes +\n     Letter.Series + Pedigrees + Letter.Group\n'\n\nfit.mod &lt;- cfa(mod, sample.cov = Thurstone, sample.nobs = 500, std.lv = TRUE, orthogonal = TRUE)\nsummary(fit.mod, standardized = TRUE, fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 30 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        27\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                57.114\n  Degrees of freedom                                18\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              2598.803\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.985\n  Tucker-Lewis Index (TLI)                       0.969\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5109.874\n  Loglikelihood unrestricted model (H1)      -5081.317\n                                                      \n  Akaike (AIC)                               10273.749\n  Bayesian (BIC)                             10387.543\n  Sample-size adjusted Bayesian (SABIC)      10301.844\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.066\n  90 Percent confidence interval - lower         0.047\n  90 Percent confidence interval - upper         0.086\n  P-value H_0: RMSEA &lt;= 0.050                    0.079\n  P-value H_0: RMSEA &gt;= 0.080                    0.124\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.035\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Comprehension =~                                                      \n    Sentences         0.487    0.053    9.203    0.000    0.487    0.488\n    Vocabulary        0.452    0.054    8.315    0.000    0.452    0.452\n    Sent.Completin    0.404    0.057    7.117    0.000    0.404    0.404\n  Fluency =~                                                            \n    First.Letters     0.613    0.055   11.132    0.000    0.613    0.614\n    Four.Lttr.Wrds    0.505    0.053    9.536    0.000    0.505    0.506\n    Suffixes          0.394    0.051    7.758    0.000    0.394    0.394\n  Reasoning =~                                                          \n    Letter.Series     0.727    0.103    7.049    0.000    0.727    0.727\n    Pedigrees         0.247    0.056    4.369    0.000    0.247    0.247\n    Letter.Group      0.409    0.074    5.535    0.000    0.409    0.409\n  G =~                                                                  \n    Sentences         0.767    0.046   16.705    0.000    0.767    0.768\n    Vocabulary        0.790    0.045   17.429    0.000    0.790    0.791\n    Sent.Completin    0.753    0.046   16.271    0.000    0.753    0.754\n    First.Letters     0.608    0.046   13.227    0.000    0.608    0.608\n    Four.Lttr.Wrds    0.597    0.046   12.933    0.000    0.597    0.597\n    Suffixes          0.571    0.047   12.268    0.000    0.571    0.572\n    Letter.Series     0.566    0.047   12.009    0.000    0.566    0.567\n    Pedigrees         0.662    0.046   14.525    0.000    0.662    0.662\n    Letter.Group      0.529    0.048   11.099    0.000    0.529    0.530\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Comprehension ~~                                                      \n    Fluency           0.000                               0.000    0.000\n    Reasoning         0.000                               0.000    0.000\n    G                 0.000                               0.000    0.000\n  Fluency ~~                                                            \n    Reasoning         0.000                               0.000    0.000\n    G                 0.000                               0.000    0.000\n  Reasoning ~~                                                          \n    G                 0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Sentences         0.172    0.022    7.773    0.000    0.172    0.172\n   .Vocabulary        0.170    0.020    8.691    0.000    0.170    0.170\n   .Sent.Completin    0.268    0.022   12.433    0.000    0.268    0.268\n   .First.Letters     0.252    0.052    4.888    0.000    0.252    0.253\n   .Four.Lttr.Wrds    0.387    0.041    9.416    0.000    0.387    0.387\n   .Suffixes          0.517    0.039   13.347    0.000    0.517    0.518\n   .Letter.Series     0.149    0.142    1.051    0.293    0.149    0.150\n   .Pedigrees         0.499    0.039   12.901    0.000    0.499    0.500\n   .Letter.Group      0.551    0.055   10.021    0.000    0.551    0.552\n    Comprehension     1.000                               1.000    1.000\n    Fluency           1.000                               1.000    1.000\n    Reasoning         1.000                               1.000    1.000\n    G                 1.000                               1.000    1.000\nsemPaths(fit.mod,\n         ###color       = \"black\",\n         what           = c(\"std\"),\n         weighted       = FALSE,\n         sizeLat        = 14,\n         sizeLat2       = 10,          \n         shapeLat       = \"ellipse\",\n         sizeMan        = 8,\n         nCharNodes     = 3,\n         edge.label.cex = 0.8,\n         label.cex      = 1, \n         edge.color     = \"black\",\n         mar            = c(5,5,5,5),\n         style          = \"lisrel\",\n         layout = \"spring\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Higher order models</span>"
    ]
  },
  {
    "objectID": "Plotting_models.html",
    "href": "Plotting_models.html",
    "title": "12  Plotting",
    "section": "",
    "text": "12.1 Plotting the unstandardized solution\nlibrary(semPlot)\nsemPaths(fit.mod1,\n         ###color       = \"black\",\n         what           = c(\"est\"), #for standardized use \"std\"\n         weighted       = FALSE,\n         sizeLat        = 14,\n         sizeLat2       = 10,          \n         shapeLat       = \"ellipse\",\n         sizeMan        = 6,\n         nCharNodes     = 0,\n         edge.label.cex = 0.8,\n         label.cex      = 1, \n         edge.color     = \"black\",\n         mar            = c(5,5,5,5))",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "Plotting_models.html#plotting-the-standardized-solution",
    "href": "Plotting_models.html#plotting-the-standardized-solution",
    "title": "12  Plotting",
    "section": "12.2 Plotting the standardized solution",
    "text": "12.2 Plotting the standardized solution\n\nlibrary(semPlot)\nsemPaths(fit.mod1,\n         ###color       = \"black\",\n         what           = c(\"std\"), # for unstandardized use \"est\"\n         weighted       = FALSE,\n         sizeLat        = 14,\n         sizeLat2       = 10,          \n         shapeLat       = \"ellipse\",\n         sizeMan        = 6,\n         nCharNodes     = 0,\n         edge.label.cex = 0.8,\n         label.cex      = 1, \n         edge.color     = \"black\",\n         mar            = c(5,5,5,5))",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "MIMIC Models.html",
    "href": "MIMIC Models.html",
    "title": "14  MIMIC models",
    "section": "",
    "text": "library(psychTools)\nlibrary(flexplot)\n\n\nAttaching package: 'flexplot'\n\n\nThe following object is masked from 'package:psychTools':\n\n    income",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>MIMIC models</span>"
    ]
  },
  {
    "objectID": "Modification_indices.html",
    "href": "Modification_indices.html",
    "title": "17  Modification Indices",
    "section": "",
    "text": "library(lavaan)\n\nThis is lavaan 0.6-19\nlavaan is FREE software! Please report any bugs.\n\nlibrary(psychTools)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::recode() masks psychTools::recode()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nmod3 &lt;- '\nNeuroticism   =~ N1 + N2 + N3 + N4 + N5\nAgreeableness =~ A1 + A2 + A3 + A4 + A5\n'\n\nfit.mod3 &lt;- cfa(model = mod3, \n                data  = bfi)\n\nsummary(fit.mod3, \n        standardized = TRUE, \n        fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 36 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n                                                  Used       Total\n  Number of observations                          2618        2800\n\nModel Test User Model:\n                                                      \n  Test statistic                               694.620\n  Degrees of freedom                                34\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              7465.706\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.911\n  Tucker-Lewis Index (TLI)                       0.882\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -43379.800\n  Loglikelihood unrestricted model (H1)     -43032.491\n                                                      \n  Akaike (AIC)                               86801.601\n  Bayesian (BIC)                             86924.874\n  Sample-size adjusted Bayesian (SABIC)      86858.151\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.086\n  90 Percent confidence interval - lower         0.081\n  90 Percent confidence interval - upper         0.092\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.966\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.054\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Neuroticism =~                                                        \n    N1                1.000                               1.292    0.823\n    N2                0.949    0.023   40.791    0.000    1.226    0.803\n    N3                0.884    0.024   36.571    0.000    1.141    0.714\n    N4                0.680    0.024   27.853    0.000    0.878    0.559\n    N5                0.626    0.025   24.591    0.000    0.809    0.499\n  Agreeableness =~                                                      \n    A1                1.000                               0.545    0.389\n    A2               -1.404    0.085  -16.527    0.000   -0.765   -0.651\n    A3               -1.822    0.107  -17.018    0.000   -0.992   -0.759\n    A4               -1.338    0.090  -14.855    0.000   -0.729   -0.489\n    A5               -1.483    0.090  -16.448    0.000   -0.808   -0.640\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Neuroticism ~~                                                        \n    Agreeableness     0.155    0.019    8.049    0.000    0.220    0.220\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                0.793    0.036   22.094    0.000    0.793    0.322\n   .N2                0.826    0.035   23.877    0.000    0.826    0.355\n   .N3                1.249    0.043   29.352    0.000    1.249    0.490\n   .N4                1.694    0.051   33.284    0.000    1.694    0.687\n   .N5                1.974    0.058   34.081    0.000    1.974    0.751\n   .A1                1.665    0.049   34.211    0.000    1.665    0.849\n   .A2                0.796    0.029   27.597    0.000    0.796    0.576\n   .A3                0.725    0.035   20.614    0.000    0.725    0.424\n   .A4                1.690    0.052   32.667    0.000    1.690    0.761\n   .A5                0.939    0.033   28.108    0.000    0.939    0.590\n    Neuroticism       1.668    0.070   23.696    0.000    1.000    1.000\n    Agreeableness     0.297    0.033    9.060    0.000    1.000    1.000\n\n\n\nmodificationindices(fit.mod3)\n\n             lhs op rhs      mi    epc sepc.lv sepc.all sepc.nox\n24   Neuroticism =~  A1  19.035  0.098   0.127    0.091    0.091\n25   Neuroticism =~  A2  20.795  0.079   0.102    0.087    0.087\n26   Neuroticism =~  A3  29.798  0.104   0.135    0.103    0.103\n27   Neuroticism =~  A4   5.262 -0.053  -0.069   -0.046   -0.046\n28   Neuroticism =~  A5  50.336 -0.132  -0.171   -0.135   -0.135\n29 Agreeableness =~  N1   0.630  0.038   0.021    0.013    0.013\n30 Agreeableness =~  N2   0.295  0.025   0.014    0.009    0.009\n31 Agreeableness =~  N3  10.202 -0.168  -0.091   -0.057   -0.057\n32 Agreeableness =~  N4  25.954  0.293   0.160    0.102    0.102\n33 Agreeableness =~  N5  11.128 -0.205  -0.112   -0.069   -0.069\n34            N1 ~~  N2 345.707  0.775   0.775    0.958    0.958\n35            N1 ~~  N3  52.136 -0.272  -0.272   -0.274   -0.274\n36            N1 ~~  N4  76.839 -0.291  -0.291   -0.251   -0.251\n37            N1 ~~  N5  19.022 -0.149  -0.149   -0.119   -0.119\n38            N1 ~~  A1  24.399  0.137   0.137    0.119    0.119\n39            N1 ~~  A2   4.495 -0.044  -0.044   -0.055   -0.055\n40            N1 ~~  A3  13.106  0.079   0.079    0.104    0.104\n41            N1 ~~  A4   5.855  0.069   0.069    0.059    0.059\n42            N1 ~~  A5   5.301 -0.051  -0.051   -0.059   -0.059\n43            N2 ~~  N3  37.647 -0.220  -0.220   -0.216   -0.216\n44            N2 ~~  N4  68.849 -0.266  -0.266   -0.225   -0.225\n45            N2 ~~  N5  42.497 -0.217  -0.217   -0.170   -0.170\n46            N2 ~~  A1   3.438  0.051   0.051    0.043    0.043\n47            N2 ~~  A2  14.146  0.077   0.077    0.095    0.095\n48            N2 ~~  A3   0.861  0.020   0.020    0.026    0.026\n49            N2 ~~  A4  14.742 -0.108  -0.108   -0.091   -0.091\n50            N2 ~~  A5   3.613 -0.042  -0.042   -0.048   -0.048\n51            N3 ~~  N4 163.251  0.436   0.436    0.300    0.300\n52            N3 ~~  N5  47.254  0.247   0.247    0.157    0.157\n53            N3 ~~  A1   0.111  0.010   0.010    0.007    0.007\n54            N3 ~~  A2   0.254  0.012   0.012    0.012    0.012\n55            N3 ~~  A3   3.474  0.046   0.046    0.048    0.048\n56            N3 ~~  A4   0.614  0.025   0.025    0.017    0.017\n57            N3 ~~  A5   0.171  0.010   0.010    0.010    0.010\n58            N4 ~~  N5  86.112  0.360   0.360    0.197    0.197\n59            N4 ~~  A1  17.767 -0.146  -0.146   -0.087   -0.087\n60            N4 ~~  A2   0.251  0.013   0.013    0.011    0.011\n61            N4 ~~  A3   3.030 -0.047  -0.047   -0.043   -0.043\n62            N4 ~~  A4  20.479 -0.161  -0.161   -0.095   -0.095\n63            N4 ~~  A5  10.813 -0.092  -0.092   -0.073   -0.073\n64            N5 ~~  A1   8.114 -0.106  -0.106   -0.058   -0.058\n65            N5 ~~  A2   7.324  0.075   0.075    0.060    0.060\n66            N5 ~~  A3   5.725 -0.069  -0.069   -0.058   -0.058\n67            N5 ~~  A4   8.224  0.109   0.109    0.060    0.060\n68            N5 ~~  A5   0.301  0.016   0.016    0.012    0.012\n69            A1 ~~  A2  64.589 -0.219  -0.219   -0.190   -0.190\n70            A1 ~~  A3   6.688  0.080   0.080    0.073    0.073\n71            A1 ~~  A4   4.991  0.080   0.080    0.048    0.048\n72            A1 ~~  A5  22.962  0.140   0.140    0.112    0.112\n73            A2 ~~  A3   0.654 -0.027  -0.027   -0.036   -0.036\n74            A2 ~~  A4   3.109  0.051   0.051    0.044    0.044\n75            A2 ~~  A5  20.439 -0.124  -0.124   -0.143   -0.143\n76            A3 ~~  A4   0.417 -0.022  -0.022   -0.020   -0.020\n77            A3 ~~  A5  31.301  0.197   0.197    0.239    0.239\n78            A4 ~~  A5   0.192 -0.014  -0.014   -0.011   -0.011\n\nmymi &lt;- modificationindices(fit.mod3)\n\n\nmymi %&gt;% \n  filter(op == \"~~\") %&gt;% \n  arrange(desc(mi)) %&gt;% \n  slice(1:20)\n\n   lhs op rhs      mi    epc sepc.lv sepc.all sepc.nox\n1   N1 ~~  N2 345.707  0.775   0.775    0.958    0.958\n2   N3 ~~  N4 163.251  0.436   0.436    0.300    0.300\n3   N4 ~~  N5  86.112  0.360   0.360    0.197    0.197\n4   N1 ~~  N4  76.839 -0.291  -0.291   -0.251   -0.251\n5   N2 ~~  N4  68.849 -0.266  -0.266   -0.225   -0.225\n6   A1 ~~  A2  64.589 -0.219  -0.219   -0.190   -0.190\n7   N1 ~~  N3  52.136 -0.272  -0.272   -0.274   -0.274\n8   N3 ~~  N5  47.254  0.247   0.247    0.157    0.157\n9   N2 ~~  N5  42.497 -0.217  -0.217   -0.170   -0.170\n10  N2 ~~  N3  37.647 -0.220  -0.220   -0.216   -0.216\n11  A3 ~~  A5  31.301  0.197   0.197    0.239    0.239\n12  N1 ~~  A1  24.399  0.137   0.137    0.119    0.119\n13  A1 ~~  A5  22.962  0.140   0.140    0.112    0.112\n14  N4 ~~  A4  20.479 -0.161  -0.161   -0.095   -0.095\n15  A2 ~~  A5  20.439 -0.124  -0.124   -0.143   -0.143\n16  N1 ~~  N5  19.022 -0.149  -0.149   -0.119   -0.119\n17  N4 ~~  A1  17.767 -0.146  -0.146   -0.087   -0.087\n18  N2 ~~  A4  14.742 -0.108  -0.108   -0.091   -0.091\n19  N2 ~~  A2  14.146  0.077   0.077    0.095    0.095\n20  N1 ~~  A3  13.106  0.079   0.079    0.104    0.104\n\nmymi %&gt;% \n  filter(op == \"~~\" & mi &gt; 100)\n\n  lhs op rhs      mi   epc sepc.lv sepc.all sepc.nox\n1  N1 ~~  N2 345.707 0.775   0.775    0.958    0.958\n2  N3 ~~  N4 163.251 0.436   0.436    0.300    0.300\n\n\n\nmod4 &lt;- '\nNeuroticism   =~ N1 + N2 + N3 + N4 + N5\nAgreeableness =~ A1 + A2 + A3 + A4 + A5\nN1 ~~ N2\n'\n\nfit.mod4 &lt;- cfa(model = mod4, \n                data  = bfi)\n\nsummary(fit.mod4, \n        standardized = TRUE, \n        fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 39 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        22\n\n                                                  Used       Total\n  Number of observations                          2618        2800\n\nModel Test User Model:\n                                                      \n  Test statistic                               394.745\n  Degrees of freedom                                33\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              7465.706\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.951\n  Tucker-Lewis Index (TLI)                       0.934\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -43229.863\n  Loglikelihood unrestricted model (H1)     -43032.491\n                                                      \n  Akaike (AIC)                               86503.726\n  Bayesian (BIC)                             86632.870\n  Sample-size adjusted Bayesian (SABIC)      86562.969\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.065\n  90 Percent confidence interval - lower         0.059\n  90 Percent confidence interval - upper         0.071\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.048\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Neuroticism =~                                                        \n    N1                1.000                               1.069    0.681\n    N2                0.940    0.024   38.841    0.000    1.005    0.658\n    N3                1.206    0.041   29.617    0.000    1.289    0.807\n    N4                0.944    0.035   26.800    0.000    1.009    0.643\n    N5                0.833    0.035   23.538    0.000    0.890    0.549\n  Agreeableness =~                                                      \n    A1                1.000                               0.542    0.387\n    A2               -1.413    0.086  -16.450    0.000   -0.765   -0.651\n    A3               -1.838    0.109  -16.933    0.000   -0.996   -0.761\n    A4               -1.343    0.091  -14.789    0.000   -0.728   -0.488\n    A5               -1.487    0.091  -16.357    0.000   -0.806   -0.638\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .N1 ~~                                                                 \n   .N2                0.621    0.040   15.696    0.000    0.621    0.471\n  Neuroticism ~~                                                        \n    Agreeableness     0.117    0.016    7.304    0.000    0.203    0.203\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                1.319    0.048   27.230    0.000    1.319    0.536\n   .N2                1.319    0.047   28.112    0.000    1.319    0.567\n   .N3                0.890    0.048   18.678    0.000    0.890    0.349\n   .N4                1.447    0.049   29.606    0.000    1.447    0.587\n   .N5                1.836    0.057   32.264    0.000    1.836    0.699\n   .A1                1.668    0.049   34.239    0.000    1.668    0.850\n   .A2                0.795    0.029   27.568    0.000    0.795    0.576\n   .A3                0.719    0.035   20.374    0.000    0.719    0.420\n   .A4                1.692    0.052   32.681    0.000    1.692    0.762\n   .A5                0.943    0.033   28.181    0.000    0.943    0.592\n    Neuroticism       1.142    0.066   17.384    0.000    1.000    1.000\n    Agreeableness     0.293    0.033    9.006    0.000    1.000    1.000\n\n\n\ncompfit &lt;- semTools::compareFit(fit.mod3, fit.mod4)\nsummary(compfit)\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n         Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nfit.mod4 33 86504 86633 394.75                                          \nfit.mod3 34 86802 86925 694.62     299.87 0.33788       1  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n####################### Model Fit Indices ###########################\n            chisq df pvalue rmsea   cfi   tli  srmr        aic        bic\nfit.mod4 394.745† 33   .000 .065† .951† .934† .048† 86503.726† 86632.870†\nfit.mod3 694.620  34   .000 .086  .911  .882  .054  86801.601  86924.874 \n\n################## Differences in Fit Indices #######################\n                    df rmsea   cfi    tli  srmr     aic     bic\nfit.mod3 - fit.mod4  1 0.021 -0.04 -0.051 0.005 297.874 292.004",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modification Indices</span>"
    ]
  },
  {
    "objectID": "EFA.html",
    "href": "EFA.html",
    "title": "18  Exploratory factor analysis",
    "section": "",
    "text": "library(lavaan)\nlibrary(flexplavaan)\nlibrary(psychTools)\nlibrary(psych)\nlibrary(tidyverse)\n\nbfi %&gt;% \n  select(1:5, 16:20) %&gt;% \n  scree()\n\n\n\n\n\n\n\nbfi %&gt;% \n  select(1:5, 16:20) %&gt;% \n  fa.parallel()\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  3  and the number of components =  2 \n\nmyfa &lt;- bfi %&gt;% \n  select(1:5, 16:20) %&gt;% \n  fa(nfactors = 2)\n\nprint(myfa$loadings, cut = 0)\n\n\nLoadings:\n   MR1    MR2   \nA1  0.074 -0.359\nA2  0.054  0.692\nA3  0.031  0.756\nA4 -0.052  0.474\nA5 -0.119  0.595\nN1  0.776 -0.027\nN2  0.760 -0.022\nN3  0.770  0.051\nN4  0.581 -0.078\nN5  0.543  0.078\n\n                 MR1   MR2\nSS loadings    2.432 1.773\nProportion Var 0.243 0.177\nCumulative Var 0.243 0.420\n\ndata.frame(myfa$complexity) %&gt;% \n  arrange(myfa.complexity)\n\n   myfa.complexity\nN2        1.001752\nN1        1.002362\nA3        1.003383\nN3        1.008872\nA2        1.012251\nA4        1.023934\nN4        1.035619\nN5        1.041347\nA5        1.079964\nA1        1.085700",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exploratory factor analysis</span>"
    ]
  },
  {
    "objectID": "Correlated_residuals.html",
    "href": "Correlated_residuals.html",
    "title": "19  Correlated residuals",
    "section": "",
    "text": "library(psychTools)\nlibrary(lavaan)\nlibrary(flexplavaan)\n\nmod4 &lt;- '\nNeuroticism   =~ N1 + N2 + N3 + N4 + N5\nAgreeableness =~ A1 + A2 + A3 + A4 + A5\n\nN1 ~~ N2\nN4 ~~ N5\n'\n\nfit.mod4 &lt;- cfa(mod4, \n                data = bfi, std.lv = TRUE)\n\nsummary(fit.mod4, \n        standardized = TRUE, \n        fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 25 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        23\n\n                                                  Used       Total\n  Number of observations                          2618        2800\n\nModel Test User Model:\n                                                      \n  Test statistic                               374.075\n  Degrees of freedom                                32\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              7465.706\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.954\n  Tucker-Lewis Index (TLI)                       0.935\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -43219.528\n  Loglikelihood unrestricted model (H1)     -43032.491\n                                                      \n  Akaike (AIC)                               86485.056\n  Bayesian (BIC)                             86620.070\n  Sample-size adjusted Bayesian (SABIC)      86546.992\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.064\n  90 Percent confidence interval - lower         0.058\n  90 Percent confidence interval - upper         0.070\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.048\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Neuroticism =~                                                        \n    N1                1.072    0.031   34.548    0.000    1.072    0.684\n    N2                1.010    0.030   33.138    0.000    1.010    0.662\n    N3                1.308    0.031   42.422    0.000    1.308    0.819\n    N4                0.980    0.031   31.196    0.000    0.980    0.624\n    N5                0.853    0.034   25.396    0.000    0.853    0.526\n  Agreeableness =~                                                      \n    A1                0.542    0.030   18.024    0.000    0.542    0.387\n    A2               -0.766    0.024  -32.231    0.000   -0.766   -0.652\n    A3               -0.996    0.026  -38.227    0.000   -0.996   -0.761\n    A4               -0.728    0.031  -23.243    0.000   -0.728   -0.488\n    A5               -0.805    0.026  -31.494    0.000   -0.805   -0.638\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .N1 ~~                                                                 \n   .N2                0.612    0.040   15.115    0.000    0.612    0.467\n .N4 ~~                                                                 \n   .N5                0.180    0.040    4.451    0.000    0.180    0.106\n  Neuroticism ~~                                                        \n    Agreeableness     0.200    0.024    8.297    0.000    0.200    0.200\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                1.312    0.049   26.597    0.000    1.312    0.533\n   .N2                1.309    0.048   27.511    0.000    1.309    0.562\n   .N3                0.841    0.051   16.442    0.000    0.841    0.329\n   .N4                1.504    0.051   29.470    0.000    1.504    0.610\n   .N5                1.900    0.059   31.996    0.000    1.900    0.723\n   .A1                1.668    0.049   34.235    0.000    1.668    0.850\n   .A2                0.795    0.029   27.550    0.000    0.795    0.575\n   .A3                0.719    0.035   20.381    0.000    0.719    0.420\n   .A4                1.692    0.052   32.680    0.000    1.692    0.762\n   .A5                0.943    0.033   28.186    0.000    0.943    0.593\n    Neuroticism       1.000                               1.000    1.000\n    Agreeableness     1.000                               1.000    1.000\n\n\n\nlibrary(semPlot)\nsemPaths(fit.mod4,\n         ###color       = \"black\",\n         what           = c(\"std\"),\n         weighted       = FALSE,\n         sizeLat        = 14,\n         sizeLat2       = 10,          \n         shapeLat       = \"ellipse\",\n         sizeMan        = 6,\n         nCharNodes     = 0,\n         edge.label.cex = 0.8,\n         label.cex      = 1, \n         edge.color     = \"black\",\n         mar            = c(5,5,5,5),\n         style          = \"lisrel\")",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Correlated residuals</span>"
    ]
  },
  {
    "objectID": "PathModels.html",
    "href": "PathModels.html",
    "title": "20  Path analysis models",
    "section": "",
    "text": "20.1 Simple regression\nSay we want to examine the effect of workload on subjectively experienced stress. We can assume that both variables are continuous and are measured on an interval level. We have data with respect to these two variables for ?? cases in the swsihonours.csv file.\nmydata &lt;- read.csv(\"data/SWSIhonours.csv\")\nAs a first step it is useful to create a scatter plot that visually depicts the relationship between the two variables. The flexplot() function of the flexplot package creates excellent graphs, which by default plots the raw data and a non-parametric loess line that shows the trend in the data. In the example below we plot the effect of WL on GWS. In addition to the loess line we can also plot straight lines (“lm”), polynomial lines (“polynomial”), logistic lines (“logistic”). In this case the loess line clearly suggests a linear relationship. The plot reveals the presence of two severe outliers (there is a person with a WL score of zero and another with a GWS score of zero!).\nlibrary(flexplot)\nflexplot(GWS ~ WL, data = mydata, method = \"loess\")\nflexplot::flexplot(GWS ~ WL, data = mydata, method = \"lm\")",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Path analysis models</span>"
    ]
  },
  {
    "objectID": "PathModels.html#dealing-with-the-outliers",
    "href": "PathModels.html#dealing-with-the-outliers",
    "title": "20  Path analysis models",
    "section": "20.2 Dealing with the outliers",
    "text": "20.2 Dealing with the outliers\nOutliers can potentially influence the observed relationship between variables. It would be best to deal with the two outliers we have detected by deleting the two scores and treating them as missing data (which in an R data frame is represented by the letters NA). The SWSIhonours.csv file contains some zeroes were such values are not permitted. There are many ways to deal with such a problem. Here we use the scrub() function of the psych package to recode all zeroes in the data frame to NA (missing).\n\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\n\nThe following object is masked from 'package:flexplot':\n\n    rescale\n\nmydata &lt;- scrub(mydata, isvalue = 0)\n\n#library(tidyverse)\n#mydata &lt;- mydata %&gt;% \n#  mutate(WL = ifelse(WL &lt; 1, NA, WL)) %&gt;% \n#  mutate(GWS = ifelse(GWS &lt; 1, NA, GWS)) %&gt;% \n#  mutate(RA = ifelse(RA &lt; 1, NA, RA)) %&gt;%\n#  mutate(REL = ifelse(REL &lt; 1, NA, REL)) %&gt;%\n#  mutate(TE = ifelse(TE &lt; 1, NA, TE)) %&gt;%\n#  mutate(JS = ifelse(JS &lt; 1, NA, JS)) %&gt;%\n#  mutate(CA = ifelse(CA &lt; 1, NA, CA)) %&gt;%\n#  mutate(LA = ifelse(LA &lt; 1, NA, LA)) %&gt;%  \n#  mutate(WH = ifelse(WH &lt; 1, NA, WH))\n\nThe scatterplots now do not include the outliers. The loess line is about dead straight which suggests that the relationship between GWS and WL is linear. We can therefore fit a straight line to the data.\n\nflexplot::flexplot(GWS ~ WL, data = mydata, method = \"loess\")\n\n\n\n\n\n\n\n\n\nflexplot::flexplot(GWS ~ WL, data = mydata, method = \"lm\")",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Path analysis models</span>"
    ]
  },
  {
    "objectID": "PathModels.html#fitting-a-simple-regression-model-of-the-relationship-between-wl-and-gws",
    "href": "PathModels.html#fitting-a-simple-regression-model-of-the-relationship-between-wl-and-gws",
    "title": "20  Path analysis models",
    "section": "20.3 Fitting a simple regression model of the relationship between WL and GWS",
    "text": "20.3 Fitting a simple regression model of the relationship between WL and GWS\n\nlibrary(lavaan)",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Path analysis models</span>"
    ]
  },
  {
    "objectID": "PathModels.html#specifying-the-model-as-a-structural-equation-model",
    "href": "PathModels.html#specifying-the-model-as-a-structural-equation-model",
    "title": "20  Path analysis models",
    "section": "20.4 Specifying the model as a structural equation model",
    "text": "20.4 Specifying the model as a structural equation model\n\nmodel1 &lt;- '\nGWS ~ WL\n'",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Path analysis models</span>"
    ]
  },
  {
    "objectID": "PathModels.html#fitting-the-model-to-observed-data",
    "href": "PathModels.html#fitting-the-model-to-observed-data",
    "title": "20  Path analysis models",
    "section": "20.5 Fitting the model to observed data",
    "text": "20.5 Fitting the model to observed data\nWe fit the model to the observed data in the mydata data frame using the sem() function. We store the results in an object we call fit.mod1.\n\nfit.model1 &lt;- sem(model1, data = mydata)",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Path analysis models</span>"
    ]
  },
  {
    "objectID": "PathModels.html#examining-the-results",
    "href": "PathModels.html#examining-the-results",
    "title": "20  Path analysis models",
    "section": "20.6 Examining the results",
    "text": "20.6 Examining the results\nWe examine the results by asking for a summary of fit.model1.\n\nsummary(fit.model1, standardized = TRUE)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         2\n\n                                                  Used       Total\n  Number of observations                          1426        1430\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  GWS ~                                                                 \n    WL                0.514    0.023   21.958    0.000    0.514    0.503\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .GWS              33.549    1.256   26.702    0.000   33.549    0.747",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Path analysis models</span>"
    ]
  },
  {
    "objectID": "PathModels.html#plotting-the-model",
    "href": "PathModels.html#plotting-the-model",
    "title": "20  Path analysis models",
    "section": "20.7 Plotting the model",
    "text": "20.7 Plotting the model\nWe use the semPaths() function of the semPlot model to plot the structural equation model. We need to specify the name of the fitted model. Students can experiment with different values for the color, what, weighted, sizeMan, nCharNodes, edge.label.cex, label.cex, edge.color, label.color and mar arguments and observe the effects thereof. With respect to the “what” argument we can request standardized parameters (“std”) or unstandardized parameters (“est). Our model does not contain latent variables and the arguments relating to such variables does not affect the plot.\n\nlibrary(semPlot)\nsemPaths(fit.model1,\n         color       = \"white\",   ## try black, yellow, blue etc.\n         what           = c(\"est\"), ## try std for standardized\n         weighted       = FALSE,\n         sizeLat        = 14,\n         sizeLat2       = 10,          \n         shapeLat       = \"ellipse\",\n         sizeMan        = 6,\n         nCharNodes     = 0,\n         edge.label.cex = 0.8,\n         label.cex      = 1, \n         edge.color     = \"black\",\n         label.color    = \"black\",\n         mar            = c(5,5,5,5),\n         style          = \"lisrel\")\n\n\n\n\n\n\n\n\nOur results show that WL has a positive effect on GWS (i.e. an increase in WL is associated with an increas in GWS). The unstandardized regression coefficient shows that a one unit increase in WL leads to a .51 unit increase in WL. The R-square statistic shows that WL accounts for about 25% of the variance in GWS.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Path analysis models</span>"
    ]
  },
  {
    "objectID": "PathModels.html#exercise",
    "href": "PathModels.html#exercise",
    "title": "20  Path analysis models",
    "section": "20.8 Exercise",
    "text": "20.8 Exercise\nFit a series of simple regression models were GWS is the dependent variable and RA (role ambiguity), REL (relationships), LA (lack of autonomy), CA (career advancement) and JS (job security) in turn are the independent variables. Note the differences between the regression coefficients and the R-square statistics across the different models. Plot each of the models using semPlot.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Path analysis models</span>"
    ]
  },
  {
    "objectID": "MissingDataExamples.html",
    "href": "MissingDataExamples.html",
    "title": "21  Dealing with missing data",
    "section": "",
    "text": "21.1 R packages used in the demonstration\nWe use the following packages: psychTools (to access the bfi data set), mice (to detect the patterns of missing data), naniar (to perform Little’s MCAR test) , Amelia (to perform the multiple imputation), lavaan (to perform the confirmatory factor analysis), and lavaan.mi (to perform the confirmatory factor analysis with multiple complete data sets).\nThese packages need to be installed once. To install the packages you need to remove the # at the beginning of each row (R does not evaluate any code that follows a #).\n#install.packages(\"psychTools\")\n#install.packages(\"naniar\")\n#install.packages(\"lavaan\")\n#install.packages(\"mice\")\n#install.packages(\"Amelia\")\nThe lavaan.mi package is not yet available from the CRAN repository, but it can be downloaded and installed from Terence Jorgenson’s github page. For this you will also need the remotes package.\n#install.packages(\"remotes\")\n#remotes::install_github(\"TDJorgensen/lavaan.mi\")",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Dealing with missing data</span>"
    ]
  },
  {
    "objectID": "MissingDataExamples.html#cfa-of-the-5-item-neuroticism-scale-of-the-big-five-inventory",
    "href": "MissingDataExamples.html#cfa-of-the-5-item-neuroticism-scale-of-the-big-five-inventory",
    "title": "21  Dealing with missing data",
    "section": "21.2 CFA of the 5-item Neuroticism scale of the Big Five Inventory",
    "text": "21.2 CFA of the 5-item Neuroticism scale of the Big Five Inventory\nI performed a confirmatory factor analysis of the Neuroticism scale of the Big Five Inventory. The data are in the bfi data frame, which can be found in the psychTools package. There were 2800 participants, but there were only 2694 complete cases. For convenience, I stored the five Neuroticism items (which are in columns 16 to 20 of the bfi data frame) in a new data frame called Ndata.\nIn this baseline analysis I ignored the missing data. By default, lavaan employs listwise deletion when missing data are encountered. At the top of the output it can be seen that the number of observed (n = 2800) and used (n = 2694) cases differ.\n\nlibrary(psychTools)\nlibrary(lavaan)\nlibrary(lavaan.mi)\n\nNdata &lt;- bfi[16:20]\n\nNmodel  &lt;- '\nNfactor =~ N1 + N2 + N3 + N4 + N5\n'\n\nfit.Nmodel &lt;- cfa(Nmodel, \n                  data      = Ndata, \n                  estimator = \"MLR\")\n\nsummary(fit.Nmodel,\n        standardized = TRUE,\n        fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 26 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n                                                  Used       Total\n  Number of observations                          2694        2800\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               360.932     313.521\n  Degrees of freedom                                 5           5\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.151\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              4724.621    3492.154\n  Degrees of freedom                                10          10\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.353\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.925       0.911\n  Tucker-Lewis Index (TLI)                       0.849       0.823\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.925\n  Robust Tucker-Lewis Index (TLI)                            0.849\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -23078.504  -23078.504\n  Scaling correction factor                                  1.007\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -22898.038  -22898.038\n  Scaling correction factor                                  1.055\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               46177.007   46177.007\n  Bayesian (BIC)                             46235.995   46235.995\n  Sample-size adjusted Bayesian (SABIC)      46204.222   46204.222\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.163       0.151\n  90 Percent confidence interval - lower         0.149       0.138\n  90 Percent confidence interval - upper         0.177       0.165\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.162\n  90 Percent confidence interval - lower                     0.147\n  90 Percent confidence interval - upper                     0.178\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.056       0.056\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Nfactor =~                                                            \n    N1                1.000                               1.286    0.818\n    N2                0.952    0.017   54.734    0.000    1.225    0.803\n    N3                0.892    0.028   31.540    0.000    1.147    0.717\n    N4                0.677    0.030   22.224    0.000    0.872    0.554\n    N5                0.632    0.030   21.255    0.000    0.813    0.502\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                0.819    0.048   16.954    0.000    0.819    0.331\n   .N2                0.828    0.046   18.124    0.000    0.828    0.356\n   .N3                1.245    0.052   23.807    0.000    1.245    0.486\n   .N4                1.714    0.055   31.118    0.000    1.714    0.693\n   .N5                1.968    0.057   34.381    0.000    1.968    0.748\n    Nfactor           1.655    0.065   25.460    0.000    1.000    1.000",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Dealing with missing data</span>"
    ]
  },
  {
    "objectID": "MissingDataExamples.html#examining-patterns-of-missing-data-and-testing-for-missing-completely-at-random",
    "href": "MissingDataExamples.html#examining-patterns-of-missing-data-and-testing-for-missing-completely-at-random",
    "title": "21  Dealing with missing data",
    "section": "21.3 Examining patterns of missing data and testing for “missing completely at random”",
    "text": "21.3 Examining patterns of missing data and testing for “missing completely at random”\nNext, I employed the md.pattern() function of the mice package to identify the patterns of missing data. I also used the mcar_test() function of the naniar package to perform Little’s missing completely at random (MCAR) test. There were 11 missing data patterns. There were 11 missing values for item N3, 21 for item N2, 22 for item N1, 29 for item N5, and 36 for item N4, which gives a total of 119 missing values. The pattern with the most missing values contained three missing values (four persons produced this pattern).\nLittle’s MCAR test showed that the null hypothesis that the missing data are completely at random could not be rejected: \\(\\chi^2(34) = 27.2, p = 0.791\\)\n\nlibrary(mice)\nmd.pattern(Ndata)\n\n\n\n\n\n\n\n\n     N3 N2 N1 N5 N4    \n2694  1  1  1  1  1   0\n32    1  1  1  1  0   1\n22    1  1  1  0  1   1\n15    1  1  0  1  1   1\n2     1  1  0  0  1   2\n4     1  1  0  0  0   3\n19    1  0  1  1  1   1\n1     1  0  1  0  1   2\n9     0  1  1  1  1   1\n1     0  1  0  1  1   2\n1     0  0  1  1  1   2\n     11 21 22 29 36 119\n\nlibrary(naniar)\nmcar_test(Ndata)\n\n# A tibble: 1 × 4\n  statistic    df p.value missing.patterns\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;int&gt;\n1      27.2    34   0.791               11",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Dealing with missing data</span>"
    ]
  },
  {
    "objectID": "MissingDataExamples.html#full-information-maximum-likelihood-estimation",
    "href": "MissingDataExamples.html#full-information-maximum-likelihood-estimation",
    "title": "21  Dealing with missing data",
    "section": "21.4 Full information maximum likelihood estimation",
    "text": "21.4 Full information maximum likelihood estimation\nSecond, I estimated the parameters of the confirmatory factor analysis model using full information maximum likelihood (“fiml”). This approach uses all the available information in the data set to estimate the parameters. It does not estimate what the missing values would be and it does not fill it in to complete the data set. Note that all 2800 cases were now used.\n\nfit.Nmodel.fiml &lt;- cfa(Nmodel,\n                       data      = Ndata, \n                       estimator = \"MLR\", \n                       missing   = \"fiml\",\n                       fixed.x   = FALSE)\n\nsummary(fit.Nmodel.fiml,\n        standardized = TRUE,\n        fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 30 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n\n  Number of observations                          2800\n  Number of missing patterns                        11\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               372.501     322.671\n  Degrees of freedom                                 5           5\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.154\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              4874.228    3604.773\n  Degrees of freedom                                10          10\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.352\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.924       0.912\n  Tucker-Lewis Index (TLI)                       0.849       0.823\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.924\n  Robust Tucker-Lewis Index (TLI)                            0.848\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -23768.675  -23768.675\n  Scaling correction factor                                  1.004\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -23582.425  -23582.425\n  Scaling correction factor                                  1.041\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               47567.350   47567.350\n  Bayesian (BIC)                             47656.411   47656.411\n  Sample-size adjusted Bayesian (SABIC)      47608.751   47608.751\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.162       0.151\n  90 Percent confidence interval - lower         0.148       0.138\n  90 Percent confidence interval - upper         0.176       0.164\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.163\n  90 Percent confidence interval - lower                     0.148\n  90 Percent confidence interval - upper                     0.179\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.049       0.049\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Nfactor =~                                                            \n    N1                1.000                               1.284    0.818\n    N2                0.956    0.017   55.605    0.000    1.227    0.804\n    N3                0.896    0.028   32.194    0.000    1.151    0.718\n    N4                0.677    0.030   22.550    0.000    0.869    0.554\n    N5                0.632    0.029   21.614    0.000    0.811    0.501\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                2.932    0.030   98.558    0.000    2.932    1.867\n   .N2                3.508    0.029  121.478    0.000    3.508    2.300\n   .N3                3.217    0.030  106.137    0.000    3.217    2.008\n   .N4                3.185    0.030  106.886    0.000    3.185    2.030\n   .N5                2.969    0.031   96.676    0.000    2.969    1.834\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                0.818    0.047   17.284    0.000    0.818    0.331\n   .N2                0.821    0.045   18.271    0.000    0.821    0.353\n   .N3                1.242    0.051   24.204    0.000    1.242    0.484\n   .N4                1.707    0.054   31.557    0.000    1.707    0.693\n   .N5                1.962    0.056   34.977    0.000    1.962    0.749\n    Nfactor           1.649    0.064   25.915    0.000    1.000    1.000",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Dealing with missing data</span>"
    ]
  },
  {
    "objectID": "MissingDataExamples.html#multiple-imputation-with-the-amelia-and-lavaan.mi-packages",
    "href": "MissingDataExamples.html#multiple-imputation-with-the-amelia-and-lavaan.mi-packages",
    "title": "21  Dealing with missing data",
    "section": "21.5 Multiple imputation with the Amelia and lavaan.mi packages",
    "text": "21.5 Multiple imputation with the Amelia and lavaan.mi packages\nNext, I used the Amelia and lavaan.mi packages to (a) perform multiple imputation to obtain 20 data sets that contain plausible estimates of the missing values, (b) fit the confirmatory factor analysis model to each of the data sets, and (c) report the pooled results.\nThe 20 complete data sets were stored as a list in an object I labeled Ndata.mi. I stored this list in a new object I labeled imps.\n\nlibrary(Amelia)\nlibrary(lavaan.mi)\n\nset.seed(12345)\nNdata.mi      &lt;- amelia(Ndata, \n                        m = 20)\n\nimps          &lt;- Ndata.mi$imputations\n\n\n21.5.1 Fit the model to the imputed data sets and pool the results\nI fitted the confirmatory factor analysis model to the imputed data sets in imps using the cfa.mi() function of the lavaan.mi package. The results are stored in fit.Nmodel.mi. We access the results by asking for a summary. Note that the parameter estimates are very similar to those obtained with listwise deletion and full information maximum likelihood estimation. Also, the standard errors of the parameters are very similar across the three analyses.\n\nfit.Nmodel.mi &lt;- cfa.mi(Nmodel, \n                        data      = imps,\n                        estimator = \"MLR\")\n\nsummary(fit.Nmodel.mi, \n        fit.measures = TRUE,\n        standardized = TRUE)\n\nlavaan.mi object fit to 20 imputed data sets using:\n - lavaan    (0.6-19)\n - lavaan.mi (0.1-0.0030)\nSee class?lavaan.mi help page for available methods. \n\nConvergence information:\nThe model converged on 20 imputed data sets.\nStandard errors were available for all imputations.\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                          2800\n\nModel Test User Model:\n\n                                                    Standard      Scaled\n  Test statistic                                     375.374     326.698\n  Degrees of freedom                                       5           5\n  P-value                                              0.000       0.000\n  Average scaling correction factor                                1.149\n  Pooling method                                          D4            \n    Pooled statistic                              \"standard\"            \n    \"yuan.bentler.mplus\" correction applied            AFTER     pooling\n\nModel Test Baseline Model:\n\n  Test statistic                              4849.800    3598.394\n  Degrees of freedom                                10          10\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.348\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.923       0.910\n  Tucker-Lewis Index (TLI)                       0.847       0.821\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.924\n  Robust Tucker-Lewis Index (TLI)                            0.847\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -23961.856  -23961.856\n  Scaling correction factor                                  1.005\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -23770.632  -23770.632\n  Scaling correction factor                                  1.054\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               47943.711   47943.711\n  Bayesian (BIC)                             48003.085   48003.085\n  Sample-size adjusted Bayesian (SABIC)      47971.312   47971.312\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.163       0.152\n  90 Percent confidence interval - lower         0.149       0.139\n  90 Percent confidence interval - upper         0.177       0.165\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.162\n  90 Percent confidence interval - lower                     0.148\n  90 Percent confidence interval - upper                     0.178\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.057       0.057\n\nParameter Estimates:\n\n  Standard errors                                    Sandwich\n  Information bread                                  Observed\n  Observed information based on                       Hessian\n                                                             \n  Pooled across imputations              Rubin's (1987) rules\n  Augment within-imputation variance     Scale by average RIV\n  Wald test for pooled parameters          t(df) distribution\n\n  Pooled t statistics with df &gt;= 1000 are displayed with\n  df = Inf(inity) to save space. Although the t distribution\n  with large df closely approximates a standard normal\n  distribution, exact df for reporting these t tests can be\n  obtained from parameterEstimates.mi() \n\n\nLatent Variables:\n                   Estimate  Std.Err  t-value       df  P(&gt;|t|)   Std.lv\n  Nfactor =~                                                            \n    N1                1.000                                        1.285\n    N2                0.955    0.017   55.650      Inf    0.000    1.227\n    N3                0.896    0.028   32.023      Inf    0.000    1.151\n    N4                0.677    0.030   22.524      Inf    0.000    0.870\n    N5                0.631    0.029   21.570      Inf    0.000    0.811\n  Std.all\n         \n    0.818\n    0.804\n    0.719\n    0.554\n    0.501\n\nVariances:\n                   Estimate  Std.Err  t-value       df  P(&gt;|t|)   Std.lv\n   .N1                0.818    0.047   17.270      Inf    0.000    0.818\n   .N2                0.821    0.045   18.257      Inf    0.000    0.821\n   .N3                1.239    0.051   24.060      Inf    0.000    1.239\n   .N4                1.707    0.054   31.524      Inf    0.000    1.707\n   .N5                1.961    0.056   34.947      Inf    0.000    1.961\n    Nfactor           1.650    0.064   25.835      Inf    0.000    1.000\n  Std.all\n    0.331\n    0.353\n    0.483\n    0.693\n    0.749\n    1.000",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Dealing with missing data</span>"
    ]
  },
  {
    "objectID": "MissingDataExamples.html#multiple-imputation-with-ordinal-variables",
    "href": "MissingDataExamples.html#multiple-imputation-with-ordinal-variables",
    "title": "21  Dealing with missing data",
    "section": "21.6 Multiple imputation with ordinal variables",
    "text": "21.6 Multiple imputation with ordinal variables\nThe items of the Neuroticism scale are strictly ordinal with six ordered categories. The observed values should be 1, 2, 3, 4, 5 or 6. If we don’t instruct amelia() to treat the items as ordinal, the function will treat them as continuous variables and the imputed values will contain decimals (which is not what we want). The imputed values may even extend beyond the range of the original six-point rating scale (which is really not what we want). Here I impute the missing data with the amelia() function, but now with the added argument that the items are ordinal. The imputed values will now be integers rather than decimals, which is what we want. The results are almost indistinguishable from the previous results.\n\nlibrary(Amelia)\nlibrary(lavaan.mi)\n\nset.seed(12345)\nNdata.mi2      &lt;- amelia(Ndata, \n                         m    = 20, \n                         ords = c(\"N1\", \"N2\", \"N3\", \"N4\", \"N5\"))\n\nimps2          &lt;- Ndata.mi2$imputations\n\n\nfit.Nmodel.mi2 &lt;- cfa.mi(Nmodel, \n                         data      = imps2,\n                         estimator = \"MLR\")\n\nsummary(fit.Nmodel.mi2, \n        fit.measures = TRUE,\n        standardized = TRUE)\n\nlavaan.mi object fit to 20 imputed data sets using:\n - lavaan    (0.6-19)\n - lavaan.mi (0.1-0.0030)\nSee class?lavaan.mi help page for available methods. \n\nConvergence information:\nThe model converged on 20 imputed data sets.\nStandard errors were available for all imputations.\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                          2800\n\nModel Test User Model:\n\n                                                    Standard      Scaled\n  Test statistic                                     363.290     315.881\n  Degrees of freedom                                       5           5\n  P-value                                              0.000       0.000\n  Average scaling correction factor                                1.150\n  Pooling method                                          D4            \n    Pooled statistic                              \"standard\"            \n    \"yuan.bentler.mplus\" correction applied            AFTER     pooling\n\nModel Test Baseline Model:\n\n  Test statistic                              4776.413    3547.745\n  Degrees of freedom                                10          10\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.346\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.925       0.912\n  Tucker-Lewis Index (TLI)                       0.850       0.824\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.925\n  Robust Tucker-Lewis Index (TLI)                            0.850\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -23982.925  -23982.925\n  Scaling correction factor                                  1.003\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -23795.906  -23795.906\n  Scaling correction factor                                  1.052\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               47985.849   47985.849\n  Bayesian (BIC)                             48045.223   48045.223\n  Sample-size adjusted Bayesian (SABIC)      48013.449   48013.449\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.160       0.149\n  90 Percent confidence interval - lower         0.146       0.136\n  90 Percent confidence interval - upper         0.174       0.162\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.160\n  90 Percent confidence interval - lower                     0.145\n  90 Percent confidence interval - upper                     0.175\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.057       0.057\n\nParameter Estimates:\n\n  Standard errors                                    Sandwich\n  Information bread                                  Observed\n  Observed information based on                       Hessian\n                                                             \n  Pooled across imputations              Rubin's (1987) rules\n  Augment within-imputation variance     Scale by average RIV\n  Wald test for pooled parameters          t(df) distribution\n\n  Pooled t statistics with df &gt;= 1000 are displayed with\n  df = Inf(inity) to save space. Although the t distribution\n  with large df closely approximates a standard normal\n  distribution, exact df for reporting these t tests can be\n  obtained from parameterEstimates.mi() \n\n\nLatent Variables:\n                   Estimate  Std.Err  t-value       df  P(&gt;|t|)   Std.lv\n  Nfactor =~                                                            \n    N1                1.000                                        1.284\n    N2                0.955    0.017   55.326      Inf    0.000    1.226\n    N3                0.896    0.028   32.064      Inf    0.000    1.151\n    N4                0.678    0.030   22.568      Inf    0.000    0.870\n    N5                0.631    0.029   21.614      Inf    0.000    0.810\n  Std.all\n         \n    0.817\n    0.803\n    0.718\n    0.554\n    0.501\n\nVariances:\n                   Estimate  Std.Err  t-value       df  P(&gt;|t|)   Std.lv\n   .N1                0.823    0.047   17.334      Inf    0.000    0.823\n   .N2                0.827    0.045   18.370      Inf    0.000    0.827\n   .N3                1.243    0.051   24.148      Inf    0.000    1.243\n   .N4                1.712    0.054   31.665      Inf    0.000    1.712\n   .N5                1.963    0.056   35.022      Inf    0.000    1.963\n    Nfactor           1.650    0.064   25.816      Inf    0.000    1.000\n  Std.all\n    0.333\n    0.355\n    0.484\n    0.693\n    0.749\n    1.000",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Dealing with missing data</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "24  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "answers.html",
    "href": "answers.html",
    "title": "25  Answers",
    "section": "",
    "text": "25.1 Chapter 2: Packages\nNote that I placed a hash at the beginning of each line that would install the respective packages. This was done to prevent R from installing the packages over and over again. You only need to install a package once and you could do that by removing the hash at the start of the line .\n### Install janitor package from CRAN\n#install.packages(\"janitor\"  , repos = \"https://cloud.r-project.org/\")\n\n#install.packages(\"EFAtools\"  , repos = \"https://cloud.r-project.org/\")\n\n# Install FAtools package from Github\nlibrary(devtools)\n#install_github(\"mattkcole/FAtools\")\n\n## You can go to the actual Github page of the package (https://github.com/xmc2/FAtools) and scroll down to the \"Read me\" section. Package developers will usually have download instructions in this section, which you can copy and paste in an Rstudio script window.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Answers</span>"
    ]
  },
  {
    "objectID": "answers.html#chapter-3-vectors-and-data-frames",
    "href": "answers.html#chapter-3-vectors-and-data-frames",
    "title": "25  Answers",
    "section": "25.2 Chapter 3: Vectors and data frames",
    "text": "25.2 Chapter 3: Vectors and data frames\n\nlibrary(dplyr)\nlibrary(gt)\n\n### Create the five vectors\narea         &lt;- c(\"North\", \"North\", \"West\", \"West\", \"South\", \"South\")\nage          &lt;- c(42, 35, 38, 50, 56, 55)\nsex          &lt;- c(1, 2, 1, 2, 1, 2)\nrank         &lt;- c(1, 1, 2, 3, 4, 5)\nsatisfaction &lt;- c(7, 5, 2, 9, 7, 6)\n\n\n## Combine the vectors into a data frame\nmydata &lt;- data.frame(area, age, sex, rank, satisfaction)\n\n\n## Store area, sex, and rank as factors\n\nmydata$area &lt;- factor(mydata$area)\n\nmydata$sex  &lt;- factor(mydata$sex,\n                      levels = c(1, 2),\n                      labels = c(\"male\", \"female\"))\n\nmydata$rank &lt;- factor(mydata$rank,\n                      levels = c(1, 2, 3, 4, 5),\n                      labels = c(\"Junior\",\n                                 \"Middle\",\n                                 \"Upper\",\n                                 \"Top\",\n                                 \"Executive\"))\n\n\n## Check the structure of the data frame\nstr(mydata)\n\n## Print the data frame as a table\nmydata %&gt;% \n  gt()",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Answers</span>"
    ]
  }
]