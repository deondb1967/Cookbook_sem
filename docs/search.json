[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Structural equation modeling with lavaan in R: Examples and excercises",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit: https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html",
    "href": "ReadingStoringData.html",
    "title": "2  Reading and storing data in R",
    "section": "",
    "text": "2.1 Setting the working directory\nThe first step in any data analysis session with R is to set the working directory. The working directory is a folder that contains your data and any objects and files that you create and store while working with R.\nYou should create a folder that will serve as your working directory on your computer or on an external drive. It is strongly recommended to keep the name of this folder short and straightforward.\nYou can set the working directory via the Rstudio menu: Session/Set Working Directory/Choose Directory. R uses the setwd() function to set the working directory. The entire command, which includes the path to the working directory will be printed in the console window. You should copy this command and then paste it into the first line of the script window. The next time that you start R and want to set the working directory, you can highlight the first row of the script and run it.\nIn the example below I first used the menus to set the working directory to “C:/myR/Masters2021”. Next, I copied the line of code in the console window and pasted it into the first line of the script window. By running the line of code in the script window I can set the working directory in future sessions without having to return to the menu.\nI also included the getwd() function, which is used to see the active working directory. Users sometimes change the working directory during an R session and getwd() can be used at any time to remind you of the current working directory.\n#setwd(\"C:/Users/deondb/OneDrive - Stellenbosch University/myR/Masters2021\")\n#getwd()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html#reading-the-data-and-storing-it-as-an-object",
    "href": "ReadingStoringData.html#reading-the-data-and-storing-it-as-an-object",
    "title": "2  Reading and storing data in R",
    "section": "2.2 Reading the data and storing it as an object",
    "text": "2.2 Reading the data and storing it as an object\nData will typically be captured in a spreadsheet programme like Excel. One of the safest ways to get data into R is to first–in the working directory folder–store the Excel file as a comma separated values file (.csv), second to read the csv file in R, and third to store the data as an object in R. In R the stored data is referred to as a data frame*.\nWe accomplish the second and third steps jointly by using the read.csv() function. In the example we read the data and then store it to an object (a data frame) with the name “mydata”. Note that we can give the data frame any name we want (but the name should not start with a number and it should not contain any special characters such as @, #, $, %, !, etc). The name also should not contain spaces. Two words can be used in a name if they are separated by an underscore or by a full stop (e.g. “my.data” or “my_data”) It is good practice to keep the name short and straightforward, but as informative as possible.\n\nmydata &lt;- read.csv(\"SA_Swiss.csv\")\n\nOn some machines Excel uses a semi-colon rather than a comma to separate values. In such cases you should use the read.csv2() function rather than the read.csv() function.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html#check-that-the-data-has-been-properly-read",
    "href": "ReadingStoringData.html#check-that-the-data-has-been-properly-read",
    "title": "2  Reading and storing data in R",
    "section": "2.3 Check that the data has been properly read",
    "text": "2.3 Check that the data has been properly read\nYou can view the contents of the data frame with the View() function, which will open a spreadsheet in the script window. Scrolling through the spreadsheet will reveal if the data were properly read.\n\nView(mydata)\n\nYou can also request that R print the first few lines of the data frame by using the head() function.\n\nhead(mydata)\n\n  GWS1 GWS2 GWS3 GWS4 GWS5 GWS6 GWS7 GWS8 GWS9 UWES1 UWES2 UWES3 UWES4 UWES5\n1    2    1    2    1    1    1    1    1    1     5     5     5     5     5\n2    3    3    2    3    3    4    4    3    3     4     4     4     4     4\n3    2    2    2    2    2    3    4    2    3     5     5     6     6     5\n4    2    2    2    3    3    3    1    3    3     4     4     4     4     3\n5    1    1    1    2    2    1    1    1    1     6     6     6     6     5\n6    2    2    2    2    2    3    3    2    1     3     3     4     4     4\n  UWES6 UWES7 UWES8 UWES9 MBI1 MBI2 MBI3 MBI4 MBI5 MBI6 MBI7 MBI8 MBI9 MBI10\n1     5     5     4     4    1    1    1    1    1    1    1    1    1     1\n2     5     5     5     5    2    2    2    1    1    3    3    1    2     2\n3     6     6     5     4    1    1    2    1    1    1    1    1    1     1\n4     4     4     4     4    1    1    1    1    1    2    1    1    1     1\n5     5     6     6     3    1    1    1    1    1    1    1    1    1     1\n6     4     4     4     4    1    1    3    2    1    2    2    2    3     2\n  MBI11 MBI12 MBI13 MBI14 MBI15 MBI16 GHQ1 GHQ2 GHQ3 GHQ4 GHQ5 GHQ6 GHQ7 GHQ8\n1     4     3     2     1     1     1    1    1    2    2    1    4    1    1\n2     3     3     3     3     3     3    1    2    1    1    2    1    2    2\n3     3     3     3     2     3     2    1    2    1    1    3    2    2    1\n4     4     4     4     4     4     4    1    2    2    2    2    1    2    2\n5     7     7     7     7     7     7    2    1    1    1    1    4    1    2\n6     4     3     3     3     4     4    2    1    1    1    2    1    2    2\n  GHQ9 GHQ10 GHQ11 GHQ12 Country Age Gender\n1    1     1     1     1       2  35      1\n2    2     2     1     1       2  35      0\n3    2     1     1     2       2  22      0\n4    1     1     1     2       2  38      1\n5    1     1     1     1       2  20      0\n6    1     1     1     1       2  25      1\n\n\nFinally, it is good practice to use the names() function to print to the console window the names of the variables in the data frame. Each row of the printed names will start with a number in square brackets and that number represents the serial position in the data frame of the first variable in that row.\n\nnames(mydata)\n\n [1] \"GWS1\"    \"GWS2\"    \"GWS3\"    \"GWS4\"    \"GWS5\"    \"GWS6\"    \"GWS7\"   \n [8] \"GWS8\"    \"GWS9\"    \"UWES1\"   \"UWES2\"   \"UWES3\"   \"UWES4\"   \"UWES5\"  \n[15] \"UWES6\"   \"UWES7\"   \"UWES8\"   \"UWES9\"   \"MBI1\"    \"MBI2\"    \"MBI3\"   \n[22] \"MBI4\"    \"MBI5\"    \"MBI6\"    \"MBI7\"    \"MBI8\"    \"MBI9\"    \"MBI10\"  \n[29] \"MBI11\"   \"MBI12\"   \"MBI13\"   \"MBI14\"   \"MBI15\"   \"MBI16\"   \"GHQ1\"   \n[36] \"GHQ2\"    \"GHQ3\"    \"GHQ4\"    \"GHQ5\"    \"GHQ6\"    \"GHQ7\"    \"GHQ8\"   \n[43] \"GHQ9\"    \"GHQ10\"   \"GHQ11\"   \"GHQ12\"   \"Country\" \"Age\"     \"Gender\" \n\ndata.frame(names(mydata))\n\n   names.mydata.\n1           GWS1\n2           GWS2\n3           GWS3\n4           GWS4\n5           GWS5\n6           GWS6\n7           GWS7\n8           GWS8\n9           GWS9\n10         UWES1\n11         UWES2\n12         UWES3\n13         UWES4\n14         UWES5\n15         UWES6\n16         UWES7\n17         UWES8\n18         UWES9\n19          MBI1\n20          MBI2\n21          MBI3\n22          MBI4\n23          MBI5\n24          MBI6\n25          MBI7\n26          MBI8\n27          MBI9\n28         MBI10\n29         MBI11\n30         MBI12\n31         MBI13\n32         MBI14\n33         MBI15\n34         MBI16\n35          GHQ1\n36          GHQ2\n37          GHQ3\n38          GHQ4\n39          GHQ5\n40          GHQ6\n41          GHQ7\n42          GHQ8\n43          GHQ9\n44         GHQ10\n45         GHQ11\n46         GHQ12\n47       Country\n48           Age\n49        Gender",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html#missing-data",
    "href": "ReadingStoringData.html#missing-data",
    "title": "2  Reading and storing data in R",
    "section": "2.4 Missing data",
    "text": "2.4 Missing data\nIt is relatively common to find missing data in a data set. In Excel, missing data can be represented by an empty cell, by the letters NA, or by a special code such as -999. In the first two cases R will recognise the empty cells or cells containing NAs as missing data. In the third case it is necessary to indicate to read.csv() that the special code represents missing data. Note that in an R data frame missing data will always be represented by NA (an abbreviation for “not available”).\n\nSA_Swiss &lt;- read.csv(\"SA_Swiss.csv\", na.strings=\"-999\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html#reading-an-excel-file",
    "href": "ReadingStoringData.html#reading-an-excel-file",
    "title": "2  Reading and storing data in R",
    "section": "2.5 Reading an Excel file",
    "text": "2.5 Reading an Excel file\nIt is possible to directly read an Excel file without first storing it in csv format. This requires that the readxl package must be installed and activated. Note how a special code such as -999 can be used to indicate missing data. Note that empty cells or NA can also be used to indicate missing data in the Excel file, in which case the read_excel() function will automatically recognise the missing data.\n\n#library(readxl)\n#mydata &lt;- read_excel(\"SA_Swiss.xlsx\", na = \"-999\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html#reading-an-spss-file",
    "href": "ReadingStoringData.html#reading-an-spss-file",
    "title": "2  Reading and storing data in R",
    "section": "2.6 Reading an SPSS file",
    "text": "2.6 Reading an SPSS file\nIt is possible to read an SPSS data file without first storing it in csv format. This requires that the haven package must be installed and activated. The read_spss() function will automatically recognise missing data in an SPSS file, on condition that special codes have been assigned missing data status in the SPSS file.\n\n#library(haven)\n#mydata &lt;- read_spss(\"SA_Swiss.sav\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "ReadingStoringData.html#reading-data-from-the-clipboard",
    "href": "ReadingStoringData.html#reading-data-from-the-clipboard",
    "title": "2  Reading and storing data in R",
    "section": "2.7 Reading data from the clipboard",
    "text": "2.7 Reading data from the clipboard\nAt times it might be convenient to highlight and copy data in Excel and then use the read.clipboard() function from the psychTools package to read and store the copied data as a data frame. Whereas this is easy and quick to do it is not recommended for general practice, because the saved code will not explicitly state where the data were obtained from (other than the clipboard). In the example code below I copied some data in Excel and then used the read.clipboard() function to store it as a data frame called “tempdata”. To execute the code users will have to first copy data in Excel, then remove the hash tags at the beginning of the lines, and then run the code.\n\n#library(psychTools)\n#tempdata &lt;- read.clipboard()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reading and storing data in *R*</span>"
    ]
  },
  {
    "objectID": "DataManagement.html",
    "href": "DataManagement.html",
    "title": "3  Data management in R",
    "section": "",
    "text": "3.1 Reading the data and storing it as a data frame\nOur first step is to read the data and to store it as a data frame. This needs to be done at the start of each session. If this had been done earlier in the session we do not need to do it again.\nmydata &lt;- read.csv(\"SA_Swiss.csv\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data management in *R*</span>"
    ]
  },
  {
    "objectID": "DataManagement.html#inspecting-the-structure-of-a-data-frame",
    "href": "DataManagement.html#inspecting-the-structure-of-a-data-frame",
    "title": "3  Data management in R",
    "section": "3.2 Inspecting the structure of a data frame",
    "text": "3.2 Inspecting the structure of a data frame\nWhen data are imported from a csv or Excel file, R will (usually) automatically classify variables that contain only numbers as numeric, whereas variables that contain text will (usually) be classified as categorical. In R, a categorical variable is referred to as a factor. There are two types of categorical variables or factors, namely nominal and ordinal.\nWe can inspect the classification of variables in a data frame with the str() function. In the example below we see that the items of the GWSS, the UWES, the MBI, and the GHQ are classified as integers, which is a type of numeric variable. We also note that Gender and Country are classified as integer (numeric) variables. The latter classification is problematic, in the sense that Gender and Country are actually categorical (nominal) variables where the numbers 0 and 1 for Gender indicate “Man” and “Woman”, respectively, and the numbers 1 and 2 for Country indicate “South Africa” and “Switzerland”, respectively.\nChecking if categorical variables were imported as factors is an important and essential step in the data analysis process. Similarly, converting variables that were incorrectly imported as numerical to factor is an essential step.\n\nstr(mydata)\n\n'data.frame':   1377 obs. of  49 variables:\n $ GWS1   : int  2 3 2 2 1 2 1 3 2 2 ...\n $ GWS2   : int  1 3 2 2 1 2 1 3 1 2 ...\n $ GWS3   : int  2 2 2 2 1 2 1 2 1 2 ...\n $ GWS4   : int  1 3 2 3 2 2 1 2 1 2 ...\n $ GWS5   : int  1 3 2 3 2 2 1 2 1 2 ...\n $ GWS6   : int  1 4 3 3 1 3 1 1 1 2 ...\n $ GWS7   : int  1 4 4 1 1 3 2 2 1 2 ...\n $ GWS8   : int  1 3 2 3 1 2 1 2 1 1 ...\n $ GWS9   : int  1 3 3 3 1 1 2 1 1 1 ...\n $ UWES1  : int  5 4 5 4 6 3 3 5 5 5 ...\n $ UWES2  : int  5 4 5 4 6 3 4 5 5 5 ...\n $ UWES3  : int  5 4 6 4 6 4 6 5 4 6 ...\n $ UWES4  : int  5 4 6 4 6 4 6 5 3 6 ...\n $ UWES5  : int  5 4 5 3 5 4 5 6 5 5 ...\n $ UWES6  : int  5 5 6 4 5 4 5 4 5 6 ...\n $ UWES7  : int  5 5 6 4 6 4 6 7 5 6 ...\n $ UWES8  : int  4 5 5 4 6 4 5 5 4 7 ...\n $ UWES9  : int  4 5 4 4 3 4 5 5 4 3 ...\n $ MBI1   : int  1 2 1 1 1 1 1 1 3 1 ...\n $ MBI2   : int  1 2 1 1 1 1 1 1 2 2 ...\n $ MBI3   : int  1 2 2 1 1 3 1 5 2 6 ...\n $ MBI4   : int  1 1 1 1 1 2 1 2 2 1 ...\n $ MBI5   : int  1 1 1 1 1 1 1 1 2 1 ...\n $ MBI6   : int  1 3 1 2 1 2 2 2 1 1 ...\n $ MBI7   : int  1 3 1 1 1 2 2 3 1 1 ...\n $ MBI8   : int  1 1 1 1 1 2 1 2 1 2 ...\n $ MBI9   : int  1 2 1 1 1 3 1 2 1 2 ...\n $ MBI10  : int  1 2 1 1 1 2 1 2 1 2 ...\n $ MBI11  : int  4 3 3 4 7 4 2 2 2 2 ...\n $ MBI12  : int  3 3 3 4 7 3 2 1 2 1 ...\n $ MBI13  : int  2 3 3 4 7 3 2 1 1 1 ...\n $ MBI14  : int  1 3 2 4 7 3 3 1 1 1 ...\n $ MBI15  : int  1 3 3 4 7 4 3 2 2 2 ...\n $ MBI16  : int  1 3 2 4 7 4 2 2 1 1 ...\n $ GHQ1   : int  1 1 1 1 2 2 2 2 2 2 ...\n $ GHQ2   : int  1 2 2 2 1 1 1 1 1 1 ...\n $ GHQ3   : int  2 1 1 2 1 1 1 2 2 2 ...\n $ GHQ4   : int  2 1 1 2 1 1 2 2 2 2 ...\n $ GHQ5   : int  1 2 3 2 1 2 1 1 1 2 ...\n $ GHQ6   : int  4 1 2 1 4 1 1 1 1 1 ...\n $ GHQ7   : int  1 2 2 2 1 2 2 2 2 3 ...\n $ GHQ8   : int  1 2 1 2 2 2 2 2 2 2 ...\n $ GHQ9   : int  1 2 2 1 1 1 1 1 2 1 ...\n $ GHQ10  : int  1 2 1 1 1 1 1 1 2 1 ...\n $ GHQ11  : int  1 1 1 1 1 1 1 1 1 1 ...\n $ GHQ12  : int  1 1 2 2 1 1 2 2 2 1 ...\n $ Country: int  2 2 2 2 2 2 2 2 2 2 ...\n $ Age    : num  35 35 22 38 20 25 58 26 28 34 ...\n $ Gender : int  1 0 0 1 0 1 1 0 1 0 ...\n\n\nTo ensure proper statistical analyses, we need to convert Gender and Country from numeric variables to factors. The code in the example below instructs R that the Gender variable in the mydata data frame should be treated as a factor, and that the newly converted Gender variable should be stored with the same name as the existing one (i.e. the new variable overwrites the old variable). The second line of code is used to check whether the newly overwritten Gender variable (mydata$Gender) actually is a factor.\n\nmydata$Gender &lt;- factor(mydata$Gender)\nis.factor(mydata$Gender)\n\n[1] TRUE\n\n\nIt is good practice, but not necessary, to assign labels to the numbers of a categorical variable or factor. In the present example we could assign the label “Man” to the value of 0, and the label “Woman” to the value of 1. To assign the labels we (a) instruct R what the levels of the categorical variable are, and (b) what the labels of each level are.\n\nmydata$Gender &lt;- factor(mydata$Gender, levels = c(\"0\", \"1\"), labels = c(\"Man\", \"Woman\"))\nis.factor(mydata$Gender)\n\n[1] TRUE\n\n\nWe now do the same with the Country variable (mydata$Country).\n\nmydata$Country &lt;- factor(mydata$Country, levels = c(\"1\", \"2\"), labels = c(\"ZAR\", \"CHE\"))\nis.factor(mydata$Country)\n\n[1] TRUE\n\n\nWe can now ask for a summary of basic descriptive statistics of all the variables in the data frame by using the summary() function. This function will return for each numeric variable the minimum and maximum values, the first and third quartiles, the median, and the mean. For each categorical variable or factor the frequencies of the different levels of the variable are returned.\n\nsummary(mydata)\n\n      GWS1            GWS2            GWS3            GWS4      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  \n Median :3.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :2.601   Mean   :2.145   Mean   :2.226   Mean   :2.243  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000  \n Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :5.000  \n      GWS5            GWS6            GWS7            GWS8      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:2.000   1st Qu.:1.000  \n Median :2.000   Median :2.000   Median :3.000   Median :2.000  \n Mean   :2.041   Mean   :2.078   Mean   :2.667   Mean   :1.969  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000  \n Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :5.000  \n      GWS9           UWES1           UWES2           UWES3          UWES4     \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.00   Min.   :1.00  \n 1st Qu.:1.000   1st Qu.:4.000   1st Qu.:4.000   1st Qu.:4.00   1st Qu.:4.00  \n Median :2.000   Median :5.000   Median :5.000   Median :5.00   Median :5.00  \n Mean   :1.993   Mean   :4.698   Mean   :4.892   Mean   :5.24   Mean   :4.99  \n 3rd Qu.:3.000   3rd Qu.:6.000   3rd Qu.:6.000   3rd Qu.:6.00   3rd Qu.:6.00  \n Max.   :5.000   Max.   :7.000   Max.   :7.000   Max.   :7.00   Max.   :7.00  \n     UWES5           UWES6           UWES7           UWES8      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:4.000   1st Qu.:5.000   1st Qu.:5.000   1st Qu.:4.000  \n Median :5.000   Median :6.000   Median :6.000   Median :5.000  \n Mean   :4.808   Mean   :5.373   Mean   :5.586   Mean   :4.914  \n 3rd Qu.:6.000   3rd Qu.:6.000   3rd Qu.:7.000   3rd Qu.:6.000  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000  \n     UWES9            MBI1            MBI2            MBI3      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:4.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:2.000  \n Median :5.000   Median :2.000   Median :2.000   Median :4.000  \n Mean   :4.754   Mean   :2.549   Mean   :2.741   Mean   :3.774  \n 3rd Qu.:6.000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:5.000  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000  \n      MBI4            MBI5            MBI6            MBI7      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:3.000  \n Median :3.000   Median :2.000   Median :3.000   Median :4.000  \n Mean   :2.873   Mean   :2.603   Mean   :3.001   Mean   :3.741  \n 3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:5.000  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000  \n      MBI8            MBI9           MBI10           MBI11      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000  \n Median :3.000   Median :3.000   Median :3.000   Median :3.000  \n Mean   :3.165   Mean   :2.895   Mean   :2.914   Mean   :2.675  \n 3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:3.000  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000  \n     MBI12           MBI13           MBI14           MBI15      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:1.000   1st Qu.:2.000   1st Qu.:2.000  \n Median :3.000   Median :2.000   Median :3.000   Median :3.000  \n Mean   :2.699   Mean   :2.261   Mean   :3.103   Mean   :2.788  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000  \n     MBI16            GHQ1            GHQ2            GHQ3      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:1.000   1st Qu.:2.000  \n Median :2.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :2.466   Mean   :2.104   Mean   :1.987   Mean   :1.977  \n 3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :7.000   Max.   :4.000   Max.   :4.000   Max.   :4.000  \n      GHQ4            GHQ5            GHQ6            GHQ7      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:1.000   1st Qu.:2.000  \n Median :2.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :1.954   Mean   :2.192   Mean   :1.938   Mean   :2.133  \n 3rd Qu.:2.000   3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :4.000  \n      GHQ8            GHQ9           GHQ10           GHQ11      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  \n Median :2.000   Median :2.000   Median :1.000   Median :1.000  \n Mean   :2.004   Mean   :1.933   Mean   :1.676   Mean   :1.441  \n 3rd Qu.:2.000   3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :4.000  \n     GHQ12       Country        Age          Gender   \n Min.   :1.000   ZAR:584   Min.   :16.00   Man  :607  \n 1st Qu.:2.000   CHE:793   1st Qu.:26.00   Woman:770  \n Median :2.000             Median :33.00              \n Mean   :1.992             Mean   :36.44              \n 3rd Qu.:2.000             3rd Qu.:47.00              \n Max.   :4.000             Max.   :65.00              \n\n\n\\[ C(n, k) = \\frac{n!}{k!(n-k)!} \\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data management in *R*</span>"
    ]
  },
  {
    "objectID": "Subsetting.html",
    "href": "Subsetting.html",
    "title": "4  Subsetting a data frame",
    "section": "",
    "text": "4.1 Selecting consecutive rows by their row numbers with square brackets\nbfi[1:20, ]\n\n      A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4\n61617  2  4  3  4  4  2  3  3  4  4  3  3  3  4  4  3  4  2  2  3  3  6  3  4\n61618  2  4  5  2  5  5  4  4  3  4  1  1  6  4  3  3  3  3  5  5  4  2  4  3\n61620  5  4  5  4  4  4  5  4  2  5  2  4  4  4  5  4  5  4  2  3  4  2  5  5\n61621  4  4  6  5  5  4  4  3  5  5  5  3  4  4  4  2  5  2  4  1  3  3  4  3\n61622  2  3  3  4  5  4  4  5  3  2  2  2  5  4  5  2  3  4  4  3  3  3  4  3\n61623  6  6  5  6  5  6  6  6  1  3  2  1  6  5  6  3  5  2  2  3  4  3  5  6\n61624  2  5  5  3  5  5  4  4  2  3  4  3  4  5  5  1  2  2  1  1  5  2  5  6\n61629  4  3  1  5  1  3  2  4  2  4  3  6  4  2  1  6  3  2  6  4  3  2  4  5\n61630  4  3  6  3  3  6  6  3  4  5  5  3 NA  4  3  5  5  2  3  3  6  6  6  6\n61633  2  5  6  6  5  6  5  6  2  1  2  2  4  5  5  5  5  5  2  4  5  1  5  5\n61634  4  4  5  6  5  4  3  5  3  2  1  3  2  5  4  3  3  4  2  3  5  3  5  6\n61636  2  5  5  5  5  5  4  5  4  5  3  3  4  5  4  4  5  3  2 NA  4  6  4  5\n61637  5  5  5  6  4  5  4  3  2  2  3  3  3  2  4  1  2  2  2  2  4  2  4  5\n61639  5  5  5  6  6  4  4  4  2  1  2  2  4  6  5  1  1  1  2  1  5  3  4  4\n61640  4  5  2  2  1  5  5  5  2  2  3  4  3  6  5  2  4  2  2  3  5  2  5  5\n61643  4  3  6  6  3  5  5  5  3  5  1  1  6  6  4  4  5  4  5  5  6  6  6  3\n61650  4  6  6  2  5  4  4  4  4  4  1  2  5  5  5  4  4  4  4  5  5  1  5  6\n61651  5  5  5  4  5  5  5  5  4  3  2  2  4  6  6  6  5  5  4  4  5  1  4  5\n61653  4  4  5  4  3  5  4  5  4  6  1  2  4  5  5  5  6  5  5  2  4  2  2  4\n61654  4  4  6  5  5  1  1  1  5  6  1  1  4  5  6  5  5  5  1  1  4  1  5  3\n      O5 gender education age\n61617  3      1        NA  16\n61618  3      2        NA  18\n61620  2      2        NA  17\n61621  5      2        NA  17\n61622  3      1        NA  17\n61623  1      2         3  21\n61624  1      1        NA  18\n61629  3      1         2  19\n61630  1      1         1  19\n61633  2      2        NA  17\n61634  3      1         1  21\n61636  4      1        NA  16\n61637  2      2        NA  16\n61639  4      1        NA  16\n61640  5      1         1  17\n61643  2      1        NA  17\n61650  3      2        NA  17\n61651  4      1        NA  17\n61653  2      2        NA  16\n61654  2      2        NA  17",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#selecting-non-consecutive-rows-by-their-row-numbers-with-square-brackets",
    "href": "Subsetting.html#selecting-non-consecutive-rows-by-their-row-numbers-with-square-brackets",
    "title": "4  Subsetting a data frame",
    "section": "4.2 Selecting non-consecutive rows by their row numbers with square brackets",
    "text": "4.2 Selecting non-consecutive rows by their row numbers with square brackets\n\nbfi[c(1,3,5,7,9,11,13,15,17,19,20), ]\n\n      A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4\n61617  2  4  3  4  4  2  3  3  4  4  3  3  3  4  4  3  4  2  2  3  3  6  3  4\n61620  5  4  5  4  4  4  5  4  2  5  2  4  4  4  5  4  5  4  2  3  4  2  5  5\n61622  2  3  3  4  5  4  4  5  3  2  2  2  5  4  5  2  3  4  4  3  3  3  4  3\n61624  2  5  5  3  5  5  4  4  2  3  4  3  4  5  5  1  2  2  1  1  5  2  5  6\n61630  4  3  6  3  3  6  6  3  4  5  5  3 NA  4  3  5  5  2  3  3  6  6  6  6\n61634  4  4  5  6  5  4  3  5  3  2  1  3  2  5  4  3  3  4  2  3  5  3  5  6\n61637  5  5  5  6  4  5  4  3  2  2  3  3  3  2  4  1  2  2  2  2  4  2  4  5\n61640  4  5  2  2  1  5  5  5  2  2  3  4  3  6  5  2  4  2  2  3  5  2  5  5\n61650  4  6  6  2  5  4  4  4  4  4  1  2  5  5  5  4  4  4  4  5  5  1  5  6\n61653  4  4  5  4  3  5  4  5  4  6  1  2  4  5  5  5  6  5  5  2  4  2  2  4\n61654  4  4  6  5  5  1  1  1  5  6  1  1  4  5  6  5  5  5  1  1  4  1  5  3\n      O5 gender education age\n61617  3      1        NA  16\n61620  2      2        NA  17\n61622  3      1        NA  17\n61624  1      1        NA  18\n61630  1      1         1  19\n61634  3      1         1  21\n61637  2      2        NA  16\n61640  5      1         1  17\n61650  3      2        NA  17\n61653  2      2        NA  16\n61654  2      2        NA  17",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#selecting-rows-and-storing-the-data-as-a-new-data-frame",
    "href": "Subsetting.html#selecting-rows-and-storing-the-data-as-a-new-data-frame",
    "title": "4  Subsetting a data frame",
    "section": "4.3 Selecting rows and storing the data as a new data frame",
    "text": "4.3 Selecting rows and storing the data as a new data frame\n\nmybfi &lt;- bfi[1000:1019, ]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#selecting-a-variable-with-notation-and-its-name",
    "href": "Subsetting.html#selecting-a-variable-with-notation-and-its-name",
    "title": "4  Subsetting a data frame",
    "section": "4.4 Selecting a variable with $ notation and its name",
    "text": "4.4 Selecting a variable with $ notation and its name\n\nmybfi$A1\n\n [1] 6 2 4 1 2 2 1 1 3 1 2 3 2 1 1 3 1 2 2 4",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#selecting-a-variable-by-its-column-number",
    "href": "Subsetting.html#selecting-a-variable-by-its-column-number",
    "title": "4  Subsetting a data frame",
    "section": "4.5 Selecting a variable by its column number",
    "text": "4.5 Selecting a variable by its column number\n\nmybfi[,1] \n\n [1] 6 2 4 1 2 2 1 1 3 1 2 3 2 1 1 3 1 2 2 4",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#selecting-a-variable-by-its-name-with-square-brackets",
    "href": "Subsetting.html#selecting-a-variable-by-its-name-with-square-brackets",
    "title": "4  Subsetting a data frame",
    "section": "4.6 Selecting a variable by its name with square brackets",
    "text": "4.6 Selecting a variable by its name with square brackets\n\nmybfi[, \"A1\"]\n\n [1] 6 2 4 1 2 2 1 1 3 1 2 3 2 1 1 3 1 2 2 4",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#selecting-two-or-more-contiguous-variables-by-their-column-numbers-with-square-brackets",
    "href": "Subsetting.html#selecting-two-or-more-contiguous-variables-by-their-column-numbers-with-square-brackets",
    "title": "4  Subsetting a data frame",
    "section": "4.7 Selecting two or more contiguous variables by their column numbers with square brackets",
    "text": "4.7 Selecting two or more contiguous variables by their column numbers with square brackets\n\nmybfi[, 1:5]\n\n      A1 A2 A3 A4 A5\n63746  6  5  5  4  5\n63748  2  4  5  1  4\n63750  4  4  4  3  3\n63752  1  6  5  5  5\n63760  2  4  6  6  5\n63761  2  5  5  4  4\n63762  1  6  6  1  6\n63763  1  6  6  4  1\n63766  3  5  6  6  5\n63767  1  6  6  6  6\n63768  2  5  5  4  5\n63770  3  6  6  6  5\n63773  2  6  5  6  6\n63775  1  5  3  4  5\n63776  1 NA  5  6  6\n63778  3  5  2  2  4\n63788  1  5  5  6  5\n63789  2  4  3  6  4\n63791  2  6  4  5  4\n63792  4  6  6  6  5",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#selecting-two-or-more-non-contiguous-variables-by-their-column-numbers-with-square-brackets",
    "href": "Subsetting.html#selecting-two-or-more-non-contiguous-variables-by-their-column-numbers-with-square-brackets",
    "title": "4  Subsetting a data frame",
    "section": "4.8 Selecting two or more non-contiguous variables by their column numbers with square brackets",
    "text": "4.8 Selecting two or more non-contiguous variables by their column numbers with square brackets\n\nmybfi[, c(1,3,5)]\n\n      A1 A3 A5\n63746  6  5  5\n63748  2  5  4\n63750  4  4  3\n63752  1  5  5\n63760  2  6  5\n63761  2  5  4\n63762  1  6  6\n63763  1  6  1\n63766  3  6  5\n63767  1  6  6\n63768  2  5  5\n63770  3  6  5\n63773  2  5  6\n63775  1  3  5\n63776  1  5  6\n63778  3  2  4\n63788  1  5  5\n63789  2  3  4\n63791  2  4  4\n63792  4  6  5",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#selecting-two-or-more-variables-by-their-names-with-square-brackets",
    "href": "Subsetting.html#selecting-two-or-more-variables-by-their-names-with-square-brackets",
    "title": "4  Subsetting a data frame",
    "section": "4.9 Selecting two or more variables by their names with square brackets",
    "text": "4.9 Selecting two or more variables by their names with square brackets\n\nmybfi[, c(\"A1\", \"A3\", \"A5\")]\n\n      A1 A3 A5\n63746  6  5  5\n63748  2  5  4\n63750  4  4  3\n63752  1  5  5\n63760  2  6  5\n63761  2  5  4\n63762  1  6  6\n63763  1  6  1\n63766  3  6  5\n63767  1  6  6\n63768  2  5  5\n63770  3  6  5\n63773  2  5  6\n63775  1  3  5\n63776  1  5  6\n63778  3  2  4\n63788  1  5  5\n63789  2  3  4\n63791  2  4  4\n63792  4  6  5",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#selecting-two-or-more-variables-with-the-dplyr-package",
    "href": "Subsetting.html#selecting-two-or-more-variables-with-the-dplyr-package",
    "title": "4  Subsetting a data frame",
    "section": "4.10 Selecting two or more variables with the dplyr package",
    "text": "4.10 Selecting two or more variables with the dplyr package\n\nlibrary(dplyr)\nmybfi %&gt;% select(A1:A5)\n\n      A1 A2 A3 A4 A5\n63746  6  5  5  4  5\n63748  2  4  5  1  4\n63750  4  4  4  3  3\n63752  1  6  5  5  5\n63760  2  4  6  6  5\n63761  2  5  5  4  4\n63762  1  6  6  1  6\n63763  1  6  6  4  1\n63766  3  5  6  6  5\n63767  1  6  6  6  6\n63768  2  5  5  4  5\n63770  3  6  6  6  5\n63773  2  6  5  6  6\n63775  1  5  3  4  5\n63776  1 NA  5  6  6\n63778  3  5  2  2  4\n63788  1  5  5  6  5\n63789  2  4  3  6  4\n63791  2  6  4  5  4\n63792  4  6  6  6  5\n\n\n\nlibrary(dplyr)\nmybfi %&gt;% select(A1,A3,A5)\n\n      A1 A3 A5\n63746  6  5  5\n63748  2  5  4\n63750  4  4  3\n63752  1  5  5\n63760  2  6  5\n63761  2  5  4\n63762  1  6  6\n63763  1  6  1\n63766  3  6  5\n63767  1  6  6\n63768  2  5  5\n63770  3  6  5\n63773  2  5  6\n63775  1  3  5\n63776  1  5  6\n63778  3  2  4\n63788  1  5  5\n63789  2  3  4\n63791  2  4  4\n63792  4  6  5",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#selecting-cases-by-consecutive-row-numbers",
    "href": "Subsetting.html#selecting-cases-by-consecutive-row-numbers",
    "title": "4  Subsetting a data frame",
    "section": "4.11 Selecting cases by consecutive row numbers",
    "text": "4.11 Selecting cases by consecutive row numbers\n\nmybfi %&gt;% slice(1:10)\n\n   A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4 O5\n1   6  5  5  4  5  5  2  5  3  5  5  5  4  2  6  6  6  6  6  2  5  3  4  6  1\n2   2  4  5  1  4  3  1  5  1  4  5  4  1 NA  3  2  6  6  5  6  3  4  1  4  3\n3   4  4  4  3  3  6  4  4  2  4  4  4  3  2  4  3  5  5  4  4  4  2  4  5  3\n4   1  6  5  5  5  6  5  5  1  5  4  4  5  5  4  1  4  4  4  2  6  4  3  5  5\n5   2  4  6  6  5  4  3  3  2  3  5  3  3  4  4  4  4  4  3  2  3  4  3  3  4\n6   2  5  5  4  4  2  2  2  4  5  4  4  4  5  5  1  2  2  5  4  6  5  5  6  1\n7   1  6  6  1  6  1  1  1  6  6  1  6  6  1  4  6  6  2  6  6  6  1  6  3  1\n8   1  6  6  4  1  1  6  5  4  2  6  1  3  5  1  5  5  5  6  4  6  1  6  6  6\n9   3  5  6  6  5  5  4  5  1  1  1  1  3  6  4  1  1  1  1  2  5  1  4  4  2\n10  1  6  6  6  6  6  6  6  1  1  4  1  6  6  6  1  1  1  1  1  6  1  6  1  1\n   gender education age\n1       2         4  53\n2       1         3  27\n3       2         4  48\n4       2         4  30\n5       2         3  25\n6       2         4  35\n7       2        NA  17\n8       2         3  41\n9       2         3  41\n10      2         3  37",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#filtering-cases-by-non-consecutive-row-numbers",
    "href": "Subsetting.html#filtering-cases-by-non-consecutive-row-numbers",
    "title": "4  Subsetting a data frame",
    "section": "4.12 Filtering cases by non-consecutive row numbers",
    "text": "4.12 Filtering cases by non-consecutive row numbers\n\nmybfi %&gt;% slice(1, 3, 5, 7)\n\n  A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4 O5\n1  6  5  5  4  5  5  2  5  3  5  5  5  4  2  6  6  6  6  6  2  5  3  4  6  1\n2  4  4  4  3  3  6  4  4  2  4  4  4  3  2  4  3  5  5  4  4  4  2  4  5  3\n3  2  4  6  6  5  4  3  3  2  3  5  3  3  4  4  4  4  4  3  2  3  4  3  3  4\n4  1  6  6  1  6  1  1  1  6  6  1  6  6  1  4  6  6  2  6  6  6  1  6  3  1\n  gender education age\n1      2         4  53\n2      2         4  48\n3      2         3  25\n4      2        NA  17",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#filtering-cases-that-meet-a-certain-condition",
    "href": "Subsetting.html#filtering-cases-that-meet-a-certain-condition",
    "title": "4  Subsetting a data frame",
    "section": "4.13 Filtering cases that meet a certain condition",
    "text": "4.13 Filtering cases that meet a certain condition\n\nmybfi %&gt;% filter(gender == 1)\n\n  A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4 O5\n1  2  4  5  1  4  3  1  5  1  4  5  4  1 NA  3  2  6  6  5  6  3  4  1  4  3\n2  2  6  5  6  6  6  6  5  1  4  4  2  5  5  6  4 NA  1  1 NA  6  4  5  5  1\n3  3  5  2  2  4  5  3  6  1  5  3  5  1  4  2  1  3  6  6  6  6  4  3  6  1\n4  1  5  5  6  5  5  5  1  5  6  1  2  5  6  5  2  2  2  4  1  5  1  5  6  1\n  gender education age\n1      1         3  27\n2      1         3  53\n3      1         2  19\n4      1         1  24\n\nmybfi %&gt;% filter(education &gt; 3)\n\n  A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4 O5\n1  6  5  5  4  5  5  2  5  3  5  5  5  4  2  6  6  6  6  6  2  5  3  4  6  1\n2  4  4  4  3  3  6  4  4  2  4  4  4  3  2  4  3  5  5  4  4  4  2  4  5  3\n3  1  6  5  5  5  6  5  5  1  5  4  4  5  5  4  1  4  4  4  2  6  4  3  5  5\n4  2  5  5  4  4  2  2  2  4  5  4  4  4  5  5  1  2  2  5  4  6  5  5  6  1\n5  4  6  6  6  5  4  3  2  2  3  2  3  5  6  4  4  4  3  3  4  2  3  4  3  3\n  gender education age\n1      2         4  53\n2      2         4  48\n3      2         4  30\n4      2         4  35\n5      2         4  25",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#filtering-cases-that-dont-meet-a-certain-condition",
    "href": "Subsetting.html#filtering-cases-that-dont-meet-a-certain-condition",
    "title": "4  Subsetting a data frame",
    "section": "4.14 Filtering cases that don’t meet a certain condition",
    "text": "4.14 Filtering cases that don’t meet a certain condition\n\nmybfi %&gt;% filter(!gender == 1)\n\n   A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4 O5\n1   6  5  5  4  5  5  2  5  3  5  5  5  4  2  6  6  6  6  6  2  5  3  4  6  1\n2   4  4  4  3  3  6  4  4  2  4  4  4  3  2  4  3  5  5  4  4  4  2  4  5  3\n3   1  6  5  5  5  6  5  5  1  5  4  4  5  5  4  1  4  4  4  2  6  4  3  5  5\n4   2  4  6  6  5  4  3  3  2  3  5  3  3  4  4  4  4  4  3  2  3  4  3  3  4\n5   2  5  5  4  4  2  2  2  4  5  4  4  4  5  5  1  2  2  5  4  6  5  5  6  1\n6   1  6  6  1  6  1  1  1  6  6  1  6  6  1  4  6  6  2  6  6  6  1  6  3  1\n7   1  6  6  4  1  1  6  5  4  2  6  1  3  5  1  5  5  5  6  4  6  1  6  6  6\n8   3  5  6  6  5  5  4  5  1  1  1  1  3  6  4  1  1  1  1  2  5  1  4  4  2\n9   1  6  6  6  6  6  6  6  1  1  4  1  6  6  6  1  1  1  1  1  6  1  6  1  1\n10  2  5  5  4  5  5  4  5  2  3  5  2  4  5  6  4  2  4  3  2  4  2  4  4  4\n11  3  6  6  6  5  5  5  3  2  4  1  2  5  5  5  4  4  4  3  2  4  2  5  6  1\n12  1  5  3  4  5  5  4  4  1  2  5  4  2  5  2  1  2  2  1  2  6  1  6  5  2\n13  1 NA  5  6  6  5  6 NA  2  6  1  1  4  6  4  3  5  3 NA  1  4  4  4  6  5\n14  2  4  3  6  4  4  5  5  1  2  6  5  4  4  4  3  3  4  3  5  5  4  3  5  4\n15  2  6  4  5  4  5  5  5  1  1  2  2  4  3  6  1  2  1  2  1  5  2  6  4  2\n16  4  6  6  6  5  4  3  2  2  3  2  3  5  6  4  4  4  3  3  4  2  3  4  3  3\n   gender education age\n1       2         4  53\n2       2         4  48\n3       2         4  30\n4       2         3  25\n5       2         4  35\n6       2        NA  17\n7       2         3  41\n8       2         3  41\n9       2         3  37\n10      2         3  60\n11      2         3  23\n12      2         2  19\n13      2         3  35\n14      2         3  21\n15      2         3  29\n16      2         4  25\n\nmybfi %&gt;% filter(!education &gt; 3)\n\n   A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4 O5\n1   2  4  5  1  4  3  1  5  1  4  5  4  1 NA  3  2  6  6  5  6  3  4  1  4  3\n2   2  4  6  6  5  4  3  3  2  3  5  3  3  4  4  4  4  4  3  2  3  4  3  3  4\n3   1  6  6  4  1  1  6  5  4  2  6  1  3  5  1  5  5  5  6  4  6  1  6  6  6\n4   3  5  6  6  5  5  4  5  1  1  1  1  3  6  4  1  1  1  1  2  5  1  4  4  2\n5   1  6  6  6  6  6  6  6  1  1  4  1  6  6  6  1  1  1  1  1  6  1  6  1  1\n6   2  5  5  4  5  5  4  5  2  3  5  2  4  5  6  4  2  4  3  2  4  2  4  4  4\n7   3  6  6  6  5  5  5  3  2  4  1  2  5  5  5  4  4  4  3  2  4  2  5  6  1\n8   2  6  5  6  6  6  6  5  1  4  4  2  5  5  6  4 NA  1  1 NA  6  4  5  5  1\n9   1  5  3  4  5  5  4  4  1  2  5  4  2  5  2  1  2  2  1  2  6  1  6  5  2\n10  1 NA  5  6  6  5  6 NA  2  6  1  1  4  6  4  3  5  3 NA  1  4  4  4  6  5\n11  3  5  2  2  4  5  3  6  1  5  3  5  1  4  2  1  3  6  6  6  6  4  3  6  1\n12  1  5  5  6  5  5  5  1  5  6  1  2  5  6  5  2  2  2  4  1  5  1  5  6  1\n13  2  4  3  6  4  4  5  5  1  2  6  5  4  4  4  3  3  4  3  5  5  4  3  5  4\n14  2  6  4  5  4  5  5  5  1  1  2  2  4  3  6  1  2  1  2  1  5  2  6  4  2\n   gender education age\n1       1         3  27\n2       2         3  25\n3       2         3  41\n4       2         3  41\n5       2         3  37\n6       2         3  60\n7       2         3  23\n8       1         3  53\n9       2         2  19\n10      2         3  35\n11      1         2  19\n12      1         1  24\n13      2         3  21\n14      2         3  29",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#filtering-cases-on-the-basis-of-two-or-more-conditions",
    "href": "Subsetting.html#filtering-cases-on-the-basis-of-two-or-more-conditions",
    "title": "4  Subsetting a data frame",
    "section": "4.15 Filtering cases on the basis of two or more conditions",
    "text": "4.15 Filtering cases on the basis of two or more conditions\n\nmybfi %&gt;% filter(gender == 2 & education &gt; 3)\n\n  A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4 O5\n1  6  5  5  4  5  5  2  5  3  5  5  5  4  2  6  6  6  6  6  2  5  3  4  6  1\n2  4  4  4  3  3  6  4  4  2  4  4  4  3  2  4  3  5  5  4  4  4  2  4  5  3\n3  1  6  5  5  5  6  5  5  1  5  4  4  5  5  4  1  4  4  4  2  6  4  3  5  5\n4  2  5  5  4  4  2  2  2  4  5  4  4  4  5  5  1  2  2  5  4  6  5  5  6  1\n5  4  6  6  6  5  4  3  2  2  3  2  3  5  6  4  4  4  3  3  4  2  3  4  3  3\n  gender education age\n1      2         4  53\n2      2         4  48\n3      2         4  30\n4      2         4  35\n5      2         4  25\n\nmybfi %&gt;% filter(gender == 2 | education &gt; 2)\n\n   A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4 O5\n1   6  5  5  4  5  5  2  5  3  5  5  5  4  2  6  6  6  6  6  2  5  3  4  6  1\n2   2  4  5  1  4  3  1  5  1  4  5  4  1 NA  3  2  6  6  5  6  3  4  1  4  3\n3   4  4  4  3  3  6  4  4  2  4  4  4  3  2  4  3  5  5  4  4  4  2  4  5  3\n4   1  6  5  5  5  6  5  5  1  5  4  4  5  5  4  1  4  4  4  2  6  4  3  5  5\n5   2  4  6  6  5  4  3  3  2  3  5  3  3  4  4  4  4  4  3  2  3  4  3  3  4\n6   2  5  5  4  4  2  2  2  4  5  4  4  4  5  5  1  2  2  5  4  6  5  5  6  1\n7   1  6  6  1  6  1  1  1  6  6  1  6  6  1  4  6  6  2  6  6  6  1  6  3  1\n8   1  6  6  4  1  1  6  5  4  2  6  1  3  5  1  5  5  5  6  4  6  1  6  6  6\n9   3  5  6  6  5  5  4  5  1  1  1  1  3  6  4  1  1  1  1  2  5  1  4  4  2\n10  1  6  6  6  6  6  6  6  1  1  4  1  6  6  6  1  1  1  1  1  6  1  6  1  1\n11  2  5  5  4  5  5  4  5  2  3  5  2  4  5  6  4  2  4  3  2  4  2  4  4  4\n12  3  6  6  6  5  5  5  3  2  4  1  2  5  5  5  4  4  4  3  2  4  2  5  6  1\n13  2  6  5  6  6  6  6  5  1  4  4  2  5  5  6  4 NA  1  1 NA  6  4  5  5  1\n14  1  5  3  4  5  5  4  4  1  2  5  4  2  5  2  1  2  2  1  2  6  1  6  5  2\n15  1 NA  5  6  6  5  6 NA  2  6  1  1  4  6  4  3  5  3 NA  1  4  4  4  6  5\n16  2  4  3  6  4  4  5  5  1  2  6  5  4  4  4  3  3  4  3  5  5  4  3  5  4\n17  2  6  4  5  4  5  5  5  1  1  2  2  4  3  6  1  2  1  2  1  5  2  6  4  2\n18  4  6  6  6  5  4  3  2  2  3  2  3  5  6  4  4  4  3  3  4  2  3  4  3  3\n   gender education age\n1       2         4  53\n2       1         3  27\n3       2         4  48\n4       2         4  30\n5       2         3  25\n6       2         4  35\n7       2        NA  17\n8       2         3  41\n9       2         3  41\n10      2         3  37\n11      2         3  60\n12      2         3  23\n13      1         3  53\n14      2         2  19\n15      2         3  35\n16      2         3  21\n17      2         3  29\n18      2         4  25",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#combining-select-and-slice",
    "href": "Subsetting.html#combining-select-and-slice",
    "title": "4  Subsetting a data frame",
    "section": "4.16 Combining select and slice",
    "text": "4.16 Combining select and slice\n\nmybfi %&gt;%\n  select(A1:5) %&gt;% \n  slice(1,3,5)\n\n  A1 A2 A3 A4 A5\n1  6  5  5  4  5\n2  4  4  4  3  3\n3  2  4  6  6  5",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#combing-select-and-filter",
    "href": "Subsetting.html#combing-select-and-filter",
    "title": "4  Subsetting a data frame",
    "section": "4.17 Combing select and filter",
    "text": "4.17 Combing select and filter\n\nmybfi %&gt;%\n  filter(gender == 2) %&gt;% \n  slice(1:10) %&gt;% \n  select(A1:A5)\n\n   A1 A2 A3 A4 A5\n1   6  5  5  4  5\n2   4  4  4  3  3\n3   1  6  5  5  5\n4   2  4  6  6  5\n5   2  5  5  4  4\n6   1  6  6  1  6\n7   1  6  6  4  1\n8   3  5  6  6  5\n9   1  6  6  6  6\n10  2  5  5  4  5",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#randomly-select-rows-from-a-data-frame-without-replacement",
    "href": "Subsetting.html#randomly-select-rows-from-a-data-frame-without-replacement",
    "title": "4  Subsetting a data frame",
    "section": "4.18 Randomly select rows from a data frame without replacement",
    "text": "4.18 Randomly select rows from a data frame without replacement\n\nmybfi %&gt;% \n  sample_n(10)\n\n   A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4 O5\n1   2  5  5  4  5  5  4  5  2  3  5  2  4  5  6  4  2  4  3  2  4  2  4  4  4\n2   2  4  6  6  5  4  3  3  2  3  5  3  3  4  4  4  4  4  3  2  3  4  3  3  4\n3   2  6  4  5  4  5  5  5  1  1  2  2  4  3  6  1  2  1  2  1  5  2  6  4  2\n4   4  6  6  6  5  4  3  2  2  3  2  3  5  6  4  4  4  3  3  4  2  3  4  3  3\n5   1  6  6  1  6  1  1  1  6  6  1  6  6  1  4  6  6  2  6  6  6  1  6  3  1\n6   2  5  5  4  4  2  2  2  4  5  4  4  4  5  5  1  2  2  5  4  6  5  5  6  1\n7   1  5  3  4  5  5  4  4  1  2  5  4  2  5  2  1  2  2  1  2  6  1  6  5  2\n8   2  4  5  1  4  3  1  5  1  4  5  4  1 NA  3  2  6  6  5  6  3  4  1  4  3\n9   4  4  4  3  3  6  4  4  2  4  4  4  3  2  4  3  5  5  4  4  4  2  4  5  3\n10  3  5  6  6  5  5  4  5  1  1  1  1  3  6  4  1  1  1  1  2  5  1  4  4  2\n   gender education age\n1       2         3  60\n2       2         3  25\n3       2         3  29\n4       2         4  25\n5       2        NA  17\n6       2         4  35\n7       2         2  19\n8       1         3  27\n9       2         4  48\n10      2         3  41",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#randomly-select-rows-from-a-data-frame-with-replacement",
    "href": "Subsetting.html#randomly-select-rows-from-a-data-frame-with-replacement",
    "title": "4  Subsetting a data frame",
    "section": "4.19 Randomly select rows from a data frame with replacement",
    "text": "4.19 Randomly select rows from a data frame with replacement\n\nmybfi %&gt;% \n  sample_n(10, replace = TRUE)\n\n   A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4 O5\n1   1  6  6  1  6  1  1  1  6  6  1  6  6  1  4  6  6  2  6  6  6  1  6  3  1\n2   2  4  5  1  4  3  1  5  1  4  5  4  1 NA  3  2  6  6  5  6  3  4  1  4  3\n3   2  4  6  6  5  4  3  3  2  3  5  3  3  4  4  4  4  4  3  2  3  4  3  3  4\n4   4  4  4  3  3  6  4  4  2  4  4  4  3  2  4  3  5  5  4  4  4  2  4  5  3\n5   2  6  5  6  6  6  6  5  1  4  4  2  5  5  6  4 NA  1  1 NA  6  4  5  5  1\n6   1  6  5  5  5  6  5  5  1  5  4  4  5  5  4  1  4  4  4  2  6  4  3  5  5\n7   2  5  5  4  5  5  4  5  2  3  5  2  4  5  6  4  2  4  3  2  4  2  4  4  4\n8   4  4  4  3  3  6  4  4  2  4  4  4  3  2  4  3  5  5  4  4  4  2  4  5  3\n9   3  6  6  6  5  5  5  3  2  4  1  2  5  5  5  4  4  4  3  2  4  2  5  6  1\n10  4  6  6  6  5  4  3  2  2  3  2  3  5  6  4  4  4  3  3  4  2  3  4  3  3\n   gender education age\n1       2        NA  17\n2       1         3  27\n3       2         3  25\n4       2         4  48\n5       1         3  53\n6       2         4  30\n7       2         3  60\n8       2         4  48\n9       2         3  23\n10      2         4  25",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#randomly-select-a-fraction-of-rows-without-replacement",
    "href": "Subsetting.html#randomly-select-a-fraction-of-rows-without-replacement",
    "title": "4  Subsetting a data frame",
    "section": "4.20 Randomly select a fraction of rows without replacement",
    "text": "4.20 Randomly select a fraction of rows without replacement\n\nmybfi %&gt;% \n  sample_frac(.50)\n\n   A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4 O5\n1   1  5  5  6  5  5  5  1  5  6  1  2  5  6  5  2  2  2  4  1  5  1  5  6  1\n2   6  5  5  4  5  5  2  5  3  5  5  5  4  2  6  6  6  6  6  2  5  3  4  6  1\n3   4  6  6  6  5  4  3  2  2  3  2  3  5  6  4  4  4  3  3  4  2  3  4  3  3\n4   2  4  5  1  4  3  1  5  1  4  5  4  1 NA  3  2  6  6  5  6  3  4  1  4  3\n5   2  5  5  4  5  5  4  5  2  3  5  2  4  5  6  4  2  4  3  2  4  2  4  4  4\n6   2  6  5  6  6  6  6  5  1  4  4  2  5  5  6  4 NA  1  1 NA  6  4  5  5  1\n7   3  6  6  6  5  5  5  3  2  4  1  2  5  5  5  4  4  4  3  2  4  2  5  6  1\n8   2  5  5  4  4  2  2  2  4  5  4  4  4  5  5  1  2  2  5  4  6  5  5  6  1\n9   1  6  5  5  5  6  5  5  1  5  4  4  5  5  4  1  4  4  4  2  6  4  3  5  5\n10  3  5  6  6  5  5  4  5  1  1  1  1  3  6  4  1  1  1  1  2  5  1  4  4  2\n   gender education age\n1       1         1  24\n2       2         4  53\n3       2         4  25\n4       1         3  27\n5       2         3  60\n6       1         3  53\n7       2         3  23\n8       2         4  35\n9       2         4  30\n10      2         3  41",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#randomly-select-a-fraction-of-rows-with-replacement",
    "href": "Subsetting.html#randomly-select-a-fraction-of-rows-with-replacement",
    "title": "4  Subsetting a data frame",
    "section": "4.21 Randomly select a fraction of rows with replacement",
    "text": "4.21 Randomly select a fraction of rows with replacement\n\nmybfi %&gt;% \n  sample_frac(.50, replace = TRUE)\n\n   A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4 O5\n1   4  4  4  3  3  6  4  4  2  4  4  4  3  2  4  3  5  5  4  4  4  2  4  5  3\n2   6  5  5  4  5  5  2  5  3  5  5  5  4  2  6  6  6  6  6  2  5  3  4  6  1\n3   6  5  5  4  5  5  2  5  3  5  5  5  4  2  6  6  6  6  6  2  5  3  4  6  1\n4   3  5  2  2  4  5  3  6  1  5  3  5  1  4  2  1  3  6  6  6  6  4  3  6  1\n5   1  6  6  1  6  1  1  1  6  6  1  6  6  1  4  6  6  2  6  6  6  1  6  3  1\n6   2  5  5  4  4  2  2  2  4  5  4  4  4  5  5  1  2  2  5  4  6  5  5  6  1\n7   1  6  6  4  1  1  6  5  4  2  6  1  3  5  1  5  5  5  6  4  6  1  6  6  6\n8   3  5  2  2  4  5  3  6  1  5  3  5  1  4  2  1  3  6  6  6  6  4  3  6  1\n9   3  5  6  6  5  5  4  5  1  1  1  1  3  6  4  1  1  1  1  2  5  1  4  4  2\n10  1  6  6  4  1  1  6  5  4  2  6  1  3  5  1  5  5  5  6  4  6  1  6  6  6\n   gender education age\n1       2         4  48\n2       2         4  53\n3       2         4  53\n4       1         2  19\n5       2        NA  17\n6       2         4  35\n7       2         3  41\n8       1         2  19\n9       2         3  41\n10      2         3  41",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Subsetting.html#randomly-split-a-data-frame-into-two-parts",
    "href": "Subsetting.html#randomly-split-a-data-frame-into-two-parts",
    "title": "4  Subsetting a data frame",
    "section": "4.22 Randomly split a data frame into two parts",
    "text": "4.22 Randomly split a data frame into two parts\n\n## Specify exactly how many persons you want in the two groups, respectively.\n## Here we want n = 12 in the first group and n = 8 in the second.\nv       &lt;- as.vector(c(rep(TRUE, 12), rep(FALSE, 8))) \nselect  &lt;- sample(v) \nmybfi1  &lt;- mybfi[select, ] \nmybfi2  &lt;- mybfi[!select, ]\n\nnrow(mybfi1)\n\n[1] 12\n\nnrow(mybfi2)\n\n[1] 8",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Subsetting a data frame</span>"
    ]
  },
  {
    "objectID": "Spearman.html",
    "href": "Spearman.html",
    "title": "5  Single factor model",
    "section": "",
    "text": "library(psychTools)\nlibrary(lavaan)\nlibrary(flexplavaan)\n\nmod1 &lt;- '\nAgreeableness =~ A1 + A2 + A3 + A4 + A5\n'\n\nfit.mod1 &lt;- cfa(mod1, data = bfi)\nsummary(fit.mod1, standardized = TRUE, fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 33 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n                                                  Used       Total\n  Number of observations                          2709        2800\n\nModel Test User Model:\n                                                      \n  Test statistic                                86.696\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              2533.636\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.968\n  Tucker-Lewis Index (TLI)                       0.935\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -21777.580\n  Loglikelihood unrestricted model (H1)     -21734.232\n                                                      \n  Akaike (AIC)                               43575.160\n  Bayesian (BIC)                             43634.203\n  Sample-size adjusted Bayesian (SABIC)      43602.430\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.078\n  90 Percent confidence interval - lower         0.064\n  90 Percent confidence interval - upper         0.092\n  P-value H_0: RMSEA &lt;= 0.050                    0.001\n  P-value H_0: RMSEA &gt;= 0.080                    0.415\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.032\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Agreeableness =~                                                      \n    A1                1.000                               0.528    0.376\n    A2               -1.465    0.090  -16.310    0.000   -0.774   -0.658\n    A3               -1.880    0.113  -16.696    0.000   -0.994   -0.762\n    A4               -1.358    0.093  -14.626    0.000   -0.717   -0.483\n    A5               -1.497    0.093  -16.098    0.000   -0.791   -0.627\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .A1                1.693    0.048   34.915    0.000    1.693    0.858\n   .A2                0.784    0.029   27.443    0.000    0.784    0.567\n   .A3                0.714    0.035   20.314    0.000    0.714    0.420\n   .A4                1.694    0.051   33.277    0.000    1.694    0.767\n   .A5                0.965    0.033   28.977    0.000    0.965    0.607\n    Agreeableness     0.279    0.031    8.870    0.000    1.000    1.000\n\nvisualize(fit.mod1)\n\n\n\n\n\n\n\nmeasurement_plot(fit.mod1, sample = 300)\n\n\n\n\n\n\n\nresidual_plots(fit.mod1)\n\n\n\n\n\n\n\n\n\nGWS2020 &lt;- read.csv(\"C:/Users/deondb/Downloads/GWS2020.csv\")\n\nmodgws &lt;- '\nStress =~ Item1 + Item2 + Item3 + Item4 + Item5 + Item6 + Item7 + Item8 + Item9\n'\n\nfit.modgws &lt;- cfa(modgws, data = GWS2020)\nsummary(fit.modgws, standardized = TRUE, fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 22 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        18\n\n  Number of observations                          1377\n\nModel Test User Model:\n                                                      \n  Test statistic                               808.572\n  Degrees of freedom                                27\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              6506.495\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.879\n  Tucker-Lewis Index (TLI)                       0.839\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -14826.170\n  Loglikelihood unrestricted model (H1)     -14421.884\n                                                      \n  Akaike (AIC)                               29688.341\n  Bayesian (BIC)                             29782.439\n  Sample-size adjusted Bayesian (SABIC)      29725.260\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.145\n  90 Percent confidence interval - lower         0.136\n  90 Percent confidence interval - upper         0.154\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.057\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Stress =~                                                             \n    Item1             1.000                               0.810    0.762\n    Item2             1.114    0.036   31.162    0.000    0.902    0.814\n    Item3             0.963    0.035   27.841    0.000    0.780    0.737\n    Item4             0.895    0.036   24.948    0.000    0.725    0.668\n    Item5             0.763    0.031   24.824    0.000    0.618    0.665\n    Item6             0.831    0.029   28.328    0.000    0.673    0.748\n    Item7             0.813    0.035   23.238    0.000    0.659    0.627\n    Item8             0.837    0.031   27.235    0.000    0.678    0.723\n    Item9             0.771    0.032   24.225    0.000    0.624    0.651\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Item1             0.474    0.021   22.527    0.000    0.474    0.419\n   .Item2             0.414    0.020   20.962    0.000    0.414    0.337\n   .Item3             0.511    0.022   23.054    0.000    0.511    0.457\n   .Item4             0.651    0.027   24.084    0.000    0.651    0.553\n   .Item5             0.481    0.020   24.118    0.000    0.481    0.557\n   .Item6             0.356    0.016   22.826    0.000    0.356    0.440\n   .Item7             0.671    0.027   24.515    0.000    0.671    0.607\n   .Item8             0.420    0.018   23.311    0.000    0.420    0.478\n   .Item9             0.530    0.022   24.278    0.000    0.530    0.576\n    Stress            0.656    0.041   16.177    0.000    1.000    1.000\n\nvisualize(fit.modgws, subset = 1:9)\n\n\n\n\n\n\n\nmeasurement_plot(fit.modgws, sample = 300)\n\n\n\n\n\n\n\nresidual_plots(fit.modgws)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Single factor model</span>"
    ]
  },
  {
    "objectID": "Multiple_factors.html",
    "href": "Multiple_factors.html",
    "title": "6  Multiple factors",
    "section": "",
    "text": "library(lavaan)\nlibrary(flexplavaan)\nlibrary(psychTools)\n\n\nmod3 &lt;- '\nNeuroticism   =~ N1 + N2 + N3 + N4 + N5\nAgreeableness =~ A1 + A2 + A3 + A4 + A5\n\nN1 ~~ N2\n'\n\nfit.mod3 &lt;- cfa(model = mod3, \n                data  = bfi)\n\nsummary(fit.mod3, \n        standardized = TRUE, \n        fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 39 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        22\n\n                                                  Used       Total\n  Number of observations                          2618        2800\n\nModel Test User Model:\n                                                      \n  Test statistic                               394.745\n  Degrees of freedom                                33\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              7465.706\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.951\n  Tucker-Lewis Index (TLI)                       0.934\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -43229.863\n  Loglikelihood unrestricted model (H1)     -43032.491\n                                                      \n  Akaike (AIC)                               86503.726\n  Bayesian (BIC)                             86632.870\n  Sample-size adjusted Bayesian (SABIC)      86562.969\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.065\n  90 Percent confidence interval - lower         0.059\n  90 Percent confidence interval - upper         0.071\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.048\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Neuroticism =~                                                        \n    N1                1.000                               1.069    0.681\n    N2                0.940    0.024   38.841    0.000    1.005    0.658\n    N3                1.206    0.041   29.617    0.000    1.289    0.807\n    N4                0.944    0.035   26.800    0.000    1.009    0.643\n    N5                0.833    0.035   23.538    0.000    0.890    0.549\n  Agreeableness =~                                                      \n    A1                1.000                               0.542    0.387\n    A2               -1.413    0.086  -16.450    0.000   -0.765   -0.651\n    A3               -1.838    0.109  -16.933    0.000   -0.996   -0.761\n    A4               -1.343    0.091  -14.789    0.000   -0.728   -0.488\n    A5               -1.487    0.091  -16.357    0.000   -0.806   -0.638\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .N1 ~~                                                                 \n   .N2                0.621    0.040   15.696    0.000    0.621    0.471\n  Neuroticism ~~                                                        \n    Agreeableness     0.117    0.016    7.304    0.000    0.203    0.203\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                1.319    0.048   27.230    0.000    1.319    0.536\n   .N2                1.319    0.047   28.112    0.000    1.319    0.567\n   .N3                0.890    0.048   18.678    0.000    0.890    0.349\n   .N4                1.447    0.049   29.606    0.000    1.447    0.587\n   .N5                1.836    0.057   32.264    0.000    1.836    0.699\n   .A1                1.668    0.049   34.239    0.000    1.668    0.850\n   .A2                0.795    0.029   27.568    0.000    0.795    0.576\n   .A3                0.719    0.035   20.374    0.000    0.719    0.420\n   .A4                1.692    0.052   32.681    0.000    1.692    0.762\n   .A5                0.943    0.033   28.181    0.000    0.943    0.592\n    Neuroticism       1.142    0.066   17.384    0.000    1.000    1.000\n    Agreeableness     0.293    0.033    9.006    0.000    1.000    1.000\n\nvisualize(fit.mod3, \n          subset = 1:10)\n\n\n\n\n\n\n\nmeasurement_plot(fit.mod3, \n                 sample = 300)\n\n$Neuroticism\n\n\n\n\n\n\n\n\n\n\n$Agreeableness\n\n\n\n\n\n\n\n\nresidual_plots(fit.mod3)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple factors</span>"
    ]
  },
  {
    "objectID": "Modification_indices.html",
    "href": "Modification_indices.html",
    "title": "7  Modification Indices",
    "section": "",
    "text": "library(lavaan)\n\nThis is lavaan 0.6-19\nlavaan is FREE software! Please report any bugs.\n\nlibrary(psychTools)\nmod3 &lt;- '\nNeuroticism   =~ N1 + N2 + N3 + N4 + N5\nAgreeableness =~ A1 + A2 + A3 + A4 + A5\n'\n\nfit.mod3 &lt;- cfa(model = mod3, \n                data  = bfi)\n\nsummary(fit.mod3, \n        standardized = TRUE, \n        fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 36 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n                                                  Used       Total\n  Number of observations                          2618        2800\n\nModel Test User Model:\n                                                      \n  Test statistic                               694.620\n  Degrees of freedom                                34\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              7465.706\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.911\n  Tucker-Lewis Index (TLI)                       0.882\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -43379.800\n  Loglikelihood unrestricted model (H1)     -43032.491\n                                                      \n  Akaike (AIC)                               86801.601\n  Bayesian (BIC)                             86924.874\n  Sample-size adjusted Bayesian (SABIC)      86858.151\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.086\n  90 Percent confidence interval - lower         0.081\n  90 Percent confidence interval - upper         0.092\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.966\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.054\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Neuroticism =~                                                        \n    N1                1.000                               1.292    0.823\n    N2                0.949    0.023   40.791    0.000    1.226    0.803\n    N3                0.884    0.024   36.571    0.000    1.141    0.714\n    N4                0.680    0.024   27.853    0.000    0.878    0.559\n    N5                0.626    0.025   24.591    0.000    0.809    0.499\n  Agreeableness =~                                                      \n    A1                1.000                               0.545    0.389\n    A2               -1.404    0.085  -16.527    0.000   -0.765   -0.651\n    A3               -1.822    0.107  -17.018    0.000   -0.992   -0.759\n    A4               -1.338    0.090  -14.855    0.000   -0.729   -0.489\n    A5               -1.483    0.090  -16.448    0.000   -0.808   -0.640\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Neuroticism ~~                                                        \n    Agreeableness     0.155    0.019    8.049    0.000    0.220    0.220\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                0.793    0.036   22.094    0.000    0.793    0.322\n   .N2                0.826    0.035   23.877    0.000    0.826    0.355\n   .N3                1.249    0.043   29.352    0.000    1.249    0.490\n   .N4                1.694    0.051   33.284    0.000    1.694    0.687\n   .N5                1.974    0.058   34.081    0.000    1.974    0.751\n   .A1                1.665    0.049   34.211    0.000    1.665    0.849\n   .A2                0.796    0.029   27.597    0.000    0.796    0.576\n   .A3                0.725    0.035   20.614    0.000    0.725    0.424\n   .A4                1.690    0.052   32.667    0.000    1.690    0.761\n   .A5                0.939    0.033   28.108    0.000    0.939    0.590\n    Neuroticism       1.668    0.070   23.696    0.000    1.000    1.000\n    Agreeableness     0.297    0.033    9.060    0.000    1.000    1.000\n\nmodificationindices(fit.mod3)\n\n             lhs op rhs      mi    epc sepc.lv sepc.all sepc.nox\n24   Neuroticism =~  A1  19.035  0.098   0.127    0.091    0.091\n25   Neuroticism =~  A2  20.795  0.079   0.102    0.087    0.087\n26   Neuroticism =~  A3  29.798  0.104   0.135    0.103    0.103\n27   Neuroticism =~  A4   5.262 -0.053  -0.069   -0.046   -0.046\n28   Neuroticism =~  A5  50.336 -0.132  -0.171   -0.135   -0.135\n29 Agreeableness =~  N1   0.630  0.038   0.021    0.013    0.013\n30 Agreeableness =~  N2   0.295  0.025   0.014    0.009    0.009\n31 Agreeableness =~  N3  10.202 -0.168  -0.091   -0.057   -0.057\n32 Agreeableness =~  N4  25.954  0.293   0.160    0.102    0.102\n33 Agreeableness =~  N5  11.128 -0.205  -0.112   -0.069   -0.069\n34            N1 ~~  N2 345.707  0.775   0.775    0.958    0.958\n35            N1 ~~  N3  52.136 -0.272  -0.272   -0.274   -0.274\n36            N1 ~~  N4  76.839 -0.291  -0.291   -0.251   -0.251\n37            N1 ~~  N5  19.022 -0.149  -0.149   -0.119   -0.119\n38            N1 ~~  A1  24.399  0.137   0.137    0.119    0.119\n39            N1 ~~  A2   4.495 -0.044  -0.044   -0.055   -0.055\n40            N1 ~~  A3  13.106  0.079   0.079    0.104    0.104\n41            N1 ~~  A4   5.855  0.069   0.069    0.059    0.059\n42            N1 ~~  A5   5.301 -0.051  -0.051   -0.059   -0.059\n43            N2 ~~  N3  37.647 -0.220  -0.220   -0.216   -0.216\n44            N2 ~~  N4  68.849 -0.266  -0.266   -0.225   -0.225\n45            N2 ~~  N5  42.497 -0.217  -0.217   -0.170   -0.170\n46            N2 ~~  A1   3.438  0.051   0.051    0.043    0.043\n47            N2 ~~  A2  14.146  0.077   0.077    0.095    0.095\n48            N2 ~~  A3   0.861  0.020   0.020    0.026    0.026\n49            N2 ~~  A4  14.742 -0.108  -0.108   -0.091   -0.091\n50            N2 ~~  A5   3.613 -0.042  -0.042   -0.048   -0.048\n51            N3 ~~  N4 163.251  0.436   0.436    0.300    0.300\n52            N3 ~~  N5  47.254  0.247   0.247    0.157    0.157\n53            N3 ~~  A1   0.111  0.010   0.010    0.007    0.007\n54            N3 ~~  A2   0.254  0.012   0.012    0.012    0.012\n55            N3 ~~  A3   3.474  0.046   0.046    0.048    0.048\n56            N3 ~~  A4   0.614  0.025   0.025    0.017    0.017\n57            N3 ~~  A5   0.171  0.010   0.010    0.010    0.010\n58            N4 ~~  N5  86.112  0.360   0.360    0.197    0.197\n59            N4 ~~  A1  17.767 -0.146  -0.146   -0.087   -0.087\n60            N4 ~~  A2   0.251  0.013   0.013    0.011    0.011\n61            N4 ~~  A3   3.030 -0.047  -0.047   -0.043   -0.043\n62            N4 ~~  A4  20.479 -0.161  -0.161   -0.095   -0.095\n63            N4 ~~  A5  10.813 -0.092  -0.092   -0.073   -0.073\n64            N5 ~~  A1   8.114 -0.106  -0.106   -0.058   -0.058\n65            N5 ~~  A2   7.324  0.075   0.075    0.060    0.060\n66            N5 ~~  A3   5.725 -0.069  -0.069   -0.058   -0.058\n67            N5 ~~  A4   8.224  0.109   0.109    0.060    0.060\n68            N5 ~~  A5   0.301  0.016   0.016    0.012    0.012\n69            A1 ~~  A2  64.589 -0.219  -0.219   -0.190   -0.190\n70            A1 ~~  A3   6.688  0.080   0.080    0.073    0.073\n71            A1 ~~  A4   4.991  0.080   0.080    0.048    0.048\n72            A1 ~~  A5  22.962  0.140   0.140    0.112    0.112\n73            A2 ~~  A3   0.654 -0.027  -0.027   -0.036   -0.036\n74            A2 ~~  A4   3.109  0.051   0.051    0.044    0.044\n75            A2 ~~  A5  20.439 -0.124  -0.124   -0.143   -0.143\n76            A3 ~~  A4   0.417 -0.022  -0.022   -0.020   -0.020\n77            A3 ~~  A5  31.301  0.197   0.197    0.239    0.239\n78            A4 ~~  A5   0.192 -0.014  -0.014   -0.011   -0.011\n\nmymi &lt;- modificationindices(fit.mod3)\nmymi[mymi$op == \"~~\" & mymi$mi &gt; 100, ]\n\n   lhs op rhs      mi   epc sepc.lv sepc.all sepc.nox\n34  N1 ~~  N2 345.707 0.775   0.775    0.958    0.958\n51  N3 ~~  N4 163.251 0.436   0.436    0.300    0.300\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:psychTools':\n\n    recode\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nmymi %&gt;% \n  filter(op == \"~~\" & mi &gt; 100)\n\n  lhs op rhs      mi   epc sepc.lv sepc.all sepc.nox\n1  N1 ~~  N2 345.707 0.775   0.775    0.958    0.958\n2  N3 ~~  N4 163.251 0.436   0.436    0.300    0.300\n\nmymi %&gt;% \n  filter(op == \"~~\") %&gt;% \n  arrange(desc(mi))\n\n   lhs op rhs      mi    epc sepc.lv sepc.all sepc.nox\n1   N1 ~~  N2 345.707  0.775   0.775    0.958    0.958\n2   N3 ~~  N4 163.251  0.436   0.436    0.300    0.300\n3   N4 ~~  N5  86.112  0.360   0.360    0.197    0.197\n4   N1 ~~  N4  76.839 -0.291  -0.291   -0.251   -0.251\n5   N2 ~~  N4  68.849 -0.266  -0.266   -0.225   -0.225\n6   A1 ~~  A2  64.589 -0.219  -0.219   -0.190   -0.190\n7   N1 ~~  N3  52.136 -0.272  -0.272   -0.274   -0.274\n8   N3 ~~  N5  47.254  0.247   0.247    0.157    0.157\n9   N2 ~~  N5  42.497 -0.217  -0.217   -0.170   -0.170\n10  N2 ~~  N3  37.647 -0.220  -0.220   -0.216   -0.216\n11  A3 ~~  A5  31.301  0.197   0.197    0.239    0.239\n12  N1 ~~  A1  24.399  0.137   0.137    0.119    0.119\n13  A1 ~~  A5  22.962  0.140   0.140    0.112    0.112\n14  N4 ~~  A4  20.479 -0.161  -0.161   -0.095   -0.095\n15  A2 ~~  A5  20.439 -0.124  -0.124   -0.143   -0.143\n16  N1 ~~  N5  19.022 -0.149  -0.149   -0.119   -0.119\n17  N4 ~~  A1  17.767 -0.146  -0.146   -0.087   -0.087\n18  N2 ~~  A4  14.742 -0.108  -0.108   -0.091   -0.091\n19  N2 ~~  A2  14.146  0.077   0.077    0.095    0.095\n20  N1 ~~  A3  13.106  0.079   0.079    0.104    0.104\n21  N4 ~~  A5  10.813 -0.092  -0.092   -0.073   -0.073\n22  N5 ~~  A4   8.224  0.109   0.109    0.060    0.060\n23  N5 ~~  A1   8.114 -0.106  -0.106   -0.058   -0.058\n24  N5 ~~  A2   7.324  0.075   0.075    0.060    0.060\n25  A1 ~~  A3   6.688  0.080   0.080    0.073    0.073\n26  N1 ~~  A4   5.855  0.069   0.069    0.059    0.059\n27  N5 ~~  A3   5.725 -0.069  -0.069   -0.058   -0.058\n28  N1 ~~  A5   5.301 -0.051  -0.051   -0.059   -0.059\n29  A1 ~~  A4   4.991  0.080   0.080    0.048    0.048\n30  N1 ~~  A2   4.495 -0.044  -0.044   -0.055   -0.055\n31  N2 ~~  A5   3.613 -0.042  -0.042   -0.048   -0.048\n32  N3 ~~  A3   3.474  0.046   0.046    0.048    0.048\n33  N2 ~~  A1   3.438  0.051   0.051    0.043    0.043\n34  A2 ~~  A4   3.109  0.051   0.051    0.044    0.044\n35  N4 ~~  A3   3.030 -0.047  -0.047   -0.043   -0.043\n36  N2 ~~  A3   0.861  0.020   0.020    0.026    0.026\n37  A2 ~~  A3   0.654 -0.027  -0.027   -0.036   -0.036\n38  N3 ~~  A4   0.614  0.025   0.025    0.017    0.017\n39  A3 ~~  A4   0.417 -0.022  -0.022   -0.020   -0.020\n40  N5 ~~  A5   0.301  0.016   0.016    0.012    0.012\n41  N3 ~~  A2   0.254  0.012   0.012    0.012    0.012\n42  N4 ~~  A2   0.251  0.013   0.013    0.011    0.011\n43  A4 ~~  A5   0.192 -0.014  -0.014   -0.011   -0.011\n44  N3 ~~  A5   0.171  0.010   0.010    0.010    0.010\n45  N3 ~~  A1   0.111  0.010   0.010    0.007    0.007",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modification Indices</span>"
    ]
  },
  {
    "objectID": "EFA.html",
    "href": "EFA.html",
    "title": "8  Exploratory factor analysis",
    "section": "",
    "text": "library(lavaan)\nlibrary(flexplavaan)\nlibrary(psychTools)\nlibrary(psych)\n\nscree(bfi[c(1:5, 16:20)])\n\n\n\n\n\n\n\nfa.parallel(bfi[c(1:5, 16:20)])\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  3  and the number of components =  2 \n\nfa(bfi[c(1:5, 16:20)], nfactors = 2)\n\nFactor Analysis using method =  minres\nCall: fa(r = bfi[c(1:5, 16:20)], nfactors = 2)\nStandardized loadings (pattern matrix) based upon correlation matrix\n     MR1   MR2   h2   u2 com\nA1  0.07 -0.36 0.14 0.86 1.1\nA2  0.05  0.69 0.47 0.53 1.0\nA3  0.03  0.76 0.56 0.44 1.0\nA4 -0.05  0.47 0.24 0.76 1.0\nA5 -0.12  0.60 0.39 0.61 1.1\nN1  0.78 -0.03 0.61 0.39 1.0\nN2  0.76 -0.02 0.58 0.42 1.0\nN3  0.77  0.05 0.58 0.42 1.0\nN4  0.58 -0.08 0.36 0.64 1.0\nN5  0.54  0.08 0.29 0.71 1.0\n\n                       MR1  MR2\nSS loadings           2.44 1.78\nProportion Var        0.24 0.18\nCumulative Var        0.24 0.42\nProportion Explained  0.58 0.42\nCumulative Proportion 0.58 1.00\n\n With factor correlations of \n      MR1   MR2\nMR1  1.00 -0.19\nMR2 -0.19  1.00\n\nMean item complexity =  1\nTest of the hypothesis that 2 factors are sufficient.\n\ndf null model =  45  with the objective function =  2.82 with Chi Square =  7880.99\ndf of  the model are 26  and the objective function was  0.23 \n\nThe root mean square of the residuals (RMSR) is  0.04 \nThe df corrected root mean square of the residuals is  0.05 \n\nThe harmonic n.obs is  2759 with the empirical chi square  396.78  with prob &lt;  5.7e-68 \nThe total n.obs was  2800  with Likelihood Chi Square =  636.27  with prob &lt;  1.6e-117 \n\nTucker Lewis Index of factoring reliability =  0.865\nRMSEA index =  0.092  and the 90 % confidence intervals are  0.085 0.098\nBIC =  429.9\nFit based upon off diagonal values = 0.98\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.92 0.88\nMultiple R square of scores with factors          0.84 0.77\nMinimum correlation of possible factor scores     0.68 0.54\n\nfa(bfi[c(1:5, 16:20)], nfactors = 3)\n\nFactor Analysis using method =  minres\nCall: fa(r = bfi[c(1:5, 16:20)], nfactors = 3)\nStandardized loadings (pattern matrix) based upon correlation matrix\n     MR2   MR1   MR3   h2   u2 com\nA1 -0.36  0.24 -0.17 0.17 0.83 2.2\nA2  0.69 -0.04  0.09 0.47 0.53 1.0\nA3  0.75  0.06 -0.05 0.57 0.43 1.0\nA4  0.47  0.00 -0.07 0.24 0.76 1.0\nA5  0.59 -0.05 -0.10 0.40 0.60 1.1\nN1 -0.01  0.88 -0.02 0.76 0.24 1.0\nN2 -0.01  0.77  0.05 0.65 0.35 1.0\nN3  0.05  0.37  0.47 0.57 0.43 1.9\nN4 -0.07 -0.02  0.75 0.57 0.43 1.0\nN5  0.08  0.16  0.46 0.32 0.68 1.3\n\n                       MR2  MR1  MR3\nSS loadings           1.77 1.74 1.19\nProportion Var        0.18 0.17 0.12\nCumulative Var        0.18 0.35 0.47\nProportion Explained  0.38 0.37 0.25\nCumulative Proportion 0.38 0.75 1.00\n\n With factor correlations of \n      MR2   MR1   MR3\nMR2  1.00 -0.16 -0.15\nMR1 -0.16  1.00  0.63\nMR3 -0.15  0.63  1.00\n\nMean item complexity =  1.3\nTest of the hypothesis that 3 factors are sufficient.\n\ndf null model =  45  with the objective function =  2.82 with Chi Square =  7880.99\ndf of  the model are 18  and the objective function was  0.06 \n\nThe root mean square of the residuals (RMSR) is  0.02 \nThe df corrected root mean square of the residuals is  0.04 \n\nThe harmonic n.obs is  2759 with the empirical chi square  121.95  with prob &lt;  1.8e-17 \nThe total n.obs was  2800  with Likelihood Chi Square =  176.12  with prob &lt;  5.6e-28 \n\nTucker Lewis Index of factoring reliability =  0.95\nRMSEA index =  0.056  and the 90 % confidence intervals are  0.049 0.064\nBIC =  33.25\nFit based upon off diagonal values = 0.99\nMeasures of factor score adequacy             \n                                                   MR2  MR1  MR3\nCorrelation of (regression) scores with factors   0.88 0.92 0.86\nMultiple R square of scores with factors          0.77 0.86 0.74\nMinimum correlation of possible factor scores     0.54 0.71 0.48",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Exploratory factor analysis</span>"
    ]
  },
  {
    "objectID": "Correlated_residuals.html",
    "href": "Correlated_residuals.html",
    "title": "9  Correlated residuals",
    "section": "",
    "text": "library(psychTools)\nlibrary(lavaan)\nlibrary(flexplavaan)\n\nmod4 &lt;- '\nNeuroticism   =~ N1 + N2 + N3 + N4 + N5\nAgreeableness =~ A1 + A2 + A3 + A4 + A5\n\nN1 ~~ N2\nN3 ~~ N4\n'\n\nfit.mod4 &lt;- cfa(mod4, \n                data = bfi)\n\nsummary(fit.mod4, \n        standardized = TRUE, \n        fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 42 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        23\n\n                                                  Used       Total\n  Number of observations                          2618        2800\n\nModel Test User Model:\n                                                      \n  Test statistic                               394.559\n  Degrees of freedom                                32\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              7465.706\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.951\n  Tucker-Lewis Index (TLI)                       0.931\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -43229.770\n  Loglikelihood unrestricted model (H1)     -43032.491\n                                                      \n  Akaike (AIC)                               86505.540\n  Bayesian (BIC)                             86640.554\n  Sample-size adjusted Bayesian (SABIC)      86567.476\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.066\n  90 Percent confidence interval - lower         0.060\n  90 Percent confidence interval - upper         0.072\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.048\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Neuroticism =~                                                        \n    N1                1.000                               1.075    0.685\n    N2                0.940    0.024   38.858    0.000    1.010    0.662\n    N3                1.192    0.050   23.837    0.000    1.282    0.802\n    N4                0.930    0.045   20.736    0.000    1.000    0.637\n    N5                0.830    0.036   23.362    0.000    0.892    0.550\n  Agreeableness =~                                                      \n    A1                1.000                               0.542    0.387\n    A2               -1.413    0.086  -16.453    0.000   -0.765   -0.651\n    A3               -1.838    0.109  -16.936    0.000   -0.996   -0.761\n    A4               -1.343    0.091  -14.790    0.000   -0.728   -0.488\n    A5               -1.487    0.091  -16.359    0.000   -0.806   -0.639\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .N1 ~~                                                                 \n   .N2                0.609    0.047   12.987    0.000    0.609    0.466\n .N3 ~~                                                                 \n   .N4                0.023    0.051    0.449    0.653    0.023    0.020\n  Neuroticism ~~                                                        \n    Agreeableness     0.119    0.016    7.262    0.000    0.204    0.204\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                1.306    0.055   23.585    0.000    1.306    0.531\n   .N2                1.308    0.053   24.875    0.000    1.308    0.562\n   .N3                0.909    0.065   14.039    0.000    0.909    0.356\n   .N4                1.465    0.062   23.684    0.000    1.465    0.594\n   .N5                1.833    0.058   31.661    0.000    1.833    0.697\n   .A1                1.668    0.049   34.238    0.000    1.668    0.850\n   .A2                0.795    0.029   27.569    0.000    0.795    0.576\n   .A3                0.719    0.035   20.378    0.000    0.719    0.420\n   .A4                1.692    0.052   32.681    0.000    1.692    0.762\n   .A5                0.943    0.033   28.179    0.000    0.943    0.592\n    Neuroticism       1.155    0.071   16.195    0.000    1.000    1.000\n    Agreeableness     0.294    0.033    9.008    0.000    1.000    1.000",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlated residuals</span>"
    ]
  },
  {
    "objectID": "MissingDataExamples.html",
    "href": "MissingDataExamples.html",
    "title": "10  Dealing with missing data",
    "section": "",
    "text": "10.1 R packages used in the demonstration\nWe use the following packages: psychTools (to access the bfi data set), mice (to detect the patterns of missing data), naniar (to perform Little’s MCAR test) , Amelia (to perform the multiple imputation), lavaan (to perform the confirmatory factor analysis), and lavaan.mi (to perform the confirmatory factor analysis with multiple complete data sets).\nThese packages need to be installed once. To install the packages you need to remove the # at the beginning of each row (R does not evaluate any code that follows a #).\n#install.packages(\"psychTools\")\n#install.packages(\"naniar\")\n#install.packages(\"lavaan\")\n#install.packages(\"mice\")\n#install.packages(\"Amelia\")\nThe lavaan.mi package is not yet available from the CRAN repository, but it can be downloaded and installed from Terence Jorgenson’s github page. For this you will also need the remotes package.\n#install.packages(\"remotes\")\n#remotes::install_github(\"TDJorgensen/lavaan.mi\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Dealing with missing data</span>"
    ]
  },
  {
    "objectID": "MissingDataExamples.html#cfa-of-the-5-item-neuroticism-scale-of-the-big-five-inventory",
    "href": "MissingDataExamples.html#cfa-of-the-5-item-neuroticism-scale-of-the-big-five-inventory",
    "title": "10  Dealing with missing data",
    "section": "10.2 CFA of the 5-item Neuroticism scale of the Big Five Inventory",
    "text": "10.2 CFA of the 5-item Neuroticism scale of the Big Five Inventory\nI performed a confirmatory factor analysis of the Neuroticism scale of the Big Five Inventory. The data are in the bfi data frame, which can be found in the psychTools package. There were 2800 participants, but there were only 2694 complete cases. For convenience, I stored the five Neuroticism items (which are in columns 16 to 20 of the bfi data frame) in a new data frame called Ndata.\nIn this baseline analysis I ignored the missing data. By default, lavaan employs listwise deletion when missing data are encountered. At the top of the output it can be seen that the number of observed (n = 2800) and used (n = 2694) cases differ.\n\nlibrary(psychTools)\nlibrary(lavaan)\nlibrary(lavaan.mi)\n\nNdata &lt;- bfi[16:20]\n\nNmodel  &lt;- '\nNfactor =~ N1 + N2 + N3 + N4 + N5\n'\n\nfit.Nmodel &lt;- cfa(Nmodel, \n                  data      = Ndata, \n                  estimator = \"MLR\")\n\nsummary(fit.Nmodel,\n        standardized = TRUE,\n        fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 26 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n                                                  Used       Total\n  Number of observations                          2694        2800\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               360.932     313.521\n  Degrees of freedom                                 5           5\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.151\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              4724.621    3492.154\n  Degrees of freedom                                10          10\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.353\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.925       0.911\n  Tucker-Lewis Index (TLI)                       0.849       0.823\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.925\n  Robust Tucker-Lewis Index (TLI)                            0.849\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -23078.504  -23078.504\n  Scaling correction factor                                  1.007\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -22898.038  -22898.038\n  Scaling correction factor                                  1.055\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               46177.007   46177.007\n  Bayesian (BIC)                             46235.995   46235.995\n  Sample-size adjusted Bayesian (SABIC)      46204.222   46204.222\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.163       0.151\n  90 Percent confidence interval - lower         0.149       0.138\n  90 Percent confidence interval - upper         0.177       0.165\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.162\n  90 Percent confidence interval - lower                     0.147\n  90 Percent confidence interval - upper                     0.178\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.056       0.056\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Nfactor =~                                                            \n    N1                1.000                               1.286    0.818\n    N2                0.952    0.017   54.734    0.000    1.225    0.803\n    N3                0.892    0.028   31.540    0.000    1.147    0.717\n    N4                0.677    0.030   22.224    0.000    0.872    0.554\n    N5                0.632    0.030   21.255    0.000    0.813    0.502\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                0.819    0.048   16.954    0.000    0.819    0.331\n   .N2                0.828    0.046   18.124    0.000    0.828    0.356\n   .N3                1.245    0.052   23.807    0.000    1.245    0.486\n   .N4                1.714    0.055   31.118    0.000    1.714    0.693\n   .N5                1.968    0.057   34.381    0.000    1.968    0.748\n    Nfactor           1.655    0.065   25.460    0.000    1.000    1.000",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Dealing with missing data</span>"
    ]
  },
  {
    "objectID": "MissingDataExamples.html#examining-patterns-of-missing-data-and-testing-for-missing-completely-at-random",
    "href": "MissingDataExamples.html#examining-patterns-of-missing-data-and-testing-for-missing-completely-at-random",
    "title": "10  Dealing with missing data",
    "section": "10.3 Examining patterns of missing data and testing for “missing completely at random”",
    "text": "10.3 Examining patterns of missing data and testing for “missing completely at random”\nNext, I employed the md.pattern() function of the mice package to identify the patterns of missing data. I also used the mcar_test() function of the naniar package to perform Little’s missing completely at random (MCAR) test. There were 11 missing data patterns. There were 11 missing values for item N3, 21 for item N2, 22 for item N1, 29 for item N5, and 36 for item N4, which gives a total of 119 missing values. The pattern with the most missing values contained three missing values (four persons produced this pattern).\nLittle’s MCAR test showed that the null hypothesis that the missing data are completely at random could not be rejected: \\(\\chi^2(34) = 27.2, p = 0.791\\)\n\nlibrary(mice)\nmd.pattern(Ndata)\n\n\n\n\n\n\n\n\n     N3 N2 N1 N5 N4    \n2694  1  1  1  1  1   0\n32    1  1  1  1  0   1\n22    1  1  1  0  1   1\n15    1  1  0  1  1   1\n2     1  1  0  0  1   2\n4     1  1  0  0  0   3\n19    1  0  1  1  1   1\n1     1  0  1  0  1   2\n9     0  1  1  1  1   1\n1     0  1  0  1  1   2\n1     0  0  1  1  1   2\n     11 21 22 29 36 119\n\nlibrary(naniar)\nmcar_test(Ndata)\n\n# A tibble: 1 × 4\n  statistic    df p.value missing.patterns\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;int&gt;\n1      27.2    34   0.791               11",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Dealing with missing data</span>"
    ]
  },
  {
    "objectID": "MissingDataExamples.html#full-information-maximum-likelihood-estimation",
    "href": "MissingDataExamples.html#full-information-maximum-likelihood-estimation",
    "title": "10  Dealing with missing data",
    "section": "10.4 Full information maximum likelihood estimation",
    "text": "10.4 Full information maximum likelihood estimation\nSecond, I estimated the parameters of the confirmatory factor analysis model using full information maximum likelihood (“fiml”). This approach uses all the available information in the data set to estimate the parameters. It does not estimate what the missing values would be and it does not fill it in to complete the data set. Note that all 2800 cases were now used.\n\nfit.Nmodel.fiml &lt;- cfa(Nmodel,\n                       data      = Ndata, \n                       estimator = \"MLR\", \n                       missing   = \"fiml\",\n                       fixed.x   = FALSE)\n\nsummary(fit.Nmodel.fiml,\n        standardized = TRUE,\n        fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 30 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n\n  Number of observations                          2800\n  Number of missing patterns                        11\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               372.501     322.671\n  Degrees of freedom                                 5           5\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.154\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              4874.228    3604.773\n  Degrees of freedom                                10          10\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.352\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.924       0.912\n  Tucker-Lewis Index (TLI)                       0.849       0.823\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.924\n  Robust Tucker-Lewis Index (TLI)                            0.848\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -23768.675  -23768.675\n  Scaling correction factor                                  1.004\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -23582.425  -23582.425\n  Scaling correction factor                                  1.041\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               47567.350   47567.350\n  Bayesian (BIC)                             47656.411   47656.411\n  Sample-size adjusted Bayesian (SABIC)      47608.751   47608.751\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.162       0.151\n  90 Percent confidence interval - lower         0.148       0.138\n  90 Percent confidence interval - upper         0.176       0.164\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.163\n  90 Percent confidence interval - lower                     0.148\n  90 Percent confidence interval - upper                     0.179\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.049       0.049\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Nfactor =~                                                            \n    N1                1.000                               1.284    0.818\n    N2                0.956    0.017   55.605    0.000    1.227    0.804\n    N3                0.896    0.028   32.194    0.000    1.151    0.718\n    N4                0.677    0.030   22.550    0.000    0.869    0.554\n    N5                0.632    0.029   21.614    0.000    0.811    0.501\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                2.932    0.030   98.558    0.000    2.932    1.867\n   .N2                3.508    0.029  121.478    0.000    3.508    2.300\n   .N3                3.217    0.030  106.137    0.000    3.217    2.008\n   .N4                3.185    0.030  106.886    0.000    3.185    2.030\n   .N5                2.969    0.031   96.676    0.000    2.969    1.834\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                0.818    0.047   17.284    0.000    0.818    0.331\n   .N2                0.821    0.045   18.271    0.000    0.821    0.353\n   .N3                1.242    0.051   24.204    0.000    1.242    0.484\n   .N4                1.707    0.054   31.557    0.000    1.707    0.693\n   .N5                1.962    0.056   34.977    0.000    1.962    0.749\n    Nfactor           1.649    0.064   25.915    0.000    1.000    1.000",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Dealing with missing data</span>"
    ]
  },
  {
    "objectID": "MissingDataExamples.html#multiple-imputation-with-the-amelia-and-lavaan.mi-packages",
    "href": "MissingDataExamples.html#multiple-imputation-with-the-amelia-and-lavaan.mi-packages",
    "title": "10  Dealing with missing data",
    "section": "10.5 Multiple imputation with the Amelia and lavaan.mi packages",
    "text": "10.5 Multiple imputation with the Amelia and lavaan.mi packages\nNext, I used the Amelia and lavaan.mi packages to (a) perform multiple imputation to obtain 20 data sets that contain plausible estimates of the missing values, (b) fit the confirmatory factor analysis model to each of the data sets, and (c) report the pooled results.\nThe 20 complete data sets were stored as a list in an object I labeled Ndata.mi. I stored this list in a new object I labeled imps.\n\nlibrary(Amelia)\nlibrary(lavaan.mi)\n\nset.seed(12345)\nNdata.mi      &lt;- amelia(Ndata, \n                        m = 20)\n\nimps          &lt;- Ndata.mi$imputations\n\n\n10.5.1 Fit the model to the imputed data sets and pool the results\nI fitted the confirmatory factor analysis model to the imputed data sets in imps using the cfa.mi() function of the lavaan.mi package. The results are stored in fit.Nmodel.mi. We access the results by asking for a summary. Note that the parameter estimates are very similar to those obtained with listwise deletion and full information maximum likelihood estimation. Also, the standard errors of the parameters are very similar across the three analyses.\n\nfit.Nmodel.mi &lt;- cfa.mi(Nmodel, \n                        data      = imps,\n                        estimator = \"MLR\")\n\nsummary(fit.Nmodel.mi, \n        fit.measures = TRUE,\n        standardized = TRUE)\n\nlavaan.mi object fit to 20 imputed data sets using:\n - lavaan    (0.6-19)\n - lavaan.mi (0.1-0.0030)\nSee class?lavaan.mi help page for available methods. \n\nConvergence information:\nThe model converged on 20 imputed data sets.\nStandard errors were available for all imputations.\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                          2800\n\nModel Test User Model:\n\n                                                    Standard      Scaled\n  Test statistic                                     375.374     326.698\n  Degrees of freedom                                       5           5\n  P-value                                              0.000       0.000\n  Average scaling correction factor                                1.149\n  Pooling method                                          D4            \n    Pooled statistic                              \"standard\"            \n    \"yuan.bentler.mplus\" correction applied            AFTER     pooling\n\nModel Test Baseline Model:\n\n  Test statistic                              4849.800    3598.394\n  Degrees of freedom                                10          10\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.348\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.923       0.910\n  Tucker-Lewis Index (TLI)                       0.847       0.821\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.924\n  Robust Tucker-Lewis Index (TLI)                            0.847\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -23961.856  -23961.856\n  Scaling correction factor                                  1.005\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -23770.632  -23770.632\n  Scaling correction factor                                  1.054\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               47943.711   47943.711\n  Bayesian (BIC)                             48003.085   48003.085\n  Sample-size adjusted Bayesian (SABIC)      47971.312   47971.312\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.163       0.152\n  90 Percent confidence interval - lower         0.149       0.139\n  90 Percent confidence interval - upper         0.177       0.165\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.162\n  90 Percent confidence interval - lower                     0.148\n  90 Percent confidence interval - upper                     0.178\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.057       0.057\n\nParameter Estimates:\n\n  Standard errors                                    Sandwich\n  Information bread                                  Observed\n  Observed information based on                       Hessian\n                                                             \n  Pooled across imputations              Rubin's (1987) rules\n  Augment within-imputation variance     Scale by average RIV\n  Wald test for pooled parameters          t(df) distribution\n\n  Pooled t statistics with df &gt;= 1000 are displayed with\n  df = Inf(inity) to save space. Although the t distribution\n  with large df closely approximates a standard normal\n  distribution, exact df for reporting these t tests can be\n  obtained from parameterEstimates.mi() \n\n\nLatent Variables:\n                   Estimate  Std.Err  t-value       df  P(&gt;|t|)   Std.lv\n  Nfactor =~                                                            \n    N1                1.000                                        1.285\n    N2                0.955    0.017   55.650      Inf    0.000    1.227\n    N3                0.896    0.028   32.023      Inf    0.000    1.151\n    N4                0.677    0.030   22.524      Inf    0.000    0.870\n    N5                0.631    0.029   21.570      Inf    0.000    0.811\n  Std.all\n         \n    0.818\n    0.804\n    0.719\n    0.554\n    0.501\n\nVariances:\n                   Estimate  Std.Err  t-value       df  P(&gt;|t|)   Std.lv\n   .N1                0.818    0.047   17.270      Inf    0.000    0.818\n   .N2                0.821    0.045   18.257      Inf    0.000    0.821\n   .N3                1.239    0.051   24.060      Inf    0.000    1.239\n   .N4                1.707    0.054   31.524      Inf    0.000    1.707\n   .N5                1.961    0.056   34.947      Inf    0.000    1.961\n    Nfactor           1.650    0.064   25.835      Inf    0.000    1.000\n  Std.all\n    0.331\n    0.353\n    0.483\n    0.693\n    0.749\n    1.000",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Dealing with missing data</span>"
    ]
  },
  {
    "objectID": "MissingDataExamples.html#multiple-imputation-with-ordinal-variables",
    "href": "MissingDataExamples.html#multiple-imputation-with-ordinal-variables",
    "title": "10  Dealing with missing data",
    "section": "10.6 Multiple imputation with ordinal variables",
    "text": "10.6 Multiple imputation with ordinal variables\nThe items of the Neuroticism scale are strictly ordinal with six ordered categories. The observed values should be 1, 2, 3, 4, 5 or 6. If we don’t instruct amelia() to treat the items as ordinal, the function will treat them as continuous variables and the imputed values will contain decimals (which is not what we want). The imputed values may even extend beyond the range of the original six-point rating scale (which is really not what we want). Here I impute the missing data with the amelia() function, but now with the added argument that the items are ordinal. The imputed values will now be integers rather than decimals, which is what we want. The results are almost indistinguishable from the previous results.\n\nlibrary(Amelia)\nlibrary(lavaan.mi)\n\nset.seed(12345)\nNdata.mi2      &lt;- amelia(Ndata, \n                         m    = 20, \n                         ords = c(\"N1\", \"N2\", \"N3\", \"N4\", \"N5\"))\n\nimps2          &lt;- Ndata.mi2$imputations\n\n\nfit.Nmodel.mi2 &lt;- cfa.mi(Nmodel, \n                         data      = imps2,\n                         estimator = \"MLR\")\n\nsummary(fit.Nmodel.mi2, \n        fit.measures = TRUE,\n        standardized = TRUE)\n\nlavaan.mi object fit to 20 imputed data sets using:\n - lavaan    (0.6-19)\n - lavaan.mi (0.1-0.0030)\nSee class?lavaan.mi help page for available methods. \n\nConvergence information:\nThe model converged on 20 imputed data sets.\nStandard errors were available for all imputations.\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                          2800\n\nModel Test User Model:\n\n                                                    Standard      Scaled\n  Test statistic                                     363.290     315.881\n  Degrees of freedom                                       5           5\n  P-value                                              0.000       0.000\n  Average scaling correction factor                                1.150\n  Pooling method                                          D4            \n    Pooled statistic                              \"standard\"            \n    \"yuan.bentler.mplus\" correction applied            AFTER     pooling\n\nModel Test Baseline Model:\n\n  Test statistic                              4776.413    3547.745\n  Degrees of freedom                                10          10\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.346\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.925       0.912\n  Tucker-Lewis Index (TLI)                       0.850       0.824\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.925\n  Robust Tucker-Lewis Index (TLI)                            0.850\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -23982.925  -23982.925\n  Scaling correction factor                                  1.003\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -23795.906  -23795.906\n  Scaling correction factor                                  1.052\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               47985.849   47985.849\n  Bayesian (BIC)                             48045.223   48045.223\n  Sample-size adjusted Bayesian (SABIC)      48013.449   48013.449\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.160       0.149\n  90 Percent confidence interval - lower         0.146       0.136\n  90 Percent confidence interval - upper         0.174       0.162\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.160\n  90 Percent confidence interval - lower                     0.145\n  90 Percent confidence interval - upper                     0.175\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.057       0.057\n\nParameter Estimates:\n\n  Standard errors                                    Sandwich\n  Information bread                                  Observed\n  Observed information based on                       Hessian\n                                                             \n  Pooled across imputations              Rubin's (1987) rules\n  Augment within-imputation variance     Scale by average RIV\n  Wald test for pooled parameters          t(df) distribution\n\n  Pooled t statistics with df &gt;= 1000 are displayed with\n  df = Inf(inity) to save space. Although the t distribution\n  with large df closely approximates a standard normal\n  distribution, exact df for reporting these t tests can be\n  obtained from parameterEstimates.mi() \n\n\nLatent Variables:\n                   Estimate  Std.Err  t-value       df  P(&gt;|t|)   Std.lv\n  Nfactor =~                                                            \n    N1                1.000                                        1.284\n    N2                0.955    0.017   55.326      Inf    0.000    1.226\n    N3                0.896    0.028   32.064      Inf    0.000    1.151\n    N4                0.678    0.030   22.568      Inf    0.000    0.870\n    N5                0.631    0.029   21.614      Inf    0.000    0.810\n  Std.all\n         \n    0.817\n    0.803\n    0.718\n    0.554\n    0.501\n\nVariances:\n                   Estimate  Std.Err  t-value       df  P(&gt;|t|)   Std.lv\n   .N1                0.823    0.047   17.334      Inf    0.000    0.823\n   .N2                0.827    0.045   18.370      Inf    0.000    0.827\n   .N3                1.243    0.051   24.148      Inf    0.000    1.243\n   .N4                1.712    0.054   31.665      Inf    0.000    1.712\n   .N5                1.963    0.056   35.022      Inf    0.000    1.963\n    Nfactor           1.650    0.064   25.816      Inf    0.000    1.000\n  Std.all\n    0.333\n    0.355\n    0.484\n    0.693\n    0.749\n    1.000",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Dealing with missing data</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "11  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  }
]